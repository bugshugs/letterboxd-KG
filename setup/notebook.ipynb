{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Global setup\n",
    "Setup of everything needed to globally"
   ],
   "id": "d0483a97ba16b160"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:44:48.971537Z",
     "start_time": "2025-09-14T15:44:48.961221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')"
   ],
   "id": "1c2be7c81e867fa1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Update Letterboxd Data\n",
    "\n",
    "Update the imported Letterboxd data to always use the most up-to-date files"
   ],
   "id": "cd11e21ee987a572"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:44:48.980856Z",
     "start_time": "2025-09-14T15:44:48.979109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Boolean flag to enable/disable downloading\n",
    "# This should ONLY be set to True if you have an .env file containing valid letterboxd login credentials.\n",
    "# Otherwise, this could possibly render the given data unusable.\n",
    "\n",
    "UPDATE_DATA = False  # Set True to enable download"
   ],
   "id": "f102d8c7643c49c2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Code updating data should the flag be set to True",
   "id": "54ebea304dc48dc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:24.210763Z",
     "start_time": "2025-09-14T15:44:48.988141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "if not UPDATE_DATA:\n",
    "    print(\"Download flag is OFF. Exiting without downloading.\")\n",
    "    exit(0)\n",
    "else:\n",
    "    USERNAME = os.getenv('LETTERBOXD_USERNAME')\n",
    "    PASSWORD = os.getenv('LETTERBOXD_PASSWORD')\n",
    "    DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR')\n",
    "\n",
    "    # Set up Firefox options\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\n",
    "    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/zip\")\n",
    "    options.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Log in\n",
    "        print(\"Logging in...\")\n",
    "        driver.get(\"https://letterboxd.com/sign-in/\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.find_element(By.ID, \"field-username\").send_keys(USERNAME)\n",
    "        driver.find_element(By.ID, \"field-password\").send_keys(PASSWORD)\n",
    "        driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "\n",
    "        time.sleep(5)  # Wait for login to complete\n",
    "\n",
    "        # Step 2: Go to data page\n",
    "        print(\"Navigating to export page...\")\n",
    "        driver.get(\"https://letterboxd.com/settings/data/\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Step 3: Click export button\n",
    "        print(\"Clicking export button...\")\n",
    "        export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/user/exportdata')]\")\n",
    "\n",
    "        export_link.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Step 4: Click confirm export button\n",
    "        print(\"Clicking confirm export button...\")\n",
    "        confirm_export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/data/export/')]\")\n",
    "\n",
    "        confirm_export_link.click()\n",
    "\n",
    "        print(\"Waiting for download to complete...\")\n",
    "        time.sleep(15)  # Adjust this depending on your connection speed\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "    # Step 4: Unzip and Load\n",
    "    print(\"Looking for ZIP file in download directory...\")\n",
    "    zip_path = None\n",
    "    for file in os.listdir(DOWNLOAD_DIR):\n",
    "        if file.endswith(\".zip\") and \"letterboxd\" in file.lower():\n",
    "            zip_path = os.path.join(DOWNLOAD_DIR, file)\n",
    "            break\n",
    "\n",
    "    if not zip_path:\n",
    "        raise FileNotFoundError(\"Export ZIP file not found!\")\n",
    "\n",
    "    extract_path = os.path.join(DOWNLOAD_DIR, \"letterboxd_export\")\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Unzipping to {extract_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    # Delete the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Deleted ZIP file: {zip_path}\")\n"
   ],
   "id": "ba04fcf36cca61e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in...\n",
      "Navigating to export page...\n",
      "Clicking export button...\n",
      "Clicking confirm export button...\n",
      "Waiting for download to complete...\n",
      "Browser closed.\n",
      "Looking for ZIP file in download directory...\n",
      "Unzipping to /Users/tschaffel/PycharmProjects/letterboxd-KG/data/letterboxd_export...\n",
      "Deleted ZIP file: /Users/tschaffel/PycharmProjects/letterboxd-KG/data/letterboxd-bugshugs-2025-09-14-15-45-utc.zip\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:25.573440Z",
     "start_time": "2025-09-14T15:45:25.558485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "    \"i want to write a python script that automatically exports my letterboxd data and loads it into my jupyter notebook. can you help me with that?\"\n",
    "\n",
    "\n",
    "    \"i can manually download my data from this page\n",
    "        https://letterboxd.com/settings/data/\n",
    "\n",
    "        can the script navigate there and download it?\"\n",
    "\n",
    "\n",
    "    \"I want to use Firefox and store the credentioals in an .env\"\n",
    "\n",
    "\n",
    "    \"add the following functionality:\n",
    "\n",
    "        there is a simple boolean flag that is per default on false. only if the flag is set to true, the download of the letterboxd data triggers. otherwise it does nothing\"\n",
    "\n",
    "\n",
    "    \"its the only button on the website. can I just try to locate any button element?\"\n",
    "\n",
    "\n",
    "    \"<a href=\"/data/export/\" class=\"button -action button-action export-data-button\">Export Data</a>\n",
    "\n",
    "        now i want to locate this button here\"\n",
    "\n",
    "\n",
    "    \"i want to write a python script that unzips a file for me and then deletes said file\"\n",
    "\n",
    "\n",
    "    \"import os\n",
    "        import time\n",
    "        import zipfile\n",
    "        from dotenv import load_dotenv\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.firefox.options import Options\n",
    "        from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "        if not UPDATE_DATA:\n",
    "            print(\"Download flag is OFF. Exiting without downloading.\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            # Load environment variables\n",
    "            load_dotenv()\n",
    "            USERNAME = os.getenv('LETTERBOXD_USERNAME')\n",
    "            PASSWORD = os.getenv('LETTERBOXD_PASSWORD')\n",
    "            DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR')\n",
    "\n",
    "            # Set up Firefox options\n",
    "            options = Options()\n",
    "            options.set_preference(\"browser.download.folderList\", 2)\n",
    "            options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\n",
    "            options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/zip\")\n",
    "            options.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "\n",
    "            try:\n",
    "                # Step 1: Log in\n",
    "                print(\"Logging in...\")\n",
    "                driver.get(\"https://letterboxd.com/sign-in/\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                driver.find_element(By.ID, \"field-username\").send_keys(USERNAME)\n",
    "                driver.find_element(By.ID, \"field-password\").send_keys(PASSWORD)\n",
    "                driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "\n",
    "                time.sleep(5)  # Wait for login to complete\n",
    "\n",
    "                # Step 2: Go to data page\n",
    "                print(\"Navigating to export page...\")\n",
    "                driver.get(\"https://letterboxd.com/settings/data/\")\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Step 3: Click export button\n",
    "                print(\"Clicking export button...\")\n",
    "                export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/user/exportdata')]\")\n",
    "\n",
    "                export_link.click()\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Step 4: Click confirm export button\n",
    "                print(\"Clicking confirm export button...\")\n",
    "                confirm_export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/data/export/')]\")\n",
    "\n",
    "                confirm_export_link.click()\n",
    "\n",
    "                print(\"Waiting for download to complete...\")\n",
    "                time.sleep(15)  # Adjust this depending on your connection speed\n",
    "\n",
    "            finally:\n",
    "                driver.quit()\n",
    "                print(\"Browser closed.\")\n",
    "\n",
    "            # Step 4: Unzip and Load\n",
    "            print(\"Looking for ZIP file in download directory...\")\n",
    "            zip_path = None\n",
    "            for file in os.listdir(DOWNLOAD_DIR):\n",
    "                if file.endswith(\".zip\") and \"letterboxd\" in file.lower():\n",
    "                    zip_path = os.path.join(DOWNLOAD_DIR, file)\n",
    "                    break\n",
    "\n",
    "            if not zip_path:\n",
    "                raise FileNotFoundError(\"Export ZIP file not found!\")\n",
    "\n",
    "            extract_path = os.path.join(DOWNLOAD_DIR, \"letterboxd_export\")\n",
    "            os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "            print(f\"Unzipping to {extract_path}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "\n",
    "            # Delete the zip file after extraction\n",
    "            os.remove(zip_path)\n",
    "            print(f\"Deleted ZIP file: {zip_path}\")\n",
    "\n",
    "\n",
    "\n",
    "        Zu beginn des else zweiges m√∂chte ich den kompletten order \"Users/tschaffel/Documents/PycharmProjects/JupyterProject/data/\" l√∂schen.\"\n",
    "\n",
    "\n",
    "    \"\"\n",
    "\n",
    "'''"
   ],
   "id": "d665c8a15072193a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\\nThese following prompts were used:\\n\\n    \"i want to write a python script that automatically exports my letterboxd data and loads it into my jupyter notebook. can you help me with that?\"\\n\\n\\n    \"i can manually download my data from this page\\n        https://letterboxd.com/settings/data/\\n\\n        can the script navigate there and download it?\"\\n\\n\\n    \"I want to use Firefox and store the credentioals in an .env\"\\n\\n\\n    \"add the following functionality:\\n\\n        there is a simple boolean flag that is per default on false. only if the flag is set to true, the download of the letterboxd data triggers. otherwise it does nothing\"\\n\\n\\n    \"its the only button on the website. can I just try to locate any button element?\"\\n\\n\\n    \"<a href=\"/data/export/\" class=\"button -action button-action export-data-button\">Export Data</a>\\n\\n        now i want to locate this button here\"\\n\\n\\n    \"i want to write a python script that unzips a file for me and then deletes said file\"\\n\\n\\n    \"import os\\n        import time\\n        import zipfile\\n        from dotenv import load_dotenv\\n        from selenium import webdriver\\n        from selenium.webdriver.firefox.options import Options\\n        from selenium.webdriver.common.by import By\\n\\n\\n        if not UPDATE_DATA:\\n            print(\"Download flag is OFF. Exiting without downloading.\")\\n            exit(0)\\n        else:\\n            # Load environment variables\\n            load_dotenv()\\n            USERNAME = os.getenv(\\'LETTERBOXD_USERNAME\\')\\n            PASSWORD = os.getenv(\\'LETTERBOXD_PASSWORD\\')\\n            DOWNLOAD_DIR = os.getenv(\\'DOWNLOAD_DIR\\')\\n\\n            # Set up Firefox options\\n            options = Options()\\n            options.set_preference(\"browser.download.folderList\", 2)\\n            options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\\n            options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/zip\")\\n            options.set_preference(\"pdfjs.disabled\", True)\\n\\n            driver = webdriver.Firefox(options=options)\\n\\n            try:\\n                # Step 1: Log in\\n                print(\"Logging in...\")\\n                driver.get(\"https://letterboxd.com/sign-in/\")\\n                time.sleep(2)\\n\\n                driver.find_element(By.ID, \"field-username\").send_keys(USERNAME)\\n                driver.find_element(By.ID, \"field-password\").send_keys(PASSWORD)\\n                driver.find_element(By.TAG_NAME, \"button\").click()\\n\\n                time.sleep(5)  # Wait for login to complete\\n\\n                # Step 2: Go to data page\\n                print(\"Navigating to export page...\")\\n                driver.get(\"https://letterboxd.com/settings/data/\")\\n                time.sleep(3)\\n\\n                # Step 3: Click export button\\n                print(\"Clicking export button...\")\\n                export_link = driver.find_element(By.XPATH, \"//a[contains(@href, \\'/user/exportdata\\')]\")\\n\\n                export_link.click()\\n                time.sleep(3)\\n\\n                # Step 4: Click confirm export button\\n                print(\"Clicking confirm export button...\")\\n                confirm_export_link = driver.find_element(By.XPATH, \"//a[contains(@href, \\'/data/export/\\')]\")\\n\\n                confirm_export_link.click()\\n\\n                print(\"Waiting for download to complete...\")\\n                time.sleep(15)  # Adjust this depending on your connection speed\\n\\n            finally:\\n                driver.quit()\\n                print(\"Browser closed.\")\\n\\n            # Step 4: Unzip and Load\\n            print(\"Looking for ZIP file in download directory...\")\\n            zip_path = None\\n            for file in os.listdir(DOWNLOAD_DIR):\\n                if file.endswith(\".zip\") and \"letterboxd\" in file.lower():\\n                    zip_path = os.path.join(DOWNLOAD_DIR, file)\\n                    break\\n\\n            if not zip_path:\\n                raise FileNotFoundError(\"Export ZIP file not found!\")\\n\\n            extract_path = os.path.join(DOWNLOAD_DIR, \"letterboxd_export\")\\n            os.makedirs(extract_path, exist_ok=True)\\n\\n            print(f\"Unzipping to {extract_path}...\")\\n            with zipfile.ZipFile(zip_path, \\'r\\') as zip_ref:\\n                zip_ref.extractall(extract_path)\\n\\n            # Delete the zip file after extraction\\n            os.remove(zip_path)\\n            print(f\"Deleted ZIP file: {zip_path}\")\\n\\n\\n\\n        Zu beginn des else zweiges m√∂chte ich den kompletten order \"Users/tschaffel/Documents/PycharmProjects/JupyterProject/data/\" l√∂schen.\"\\n\\n\\n    \"\"\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Imports and files setup"
   ],
   "id": "11257c811890c6e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:27.090366Z",
     "start_time": "2025-09-14T15:45:25.683393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"../data/letterboxd_export/ratings.csv\")\n",
    "watched = pd.read_csv(\"../data/letterboxd_export/watched.csv\")\n",
    "likes = pd.read_csv(\"../data/letterboxd_export/likes/films.csv\")\n",
    "diary = pd.read_csv(\"../data/letterboxd_export/diary.csv\")\n",
    "\n",
    "orphaned_diary = pd.read_csv(\"../data/letterboxd_export/orphaned/diary.csv\")"
   ],
   "id": "ea0758aac0c1eadd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See Data\n",
    "\n",
    "First check on how the tables look"
   ],
   "id": "728cab10b3155747"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:27.208809Z",
     "start_time": "2025-09-14T15:45:27.133963Z"
    }
   },
   "cell_type": "code",
   "source": "ratings.tail(10)",
   "id": "b783b33168960af3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date                                   Name  Year  \\\n",
       "318  2025-07-20                               Dominion  2018   \n",
       "319  2025-07-28                                   Okja  2017   \n",
       "320  2025-07-29                            Challengers  2024   \n",
       "321  2025-08-02                             BlackBerry  2023   \n",
       "322  2025-08-13  The Hunger Games: Mockingjay ‚Äì Part 2  2015   \n",
       "323  2025-08-21                                Weapons  2025   \n",
       "324  2025-08-21            Predator: Killer of Killers  2025   \n",
       "325  2025-08-23                AVP: Alien vs. Predator  2004   \n",
       "326  2025-08-24                         Alien: Romulus  2024   \n",
       "327  2025-09-01                             The Matrix  1999   \n",
       "\n",
       "           Letterboxd URI  Rating  \n",
       "318  https://boxd.it/gXqy     4.0  \n",
       "319  https://boxd.it/dvXe     4.0  \n",
       "320  https://boxd.it/zld0     3.0  \n",
       "321  https://boxd.it/CoVK     4.0  \n",
       "322  https://boxd.it/4hk0     3.0  \n",
       "323  https://boxd.it/EMTM     3.0  \n",
       "324  https://boxd.it/QYvQ     3.0  \n",
       "325  https://boxd.it/2atU     1.5  \n",
       "326  https://boxd.it/zGqO     4.5  \n",
       "327  https://boxd.it/2a1m     5.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://boxd.it/gXqy</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2025-07-28</td>\n",
       "      <td>Okja</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://boxd.it/dvXe</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>Challengers</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zld0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://boxd.it/CoVK</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://boxd.it/4hk0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Weapons</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/EMTM</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Predator: Killer of Killers</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/QYvQ</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>2025-08-23</td>\n",
       "      <td>AVP: Alien vs. Predator</td>\n",
       "      <td>2004</td>\n",
       "      <td>https://boxd.it/2atU</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>2025-08-24</td>\n",
       "      <td>Alien: Romulus</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zGqO</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>The Matrix</td>\n",
       "      <td>1999</td>\n",
       "      <td>https://boxd.it/2a1m</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:27.384097Z",
     "start_time": "2025-09-14T15:45:27.369838Z"
    }
   },
   "cell_type": "code",
   "source": "watched.tail(10)",
   "id": "801888289335e675",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date                                   Name  Year  \\\n",
       "745  2025-07-20                               Superman  2025   \n",
       "746  2025-07-20                               Dominion  2018   \n",
       "747  2025-07-29                            Challengers  2024   \n",
       "748  2025-08-02                 A Very Sunny Christmas  2009   \n",
       "749  2025-08-02                             BlackBerry  2023   \n",
       "750  2025-08-13  The Hunger Games: Mockingjay ‚Äì Part 2  2015   \n",
       "751  2025-08-21                                Weapons  2025   \n",
       "752  2025-08-21            Predator: Killer of Killers  2025   \n",
       "753  2025-08-24                                  Cargo  2017   \n",
       "754  2025-09-08                          Evan Almighty  2007   \n",
       "\n",
       "           Letterboxd URI  \n",
       "745  https://boxd.it/E9IU  \n",
       "746  https://boxd.it/gXqy  \n",
       "747  https://boxd.it/zld0  \n",
       "748  https://boxd.it/G7Yg  \n",
       "749  https://boxd.it/CoVK  \n",
       "750  https://boxd.it/4hk0  \n",
       "751  https://boxd.it/EMTM  \n",
       "752  https://boxd.it/QYvQ  \n",
       "753  https://boxd.it/f55Y  \n",
       "754  https://boxd.it/26s4  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>Superman</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/E9IU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://boxd.it/gXqy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>Challengers</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zld0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>A Very Sunny Christmas</td>\n",
       "      <td>2009</td>\n",
       "      <td>https://boxd.it/G7Yg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://boxd.it/CoVK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://boxd.it/4hk0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Weapons</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/EMTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Predator: Killer of Killers</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/QYvQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>2025-08-24</td>\n",
       "      <td>Cargo</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://boxd.it/f55Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>Evan Almighty</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://boxd.it/26s4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:27.791372Z",
     "start_time": "2025-09-14T15:45:27.754770Z"
    }
   },
   "cell_type": "code",
   "source": "likes.tail(10)",
   "id": "c2063d1cf83f6513",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date                                   Name  Year  \\\n",
       "226  2025-01-20               David Lynch Cooks Quinoa  2007   \n",
       "227  2025-01-30                   Sonic the Hedgehog 3  2024   \n",
       "228  2025-02-23                       The Elephant Man  1980   \n",
       "229  2025-02-23                             Eraserhead  1977   \n",
       "230  2025-07-04                              Ballerina  2025   \n",
       "231  2025-07-04        The Hunger Games: Catching Fire  2013   \n",
       "232  2025-07-04  The Hunger Games: Mockingjay ‚Äì Part 1  2014   \n",
       "233  2025-07-20                               Superman  2025   \n",
       "234  2025-07-20                               Dominion  2018   \n",
       "235  2025-08-02                             BlackBerry  2023   \n",
       "\n",
       "           Letterboxd URI  \n",
       "226  https://boxd.it/Ljok  \n",
       "227  https://boxd.it/zq0U  \n",
       "228  https://boxd.it/27LQ  \n",
       "229  https://boxd.it/299u  \n",
       "230  https://boxd.it/jKqG  \n",
       "231  https://boxd.it/3sAw  \n",
       "232  https://boxd.it/4hka  \n",
       "233  https://boxd.it/E9IU  \n",
       "234  https://boxd.it/gXqy  \n",
       "235  https://boxd.it/CoVK  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2025-01-20</td>\n",
       "      <td>David Lynch Cooks Quinoa</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://boxd.it/Ljok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>Sonic the Hedgehog 3</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zq0U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2025-02-23</td>\n",
       "      <td>The Elephant Man</td>\n",
       "      <td>1980</td>\n",
       "      <td>https://boxd.it/27LQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2025-02-23</td>\n",
       "      <td>Eraserhead</td>\n",
       "      <td>1977</td>\n",
       "      <td>https://boxd.it/299u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2025-07-04</td>\n",
       "      <td>Ballerina</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/jKqG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2025-07-04</td>\n",
       "      <td>The Hunger Games: Catching Fire</td>\n",
       "      <td>2013</td>\n",
       "      <td>https://boxd.it/3sAw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>2025-07-04</td>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 1</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://boxd.it/4hka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>Superman</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/E9IU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2025-07-20</td>\n",
       "      <td>Dominion</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://boxd.it/gXqy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://boxd.it/CoVK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:27.979410Z",
     "start_time": "2025-09-14T15:45:27.958310Z"
    }
   },
   "cell_type": "code",
   "source": "diary.tail(10)",
   "id": "22330eeffb2f6b76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Date                                   Name  Year  \\\n",
       "352  2025-08-02                             BlackBerry  2023   \n",
       "353  2025-08-13  The Hunger Games: Mockingjay ‚Äì Part 2  2015   \n",
       "354  2025-08-21                                Weapons  2025   \n",
       "355  2025-08-21            Predator: Killer of Killers  2025   \n",
       "356  2025-08-23                AVP: Alien vs. Predator  2004   \n",
       "357  2025-08-24                                  Alien  1979   \n",
       "358  2025-08-24                         Alien: Romulus  2024   \n",
       "359  2025-09-01                             The Matrix  1999   \n",
       "360  2025-09-04               David Lynch Cooks Quinoa  2007   \n",
       "361  2025-09-10                             Twin Peaks  1989   \n",
       "\n",
       "             Letterboxd URI  Rating Rewatch  \\\n",
       "352  https://boxd.it/azoK2P     4.0     NaN   \n",
       "353  https://boxd.it/aI2Heb     3.0     NaN   \n",
       "354  https://boxd.it/aO2JS7     3.0     NaN   \n",
       "355  https://boxd.it/aO3aPJ     3.0     NaN   \n",
       "356  https://boxd.it/aPgtlt     1.5     NaN   \n",
       "357  https://boxd.it/aPGdlf     NaN     Yes   \n",
       "358  https://boxd.it/aQ58b5     4.5     Yes   \n",
       "359  https://boxd.it/aVJ8sP     5.0     Yes   \n",
       "360  https://boxd.it/aXzHc1     NaN     Yes   \n",
       "361  https://boxd.it/b1zd35     NaN     Yes   \n",
       "\n",
       "                                                  Tags Watched Date  \n",
       "352                                                NaN   2025-08-01  \n",
       "353                             with friends, with kai   2025-08-12  \n",
       "354                       in cinema, nonstop :), haydn   2025-08-18  \n",
       "355                                          animation   2025-08-21  \n",
       "356                            cool creature design!!!   2025-08-22  \n",
       "357                                commentary, youtube   2025-08-23  \n",
       "358                               simping for rain btw   2025-08-23  \n",
       "359  üè≥Ô∏è‚Äç‚ößÔ∏è, i ain't reading all that i'm happy for ...   2025-08-31  \n",
       "360   joke's over david you can come back now, youtube   2025-09-03  \n",
       "361     library, DVD, twin peaks, david lynch, tv show   2025-09-09  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rewatch</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Watched Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>2025-08-02</td>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://boxd.it/azoK2P</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>2025-08-13</td>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://boxd.it/aI2Heb</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with friends, with kai</td>\n",
       "      <td>2025-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Weapons</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/aO2JS7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in cinema, nonstop :), haydn</td>\n",
       "      <td>2025-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>Predator: Killer of Killers</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/aO3aPJ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>animation</td>\n",
       "      <td>2025-08-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2025-08-23</td>\n",
       "      <td>AVP: Alien vs. Predator</td>\n",
       "      <td>2004</td>\n",
       "      <td>https://boxd.it/aPgtlt</td>\n",
       "      <td>1.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cool creature design!!!</td>\n",
       "      <td>2025-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2025-08-24</td>\n",
       "      <td>Alien</td>\n",
       "      <td>1979</td>\n",
       "      <td>https://boxd.it/aPGdlf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>commentary, youtube</td>\n",
       "      <td>2025-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>2025-08-24</td>\n",
       "      <td>Alien: Romulus</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/aQ58b5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>simping for rain btw</td>\n",
       "      <td>2025-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>The Matrix</td>\n",
       "      <td>1999</td>\n",
       "      <td>https://boxd.it/aVJ8sP</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>üè≥Ô∏è‚Äç‚ößÔ∏è, i ain't reading all that i'm happy for ...</td>\n",
       "      <td>2025-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>David Lynch Cooks Quinoa</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://boxd.it/aXzHc1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>joke's over david you can come back now, youtube</td>\n",
       "      <td>2025-09-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2025-09-10</td>\n",
       "      <td>Twin Peaks</td>\n",
       "      <td>1989</td>\n",
       "      <td>https://boxd.it/b1zd35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>library, DVD, twin peaks, david lynch, tv show</td>\n",
       "      <td>2025-09-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:28.440537Z",
     "start_time": "2025-09-14T15:45:28.427013Z"
    }
   },
   "cell_type": "code",
   "source": "orphaned_diary.tail(10)",
   "id": "b785160cb5f5c5b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Date                   Name    Year  Letterboxd URI  Rating  Rewatch  \\\n",
       "0  2022-06-18  Threat Level Midnight     NaN             NaN     5.0      NaN   \n",
       "1  2023-03-28  The Dumbest Boy Alive  2016.0             NaN     3.5      NaN   \n",
       "2  2024-11-25                 Arcane  2024.0             NaN     5.0      NaN   \n",
       "\n",
       "                         Tags Watched Date  \n",
       "0                         NaN   2022-06-18  \n",
       "1  youtube, shuffledwatchlist   2023-03-27  \n",
       "2                         NaN   2024-11-24  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rewatch</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Watched Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-18</td>\n",
       "      <td>Threat Level Midnight</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>The Dumbest Boy Alive</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>youtube, shuffledwatchlist</td>\n",
       "      <td>2023-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>Arcane</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-11-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Connect tables\n",
    "Connect letterboxd tables to get one table with following data for every movie:\n",
    "- Title and Year\n",
    "- Letterboxd URI\n",
    "- User Rating\n",
    "- Was the movie liked\n",
    "- When was the movie last watched\n",
    "- How many times has the movie been watched"
   ],
   "id": "fe426351a58bc056"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:28.606651Z",
     "start_time": "2025-09-14T15:45:28.537611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged = pd.merge(\n",
    "    watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "    ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "    on=['Name', 'Year', 'Letterboxd URI'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add \"Liked\" information\n",
    "liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "    lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add \"Last Watched\" information\n",
    "\n",
    "# Step 1: Group diary entries by movie and take the latest date as a string\n",
    "last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "# Step 2: Merge with the existing merged DataFrame\n",
    "merged = pd.merge(\n",
    "    merged,\n",
    "    last_watched,\n",
    "    on=[\"Name\", \"Year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add \"Times watched\" information\n",
    "\n",
    "# Step 1: Sort diary by \"Watched Date\" so the earliest watches come first\n",
    "diary_sorted = diary.sort_values(\"Watched Date\")\n",
    "\n",
    "# Step 2: Define a function to count watches and check if first was a rewatch\n",
    "def get_times_watched(group):\n",
    "    count = len(group)\n",
    "    first_rewatch = group.iloc[0][\"Rewatch\"] == \"Yes\"\n",
    "    return f\"{count}+\" if first_rewatch else str(count)\n",
    "\n",
    "# Step 3: Group and apply, excluding group keys from the inner DataFrame\n",
    "times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\n",
    "    get_times_watched\n",
    ").reset_index(name=\"Times watched\")\n",
    "\n",
    "\n",
    "# Step 4: Merge into the main DataFrame\n",
    "merged = (pd.merge(\n",
    "    merged,\n",
    "    times_watched,\n",
    "    on=[\"Name\", \"Year\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={'Name': 'Title'}).sort_values(by=\"Last watched date\", ascending=False))\n",
    "\n",
    "merged.head(15)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8caefb07b3b194b1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/g4r6gn7d0tj7xck34sb648br0000gn/T/ipykernel_4004/1782928480.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                     Title  Year        Letterboxd URI  \\\n",
       "717                             Twin Peaks  1989  https://boxd.it/g8pk   \n",
       "735               David Lynch Cooks Quinoa  2007  https://boxd.it/Ljok   \n",
       "88                              The Matrix  1999  https://boxd.it/2a1m   \n",
       "99                                   Alien  1979  https://boxd.it/2awY   \n",
       "714                         Alien: Romulus  2024  https://boxd.it/zGqO   \n",
       "108                AVP: Alien vs. Predator  2004  https://boxd.it/2atU   \n",
       "752            Predator: Killer of Killers  2025  https://boxd.it/QYvQ   \n",
       "751                                Weapons  2025  https://boxd.it/EMTM   \n",
       "750  The Hunger Games: Mockingjay ‚Äì Part 2  2015  https://boxd.it/4hk0   \n",
       "749                             BlackBerry  2023  https://boxd.it/CoVK   \n",
       "747                            Challengers  2024  https://boxd.it/zld0   \n",
       "446                                   Okja  2017  https://boxd.it/dvXe   \n",
       "746                               Dominion  2018  https://boxd.it/gXqy   \n",
       "745                               Superman  2025  https://boxd.it/E9IU   \n",
       "744  The Hunger Games: Mockingjay ‚Äì Part 1  2014  https://boxd.it/4hka   \n",
       "\n",
       "     Rating Liked Last watched date Times watched  \n",
       "717     5.0   Yes        2025-09-09             2  \n",
       "735     NaN   Yes        2025-09-03             2  \n",
       "88      5.0   Yes        2025-08-31            2+  \n",
       "99      5.0   Yes        2025-08-23            2+  \n",
       "714     4.5   Yes        2025-08-23             2  \n",
       "108     1.5    No        2025-08-22             1  \n",
       "752     3.0    No        2025-08-21             1  \n",
       "751     3.0    No        2025-08-18             1  \n",
       "750     3.0    No        2025-08-12             1  \n",
       "749     4.0   Yes        2025-08-01             1  \n",
       "747     3.0    No        2025-07-29             1  \n",
       "446     4.0   Yes        2025-07-26             2  \n",
       "746     4.0   Yes        2025-07-19             1  \n",
       "745     3.5   Yes        2025-07-16             1  \n",
       "744     3.0   Yes        2025-07-04             1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Letterboxd URI</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Liked</th>\n",
       "      <th>Last watched date</th>\n",
       "      <th>Times watched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>Twin Peaks</td>\n",
       "      <td>1989</td>\n",
       "      <td>https://boxd.it/g8pk</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>David Lynch Cooks Quinoa</td>\n",
       "      <td>2007</td>\n",
       "      <td>https://boxd.it/Ljok</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>1999</td>\n",
       "      <td>https://boxd.it/2a1m</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Alien</td>\n",
       "      <td>1979</td>\n",
       "      <td>https://boxd.it/2awY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-08-23</td>\n",
       "      <td>2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>Alien: Romulus</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zGqO</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-08-23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>AVP: Alien vs. Predator</td>\n",
       "      <td>2004</td>\n",
       "      <td>https://boxd.it/2atU</td>\n",
       "      <td>1.5</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-08-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Predator: Killer of Killers</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/QYvQ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-08-21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Weapons</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/EMTM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-08-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://boxd.it/4hk0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-08-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>BlackBerry</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://boxd.it/CoVK</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-08-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>Challengers</td>\n",
       "      <td>2024</td>\n",
       "      <td>https://boxd.it/zld0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2025-07-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Okja</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://boxd.it/dvXe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-07-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>Dominion</td>\n",
       "      <td>2018</td>\n",
       "      <td>https://boxd.it/gXqy</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-07-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>Superman</td>\n",
       "      <td>2025</td>\n",
       "      <td>https://boxd.it/E9IU</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-07-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>The Hunger Games: Mockingjay ‚Äì Part 1</td>\n",
       "      <td>2014</td>\n",
       "      <td>https://boxd.it/4hka</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2025-07-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:45:29.349795Z",
     "start_time": "2025-09-14T15:45:29.342816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "    \"wie kann ich selbst technisch die beiden tabellen verkn√ºpfen?\"\n",
    "\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched,\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "        merged.head()\n",
    "\n",
    "        adaptiere mir diesen code bitte so, dass das CSV file unter \"likes/films\" auch eingelesen wird. in der resultierenden tabelle gibt es eine neue spalte namens \"Liked\", in der jeder film, der in \"likes/films\" enhalten ist, einen eintrag \"Yes\" bekommt, jeder andere einen eintrag \"No\" \"\n",
    "\n",
    "\n",
    "    \"Anstelle von \"ratings\", benutze \"diary\" und mach eine neue Spalte \"Anzahl\", die beinhaltet wie oft der selbe Film in \"diary\" vorkommt. als rating soll immer das datumsm√§√üig letzte verwendet werden.\"\n",
    "\n",
    "\n",
    "    \"das funktioniert nicht, date rating und anzahl sind alles NaN jetzt\"\n",
    "\n",
    "\n",
    "    \"date ist jetzt NaT, rating und number immer noch NaN\"\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "        this is my combined table so far. Now i want to add further information.\n",
    "        I want to add this:\n",
    "        First, from the table \"diary\" i want to add the LAST watched date of the movie. use the column \"watched date\", and if the movie has multiple entries i want the last one. call this new column \"Last watched date\". For movies that don't exist in \"diary\" we will have NaN.\"\n",
    "\n",
    "\n",
    "    \"I dont want to convert the date to datetime\"\n",
    "\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add \"Last Watched\" information\n",
    "\n",
    "        # Step 1: Group diary entries by movie and take the latest date as a string\n",
    "        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "        # Step 2: Merge with the existing merged DataFrame\n",
    "        merged = pd.merge(\n",
    "            merged,\n",
    "            last_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        this is my code rn\n",
    "\n",
    "        now I want another thing:\n",
    "        I want a new column called \"Times watched\". From diary use the following information:\n",
    "        - Count the number of entries and display the number in the column \"Times watched\".\n",
    "        - If the oldest entry has \"Yes\" in the \"Rewatch\" column, then add a \"+\" after the number. Another way to do this is if every entry has \"Yes\" in the \"Rewatch\" column, do whatever is easier.\"\n",
    "\n",
    "\n",
    "    \"it is not chronologically correct, so i would prefer the safer option\"\n",
    "\n",
    "\n",
    "'''"
   ],
   "id": "f83a062950ad4864",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\\nThese following prompts were used:\\n\\n    \"wie kann ich selbst technisch die beiden tabellen verkn√ºpfen?\"\\n\\n\\n    \"merged = pd.merge(\\n            watched,\\n            ratings[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\', \\'Rating\\']],\\n            on=[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\'],\\n            how=\\'left\\'\\n        )\\n        merged.head()\\n\\n        adaptiere mir diesen code bitte so, dass das CSV file unter \"likes/films\" auch eingelesen wird. in der resultierenden tabelle gibt es eine neue spalte namens \"Liked\", in der jeder film, der in \"likes/films\" enhalten ist, einen eintrag \"Yes\" bekommt, jeder andere einen eintrag \"No\" \"\\n\\n\\n    \"Anstelle von \"ratings\", benutze \"diary\" und mach eine neue Spalte \"Anzahl\", die beinhaltet wie oft der selbe Film in \"diary\" vorkommt. als rating soll immer das datumsm√§√üig letzte verwendet werden.\"\\n\\n\\n    \"das funktioniert nicht, date rating und anzahl sind alles NaN jetzt\"\\n\\n\\n    \"date ist jetzt NaT, rating und number immer noch NaN\"\\n\\n    \"merged = pd.merge(\\n            watched[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\']],\\n            ratings[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\', \\'Rating\\']],\\n            on=[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\'],\\n            how=\\'left\\'\\n        )\\n\\n        # Add \"Liked\" information\\n        liked_uris = set(likes[\"Letterboxd URI\"])\\n\\n        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\\n            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\\n        )\\n\\n        this is my combined table so far. Now i want to add further information.\\n        I want to add this:\\n        First, from the table \"diary\" i want to add the LAST watched date of the movie. use the column \"watched date\", and if the movie has multiple entries i want the last one. call this new column \"Last watched date\". For movies that don\\'t exist in \"diary\" we will have NaN.\"\\n\\n\\n    \"I dont want to convert the date to datetime\"\\n\\n\\n    \"merged = pd.merge(\\n            watched[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\']],\\n            ratings[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\', \\'Rating\\']],\\n            on=[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\'],\\n            how=\\'left\\'\\n        )\\n\\n        # Add \"Liked\" information\\n        liked_uris = set(likes[\"Letterboxd URI\"])\\n\\n        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\\n            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\\n        )\\n\\n\\n        # Add \"Last Watched\" information\\n\\n        # Step 1: Group diary entries by movie and take the latest date as a string\\n        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\\n        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\\n\\n        # Step 2: Merge with the existing merged DataFrame\\n        merged = pd.merge(\\n            merged,\\n            last_watched,\\n            on=[\"Name\", \"Year\"],\\n            how=\"left\"\\n        )\\n\\n        this is my code rn\\n\\n        now I want another thing:\\n        I want a new column called \"Times watched\". From diary use the following information:\\n        - Count the number of entries and display the number in the column \"Times watched\".\\n        - If the oldest entry has \"Yes\" in the \"Rewatch\" column, then add a \"+\" after the number. Another way to do this is if every entry has \"Yes\" in the \"Rewatch\" column, do whatever is easier.\"\\n\\n\\n    \"it is not chronologically correct, so i would prefer the safer option\"\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Connect Letterboxd Data with TMDB data",
   "id": "31806bb8a835f302"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:55:51.382852Z",
     "start_time": "2025-09-14T15:45:29.394231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Bearer \" + TMDB_API_TOKEN,\n",
    "    \"Content-Type\": \"application/json;charset=utf-8\"\n",
    "}\n",
    "\n",
    "def normalize_title(title):\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    # Unicode normalize, convert to ASCII-compatible\n",
    "    title = unicodedata.normalize(\"NFKC\", title)\n",
    "\n",
    "    # Replace common visually similar characters\n",
    "    substitutions = {\n",
    "        \"‚Äì\": \"-\",  # en dash\n",
    "        \"‚Äî\": \"-\",  # em dash\n",
    "        \"‚àí\": \"-\",  # minus\n",
    "        \"√ó\": \"x\",  # multiplication sign\n",
    "        \"‚Äô\": \"'\",  # curly apostrophe\n",
    "        \"‚Äú\": '\"',\n",
    "        \"‚Äù\": '\"',\n",
    "        \"‚Ä¶\": \"...\",\n",
    "        \"&\": \"and\",  # optional\n",
    "    }\n",
    "\n",
    "    for orig, repl in substitutions.items():\n",
    "        title = title.replace(orig, repl)\n",
    "\n",
    "    # Collapse multiple spaces and lowercase\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip().lower()\n",
    "    return title\n",
    "\n",
    "def search_exact_match(results, search_title):\n",
    "    norm_search = normalize_title(search_title)\n",
    "    for r in results:\n",
    "        tmdb_title = r.get(\"title\") or r.get(\"name\") or \"\"\n",
    "        if normalize_title(tmdb_title) == norm_search:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def search_movie_or_tv(title, year=None):\n",
    "    # First: try movie search\n",
    "    movie_url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "    params = {\"query\": title}\n",
    "    if year:\n",
    "        params[\"year\"] = year\n",
    "\n",
    "    response = requests.get(movie_url, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        match = search_exact_match(results, title)\n",
    "        if match:\n",
    "            match[\"media_type\"] = \"movie\"\n",
    "            return match\n",
    "\n",
    "    # Second: try TV search\n",
    "    tv_url = \"https://api.themoviedb.org/3/search/tv\"\n",
    "    params = {\"query\": title}\n",
    "    if year:\n",
    "        params[\"first_air_date_year\"] = year\n",
    "\n",
    "    response = requests.get(tv_url, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        match = search_exact_match(results, title)\n",
    "        if match:\n",
    "            match[\"media_type\"] = \"tv\"\n",
    "            return match\n",
    "    return None\n",
    "\n",
    "def get_details(tmdb_id, media_type):\n",
    "    url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "def get_credits(tmdb_id, media_type):\n",
    "    url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}/credits\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "def enrich_dataframe(df):\n",
    "    tmdb_media_base_url = \"https://www.themoviedb.org/\"\n",
    "    tmdb_poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "    tmdb_person_base_url = \"http://www.themoviedb.org/person/\"\n",
    "    tmdb_genre_base_url = \"https://www.themoviedb.org/genre/\"\n",
    "    tmdb_company_base_url = \"https://www.themoviedb.org/company/\"\n",
    "    size = \"original\"\n",
    "\n",
    "    # Create empty columns for TMDB data\n",
    "    df[\"tmdb_url\"] = None\n",
    "    df[\"overview\"] = None\n",
    "    df[\"genres\"] = None\n",
    "    df[\"runtime\"] = None\n",
    "    df[\"vote_average\"] = None\n",
    "    df[\"poster_url\"] = None\n",
    "    df[\"media_type\"] = None\n",
    "    df[\"director\"] = None\n",
    "    df[\"actors\"] = None\n",
    "    df[\"characters\"] = None\n",
    "    df[\"origin_country\"] = None # from here\n",
    "    df[\"original_language\"] = None\n",
    "    df[\"popularity\"] = None\n",
    "    df[\"production_companies\"] = None\n",
    "    df[\"production_countries\"] = None\n",
    "    df[\"spoken_languages\"] = None\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"Starting enrichment for {total} titles using TMDB data...\\n\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row[\"Title\"]\n",
    "        year = row.get(\"Year\", None)\n",
    "\n",
    "        print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "        result = search_movie_or_tv(title, year)\n",
    "        if result:\n",
    "            print(\" ‚úÖ Match found\")\n",
    "            media_type = result[\"media_type\"]\n",
    "            tmdb_id = result[\"id\"]\n",
    "\n",
    "            details = get_details(tmdb_id, media_type)\n",
    "            credits = get_credits(tmdb_id, media_type)\n",
    "\n",
    "            if details:\n",
    "                df.at[idx, \"media_type\"] = media_type\n",
    "                df.at[idx, \"tmdb_url\"] = tmdb_media_base_url + f\"{media_type}/\" + str(tmdb_id)\n",
    "                df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                df.at[idx, \"genres\"] = [str(g[\"name\"]) + \":\" + tmdb_genre_base_url + str(g[\"id\"]) for g in details.get(\"genres\", [])]\n",
    "                df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "                df.at[idx, \"origin_country\"] = details.get(\"origin_country\")\n",
    "                df.at[idx, \"original_language\"] = details.get(\"original_language\")\n",
    "                df.at[idx, \"popularity\"] = details.get(\"popularity\")\n",
    "                df.at[idx, \"production_companies\"] = [str(p[\"name\"]) + \":\" + tmdb_company_base_url + str(p[\"id\"]) + \":\" + str(p[\"origin_country\"]) for p in details.get(\"production_companies\", [])]\n",
    "                df.at[idx, \"production_countries\"] = [str(p[\"name\"]) + \":\" + str(p[\"iso_3166_1\"]) for p in details.get(\"production_countries\", [])]\n",
    "                df.at[idx, \"spoken_languages\"] = [str(s[\"english_name\"]) + \":\" + str(s[\"iso_639_1\"]) for s in details.get(\"spoken_languages\", [])]\n",
    "\n",
    "                if media_type == \"movie\":\n",
    "                    df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                else:\n",
    "                    df.at[idx, \"runtime\"] = None\n",
    "\n",
    "                poster_path = details.get(\"poster_path\")\n",
    "                if poster_path:\n",
    "                    df.at[idx, \"poster_url\"] = tmdb_poster_base_url + size + poster_path\n",
    "\n",
    "            if credits:\n",
    "                # Directors (may be multiple)\n",
    "                crew = credits.get(\"crew\", [])\n",
    "                directors = [str(p[\"name\"]) + \":\" + tmdb_person_base_url + str(p[\"id\"]) for p in crew if p.get(\"job\") == \"Director\"]\n",
    "                df.at[idx, \"director\"] = directors if directors else None\n",
    "\n",
    "                # Top 5 actors and their characters\n",
    "                cast = credits.get(\"cast\", [])[:10]\n",
    "                actor_names = [str(a[\"name\"]) + \":\" + tmdb_person_base_url + str(a[\"id\"]) for a in cast]\n",
    "                character_names = [a[\"character\"] for a in cast]\n",
    "                df.at[idx, \"actors\"] = actor_names if actor_names else None\n",
    "                df.at[idx, \"characters\"] = character_names if character_names else None\n",
    "        else:\n",
    "            print(\" ‚ùå No exact match found\")\n",
    "\n",
    "        sleep(0.25)\n",
    "\n",
    "    print(\"\\n‚úîÔ∏è  Enrichment completed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enriched_merged = enrich_dataframe(merged)\n",
    "enriched_merged.to_csv(\"../data/enriched_merged.csv\", index=False)\n"
   ],
   "id": "751ffe9a86ea630f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enrichment for 755 titles using TMDB data...\n",
      "\n",
      "[718/755] Searching: 'Twin Peaks' (1989) ‚úÖ Match found\n",
      "[736/755] Searching: 'David Lynch Cooks Quinoa' (2007) ‚ùå No exact match found\n",
      "[89/755] Searching: 'The Matrix' (1999) ‚úÖ Match found\n",
      "[100/755] Searching: 'Alien' (1979) ‚úÖ Match found\n",
      "[715/755] Searching: 'Alien: Romulus' (2024) ‚úÖ Match found\n",
      "[109/755] Searching: 'AVP: Alien vs. Predator' (2004) ‚úÖ Match found\n",
      "[753/755] Searching: 'Predator: Killer of Killers' (2025) ‚úÖ Match found\n",
      "[752/755] Searching: 'Weapons' (2025) ‚úÖ Match found\n",
      "[751/755] Searching: 'The Hunger Games: Mockingjay ‚Äì Part 2' (2015) ‚úÖ Match found\n",
      "[750/755] Searching: 'BlackBerry' (2023) ‚úÖ Match found\n",
      "[748/755] Searching: 'Challengers' (2024) ‚úÖ Match found\n",
      "[447/755] Searching: 'Okja' (2017) ‚úÖ Match found\n",
      "[747/755] Searching: 'Dominion' (2018) ‚úÖ Match found\n",
      "[746/755] Searching: 'Superman' (2025) ‚úÖ Match found\n",
      "[745/755] Searching: 'The Hunger Games: Mockingjay ‚Äì Part 1' (2014) ‚úÖ Match found\n",
      "[744/755] Searching: 'The Hunger Games: Catching Fire' (2013) ‚úÖ Match found\n",
      "[743/755] Searching: 'Ballerina' (2025) ‚úÖ Match found\n",
      "[742/755] Searching: 'The Hunger Games' (2012) ‚úÖ Match found\n",
      "[741/755] Searching: 'Soldier Monika' (2024) ‚úÖ Match found\n",
      "[293/755] Searching: 'The Big Lebowski' (1998) ‚úÖ Match found\n",
      "[740/755] Searching: 'Eraserhead' (1977) ‚úÖ Match found\n",
      "[739/755] Searching: 'The Elephant Man' (1980) ‚úÖ Match found\n",
      "[738/755] Searching: 'David Lynch: The Art Life' (2016) ‚úÖ Match found\n",
      "[737/755] Searching: 'Sonic the Hedgehog 3' (2024) ‚úÖ Match found\n",
      "[735/755] Searching: 'The Straight Story' (1999) ‚úÖ Match found\n",
      "[734/755] Searching: 'Mulholland Drive' (2001) ‚úÖ Match found\n",
      "[733/755] Searching: 'Sonic the Hedgehog 2' (2022) ‚úÖ Match found\n",
      "[271/755] Searching: 'Sonic the Hedgehog' (2020) ‚úÖ Match found\n",
      "[144/755] Searching: 'How to Train Your Dragon' (2010) ‚úÖ Match found\n",
      "[139/755] Searching: 'John Wick: Chapter 2' (2017) ‚úÖ Match found\n",
      "[138/755] Searching: 'John Wick' (2014) ‚úÖ Match found\n",
      "[732/755] Searching: 'Deadpool & Wolverine' (2024) ‚úÖ Match found\n",
      "[730/755] Searching: 'Conclave' (2024) ‚úÖ Match found\n",
      "[729/755] Searching: 'Pawn Sacrifice' (2014) ‚úÖ Match found\n",
      "[728/755] Searching: 'AlphaGo' (2017) ‚úÖ Match found\n",
      "[725/755] Searching: 'I Saw the TV Glow' (2024) ‚úÖ Match found\n",
      "[724/755] Searching: 'Twin Peaks: The Return' (2017) ‚ùå No exact match found\n",
      "[722/755] Searching: 'Airplane!' (1980) ‚úÖ Match found\n",
      "[720/755] Searching: 'The Substance' (2024) ‚úÖ Match found\n",
      "[719/755] Searching: 'Twin Peaks: Fire Walk with Me' (1992) ‚úÖ Match found\n",
      "[717/755] Searching: 'Clueless' (1995) ‚úÖ Match found\n",
      "[344/755] Searching: 'Life of Brian' (1979) ‚úÖ Match found\n",
      "[481/755] Searching: 'Monty Python and the Holy Grail' (1975) ‚úÖ Match found\n",
      "[716/755] Searching: 'How to Train Your Dragon: The Hidden World' (2019) ‚úÖ Match found\n",
      "[145/755] Searching: 'How to Train Your Dragon 2' (2014) ‚úÖ Match found\n",
      "[99/755] Searching: 'Aliens' (1986) ‚úÖ Match found\n",
      "[707/755] Searching: 'Longlegs' (2024) ‚úÖ Match found\n",
      "[706/755] Searching: 'Sunshine Barry & the Disco Worms' (2008) ‚úÖ Match found\n",
      "[705/755] Searching: 'A Quiet Place: Day One' (2024) ‚úÖ Match found\n",
      "[704/755] Searching: 'Free Solo' (2018) ‚úÖ Match found\n",
      "[703/755] Searching: 'The Promised Land' (2023) ‚úÖ Match found\n",
      "[679/755] Searching: 'The Rescue' (2021) ‚úÖ Match found\n",
      "[702/755] Searching: 'Titane' (2021) ‚úÖ Match found\n",
      "[701/755] Searching: 'Love Lies Bleeding' (2024) ‚úÖ Match found\n",
      "[700/755] Searching: 'Summer Storm' (2004) ‚úÖ Match found\n",
      "[13/755] Searching: 'Robots' (2005) ‚úÖ Match found\n",
      "[264/755] Searching: 'Madagascar' (2005) ‚úÖ Match found\n",
      "[699/755] Searching: 'Chappie' (2015) ‚úÖ Match found\n",
      "[696/755] Searching: 'Mad Max 2' (1981) ‚úÖ Match found\n",
      "[697/755] Searching: 'Mad Max Beyond Thunderdome' (1985) ‚úÖ Match found\n",
      "[698/755] Searching: 'Mad Max: Fury Road' (2015) ‚úÖ Match found\n",
      "[695/755] Searching: 'Mad Max' (1979) ‚úÖ Match found\n",
      "[694/755] Searching: 'The Son of Kong' (1933) ‚úÖ Match found\n",
      "[693/755] Searching: 'King Kong' (1933) ‚úÖ Match found\n",
      "[692/755] Searching: 'Godzilla Minus One' (2023) ‚úÖ Match found\n",
      "[691/755] Searching: 'Godzilla √ó Kong: The New Empire' (2024) ‚úÖ Match found\n",
      "[690/755] Searching: 'Futurama: Into the Wild Green Yonder' (2009) ‚úÖ Match found\n",
      "[689/755] Searching: 'Futurama: Bender's Game' (2008) ‚úÖ Match found\n",
      "[688/755] Searching: 'Kill Bill: Vol. 2' (2004) ‚úÖ Match found\n",
      "[687/755] Searching: 'Futurama: The Beast with a Billion Backs' (2008) ‚úÖ Match found\n",
      "[686/755] Searching: 'Futurama: Bender's Big Score' (2007) ‚úÖ Match found\n",
      "[685/755] Searching: 'Kill Bill: Vol. 1' (2003) ‚úÖ Match found\n",
      "[684/755] Searching: 'Once Upon a Time... in Hollywood' (2019) ‚úÖ Match found\n",
      "[487/755] Searching: 'Inglourious Basterds' (2009) ‚úÖ Match found\n",
      "[148/755] Searching: 'Django Unchained' (2012) ‚úÖ Match found\n",
      "[650/755] Searching: 'Ed Wood' (1994) ‚úÖ Match found\n",
      "[683/755] Searching: 'Anatomy of a Fall' (2023) ‚úÖ Match found\n",
      "[682/755] Searching: 'Ricky Stanicky' (2024) ‚úÖ Match found\n",
      "[642/755] Searching: 'Jesus Christ Superstar' (1973) ‚úÖ Match found\n",
      "[681/755] Searching: 'Antiviral' (2012) ‚úÖ Match found\n",
      "[680/755] Searching: 'The House That Jack Built' (2018) ‚úÖ Match found\n",
      "[678/755] Searching: 'Sharknado 2: The Second One' (2014) ‚úÖ Match found\n",
      "[677/755] Searching: 'Sharknado' (2013) ‚úÖ Match found\n",
      "[676/755] Searching: 'Full Metal Jacket' (1987) ‚úÖ Match found\n",
      "[81/755] Searching: '2001: A Space Odyssey' (1968) ‚úÖ Match found\n",
      "[528/755] Searching: 'Everything Everywhere All at Once' (2022) ‚úÖ Match found\n",
      "[674/755] Searching: 'Marry My Dead Body' (2022) ‚úÖ Match found\n",
      "[673/755] Searching: 'Moxie' (2021) ‚úÖ Match found\n",
      "[672/755] Searching: 'Leo' (2023) ‚úÖ Match found\n",
      "[196/755] Searching: 'Brokeback Mountain' (2005) ‚úÖ Match found\n",
      "[671/755] Searching: 'The Wolf of Wall Street' (2013) ‚úÖ Match found\n",
      "[35/755] Searching: 'Fight Club' (1999) ‚úÖ Match found\n",
      "[670/755] Searching: 'The Land Before Time' (1988) ‚úÖ Match found\n",
      "[658/755] Searching: 'Howl's Moving Castle' (2004) ‚úÖ Match found\n",
      "[656/755] Searching: 'Snowpiercer' (2013) ‚úÖ Match found\n",
      "[655/755] Searching: 'The Pale Blue Eye' (2022) ‚úÖ Match found\n",
      "[654/755] Searching: 'The Prestige' (2006) ‚úÖ Match found\n",
      "[653/755] Searching: 'Glen or Glenda' (1953) ‚úÖ Match found\n",
      "[649/755] Searching: 'Shadow of the Vampire' (2000) ‚úÖ Match found\n",
      "[648/755] Searching: 'Renfield' (2023) ‚úÖ Match found\n",
      "[647/755] Searching: 'Dracula' (1931) ‚úÖ Match found\n",
      "[646/755] Searching: 'Paint Drying' (2016) ‚ùå No exact match found\n",
      "[644/755] Searching: 'Ford v Ferrari' (2019) ‚úÖ Match found\n",
      "[569/755] Searching: 'Bullet Train' (2022) ‚úÖ Match found\n",
      "[638/755] Searching: 'The Nice Guys' (2016) ‚úÖ Match found\n",
      "[639/755] Searching: 'Oppenheimer' (2023) ‚úÖ Match found\n",
      "[404/755] Searching: 'Ratatouille' (2007) ‚úÖ Match found\n",
      "[637/755] Searching: 'Hellbound: Hellraiser II' (1988) ‚úÖ Match found\n",
      "[635/755] Searching: 'The Death of Stalin' (2017) ‚úÖ Match found\n",
      "[634/755] Searching: 'Barbie' (2023) ‚úÖ Match found\n",
      "[633/755] Searching: 'Asteroid City' (2023) ‚úÖ Match found\n",
      "[631/755] Searching: 'National Theatre Live: Fleabag' (2019) ‚úÖ Match found\n",
      "[632/755] Searching: 'Little Women' (2019) ‚úÖ Match found\n",
      "[630/755] Searching: 'Lady Bird' (2017) ‚úÖ Match found\n",
      "[629/755] Searching: 'Nimona' (2023) ‚úÖ Match found\n",
      "[628/755] Searching: 'Spider-Man: Across the Spider-Verse' (2023) ‚úÖ Match found\n",
      "[65/755] Searching: 'The Darjeeling Limited' (2007) ‚úÖ Match found\n",
      "[118/755] Searching: 'Pacific Rim' (2013) ‚úÖ Match found\n",
      "[27/755] Searching: 'The Mitchells vs. the Machines' (2021) ‚úÖ Match found\n",
      "[626/755] Searching: 'Puppy Swap: Love Unleashed' (2019) ‚úÖ Match found\n",
      "[621/755] Searching: 'Chicken Run' (2000) ‚úÖ Match found\n",
      "[147/755] Searching: 'The Pirates! In an Adventure with Scientists!' (2012) ‚úÖ Match found\n",
      "[618/755] Searching: 'The Inbetweeners Movie' (2011) ‚úÖ Match found\n",
      "[617/755] Searching: 'Alice, Darling' (2022) ‚úÖ Match found\n",
      "[616/755] Searching: 'Mother's Day' (1993) ‚úÖ Match found\n",
      "[608/755] Searching: 'Aftersun' (2022) ‚úÖ Match found\n",
      "[607/755] Searching: 'Oldboy' (2003) ‚úÖ Match found\n",
      "[605/755] Searching: 'The Super Mario Bros. Movie' (2023) ‚úÖ Match found\n",
      "[604/755] Searching: 'Gamera: Guardian of the Universe' (1995) ‚úÖ Match found\n",
      "[598/755] Searching: 'Good Bye, Lenin!' (2003) ‚úÖ Match found\n",
      "[66/755] Searching: 'The Royal Tenenbaums' (2001) ‚úÖ Match found\n",
      "[478/755] Searching: 'About Time' (2013) ‚úÖ Match found\n",
      "[597/755] Searching: 'Princess Mononoke' (1997) ‚úÖ Match found\n",
      "[227/755] Searching: 'Spirited Away' (2001) ‚úÖ Match found\n",
      "[596/755] Searching: 'John Wick: Chapter 4' (2023) ‚úÖ Match found\n",
      "[116/755] Searching: 'The Shape of Water' (2017) ‚úÖ Match found\n",
      "[594/755] Searching: 'Gladiator' (2000) ‚úÖ Match found\n",
      "[591/755] Searching: 'Ponyo' (2008) ‚úÖ Match found\n",
      "[589/755] Searching: 'CODA' (2021) ‚úÖ Match found\n",
      "[587/755] Searching: 'All My Life' (2020) ‚úÖ Match found\n",
      "[176/755] Searching: 'Another Round' (2020) ‚úÖ Match found\n",
      "[586/755] Searching: 'The Banshees of Inisherin' (2022) ‚úÖ Match found\n",
      "[585/755] Searching: 'Ticket to Paradise' (2022) ‚úÖ Match found\n",
      "[401/755] Searching: 'Spider-Man: Into the Spider-Verse' (2018) ‚úÖ Match found\n",
      "[146/755] Searching: 'Good Omens' (2019) ‚úÖ Match found\n",
      "[580/755] Searching: 'The Menu' (2022) ‚úÖ Match found\n",
      "[579/755] Searching: 'Puss in Boots: The Last Wish' (2022) ‚úÖ Match found\n",
      "[577/755] Searching: 'Jumanji: Welcome to the Jungle' (2017) ‚úÖ Match found\n",
      "[576/755] Searching: 'Inside Man' (2022) ‚úÖ Match found\n",
      "[463/755] Searching: 'Pride' (2014) ‚úÖ Match found\n",
      "[575/755] Searching: 'Guillermo del Toro's Pinocchio' (2022) ‚úÖ Match found\n",
      "[574/755] Searching: 'Glass Onion' (2022) ‚ùå No exact match found\n",
      "[573/755] Searching: 'Avatar: The Way of Water' (2022) ‚úÖ Match found\n",
      "[413/755] Searching: 'Avatar' (2009) ‚úÖ Match found\n",
      "[267/755] Searching: 'Bee Movie' (2007) ‚úÖ Match found\n",
      "[570/755] Searching: 'Mamma Mia!' (2008) ‚úÖ Match found\n",
      "[568/755] Searching: 'Red Notice' (2021) ‚úÖ Match found\n",
      "[566/755] Searching: 'Hellraiser' (2022) ‚úÖ Match found\n",
      "[567/755] Searching: 'Midnight Mass' (2021) ‚úÖ Match found\n",
      "[563/755] Searching: 'The Drowning of Arthur Braxton' (2021) ‚úÖ Match found\n",
      "[562/755] Searching: 'The Imitation Game' (2014) ‚úÖ Match found\n",
      "[556/755] Searching: 'Nope' (2022) ‚úÖ Match found\n",
      "[555/755] Searching: 'Enola Holmes' (2020) ‚úÖ Match found\n",
      "[149/755] Searching: 'Inception' (2010) ‚úÖ Match found\n",
      "[552/755] Searching: 'Love, Death & Robots: The Drowned Giant' (2021) ‚ùå No exact match found\n",
      "[550/755] Searching: 'Love, Death & Robots: All Through the House' (2021) ‚ùå No exact match found\n",
      "[549/755] Searching: 'Love, Death & Robots: The Tall Grass' (2021) ‚ùå No exact match found\n",
      "[551/755] Searching: 'Love, Death & Robots: Life Hutch' (2021) ‚ùå No exact match found\n",
      "[548/755] Searching: 'Good Time' (2017) ‚úÖ Match found\n",
      "[538/755] Searching: 'Love, Death & Robots: Kill Team Kill' (2022) ‚ùå No exact match found\n",
      "[536/755] Searching: 'Love, Death & Robots: The Very Pulse of the Machine' (2022) ‚ùå No exact match found\n",
      "[537/755] Searching: 'Love, Death & Robots: Night of the Mini Dead' (2022) ‚ùå No exact match found\n",
      "[541/755] Searching: 'Love, Death & Robots: In Vaulted Halls Entombed' (2022) ‚ùå No exact match found\n",
      "[539/755] Searching: 'Love, Death & Robots: Swarm' (2022) ‚ùå No exact match found\n",
      "[540/755] Searching: 'Love, Death & Robots: Mason's Rats' (2022) ‚ùå No exact match found\n",
      "[542/755] Searching: 'Love, Death & Robots: Jibaro' (2022) ‚ùå No exact match found\n",
      "[534/755] Searching: 'Love, Death & Robots: Three Robots: Exit Strategies' (2022) ‚ùå No exact match found\n",
      "[544/755] Searching: 'Love, Death & Robots: Automated Customer Service' (2021) ‚ùå No exact match found\n",
      "[535/755] Searching: 'Love, Death & Robots: Bad Travelling' (2022) ‚ùå No exact match found\n",
      "[545/755] Searching: 'Love, Death & Robots: Ice' (2021) ‚ùå No exact match found\n",
      "[546/755] Searching: 'Love, Death & Robots: Pop Squad' (2021) ‚ùå No exact match found\n",
      "[547/755] Searching: 'Love, Death & Robots: Snow in the Desert' (2021) ‚ùå No exact match found\n",
      "[533/755] Searching: 'Thirteen Lives' (2022) ‚úÖ Match found\n",
      "[532/755] Searching: 'Prey' (2022) ‚úÖ Match found\n",
      "[527/755] Searching: 'Raw' (2016) ‚úÖ Match found\n",
      "[523/755] Searching: 'Battle at Big Rock' (2019) ‚úÖ Match found\n",
      "[183/755] Searching: 'The Blair Witch Project' (1999) ‚úÖ Match found\n",
      "[522/755] Searching: 'RRR' (2022) ‚úÖ Match found\n",
      "[248/755] Searching: 'Pirates of the Caribbean: At World's End' (2007) ‚úÖ Match found\n",
      "[247/755] Searching: 'Pirates of the Caribbean: Dead Man's Chest' (2006) ‚úÖ Match found\n",
      "[246/755] Searching: 'Pirates of the Caribbean: The Curse of the Black Pearl' (2003) ‚úÖ Match found\n",
      "[505/755] Searching: 'It's Only the End of the World' (2016) ‚úÖ Match found\n",
      "[502/755] Searching: 'Jurassic World Dominion' (2022) ‚úÖ Match found\n",
      "[501/755] Searching: 'The Sea Beast' (2022) ‚úÖ Match found\n",
      "[495/755] Searching: 'Pretty Woman' (1990) ‚úÖ Match found\n",
      "[492/755] Searching: 'Moon Knight' (2022) ‚úÖ Match found\n",
      "[34/755] Searching: 'The Grand Budapest Hotel' (2014) ‚úÖ Match found\n",
      "[410/755] Searching: 'Crimes of the Future' (2022) ‚úÖ Match found\n",
      "[380/755] Searching: 'In Bruges' (2008) ‚úÖ Match found\n",
      "[373/755] Searching: 'Hustle' (2022) ‚úÖ Match found\n",
      "[365/755] Searching: 'Paddington 2' (2017) ‚úÖ Match found\n",
      "[364/755] Searching: 'Paddington' (2014) ‚úÖ Match found\n",
      "[362/755] Searching: 'Christopher Robin' (2018) ‚úÖ Match found\n",
      "[355/755] Searching: 'Hellraiser' (1987) ‚úÖ Match found\n",
      "[354/755] Searching: 'Trainspotting' (1996) ‚úÖ Match found\n",
      "[334/755] Searching: 'Bo Burnham: The Inside Outtakes' (2022) ‚úÖ Match found\n",
      "[303/755] Searching: 'Prehistoric Planet' (2022) ‚úÖ Match found\n",
      "[301/755] Searching: 'Sherlock: The Final Problem' (2017) ‚ùå No exact match found\n",
      "[300/755] Searching: 'Sherlock: The Lying Detective' (2017) ‚ùå No exact match found\n",
      "[299/755] Searching: 'Sherlock: The Six Thatchers' (2017) ‚ùå No exact match found\n",
      "[298/755] Searching: 'Sherlock: The Abominable Bride' (2016) ‚úÖ Match found\n",
      "[296/755] Searching: 'Sherlock: His Last Vow' (2014) ‚ùå No exact match found\n",
      "[295/755] Searching: 'Sherlock: The Sign of Three' (2014) ‚ùå No exact match found\n",
      "[294/755] Searching: 'Sherlock: The Empty Hearse' (2014) ‚ùå No exact match found\n",
      "[292/755] Searching: 'Sherlock: The Reichenbach Fall' (2012) ‚ùå No exact match found\n",
      "[282/755] Searching: 'Sherlock: The Hounds of Baskerville' (2012) ‚ùå No exact match found\n",
      "[280/755] Searching: 'Sherlock: The Great Game' (2010) ‚ùå No exact match found\n",
      "[279/755] Searching: 'Sherlock: The Blind Banker' (2010) ‚ùå No exact match found\n",
      "[278/755] Searching: 'Sherlock: A Study in Pink' (2010) ‚ùå No exact match found\n",
      "[245/755] Searching: 'A Cure for Wellness' (2016) ‚úÖ Match found\n",
      "[242/755] Searching: 'Tusk' (2014) ‚úÖ Match found\n",
      "[241/755] Searching: 'In My Skin' (2002) ‚úÖ Match found\n",
      "[240/755] Searching: 'The Suicide Squad' (2021) ‚úÖ Match found\n",
      "[239/755] Searching: 'Zack Snyder's Justice League' (2021) ‚úÖ Match found\n",
      "[228/755] Searching: 'What We Do in the Shadows' (2014) ‚úÖ Match found\n",
      "[237/755] Searching: 'The Green Mile' (1999) ‚úÖ Match found\n",
      "[236/755] Searching: 'Hunt for the Wilderpeople' (2016) ‚úÖ Match found\n",
      "[235/755] Searching: 'Life of Pi' (2012) ‚úÖ Match found\n",
      "[230/755] Searching: 'Bo Burnham: What.' (2013) ‚úÖ Match found\n",
      "[229/755] Searching: 'Wonder Woman 1984' (2020) ‚úÖ Match found\n",
      "[231/755] Searching: 'Bo Burnham: Make Happy' (2016) ‚úÖ Match found\n",
      "[225/755] Searching: 'Fright Night' (2011) ‚úÖ Match found\n",
      "[224/755] Searching: 'Birds of Prey (and the Fantabulous Emancipation of One Harley Quinn)' (2020) ‚úÖ Match found\n",
      "[223/755] Searching: 'Shazam!' (2019) ‚úÖ Match found\n",
      "[221/755] Searching: 'The Batman' (2022) ‚úÖ Match found\n",
      "[220/755] Searching: 'Aquaman' (2018) ‚úÖ Match found\n",
      "[214/755] Searching: 'Justice League' (2017) ‚úÖ Match found\n",
      "[9/755] Searching: 'Little Miss Sunshine' (2006) ‚úÖ Match found\n",
      "[212/755] Searching: 'Wonder Woman' (2017) ‚úÖ Match found\n",
      "[201/755] Searching: 'Suicide Squad' (2016) ‚úÖ Match found\n",
      "[200/755] Searching: 'Batman v Superman: Dawn of Justice' (2016) ‚úÖ Match found\n",
      "[199/755] Searching: 'Man of Steel' (2013) ‚úÖ Match found\n",
      "[175/755] Searching: 'The Conference' (2022) ‚úÖ Match found\n",
      "[171/755] Searching: 'Antlers' (2021) ‚úÖ Match found\n",
      "[153/755] Searching: 'The Amazing Spider-Man' (2012) ‚úÖ Match found\n",
      "[141/755] Searching: 'Encanto' (2021) ‚úÖ Match found\n",
      "[134/755] Searching: 'Spider-Man 3' (2007) ‚úÖ Match found\n",
      "[133/755] Searching: 'Spider-Man 2' (2004) ‚úÖ Match found\n",
      "[132/755] Searching: 'Spider-Man' (2002) ‚úÖ Match found\n",
      "[131/755] Searching: 'Coco' (2017) ‚úÖ Match found\n",
      "[130/755] Searching: 'Making The Witcher: Season 2' (2021) ‚úÖ Match found\n",
      "[129/755] Searching: 'The Witcher Season One Recap: From the Beginning' (2021) ‚úÖ Match found\n",
      "[128/755] Searching: 'The Witcher: Nightmare of the Wolf' (2021) ‚úÖ Match found\n",
      "[126/755] Searching: 'The Social Network' (2010) ‚úÖ Match found\n",
      "[124/755] Searching: 'Adam Sandler: 100% Fresh' (2018) ‚úÖ Match found\n",
      "[115/755] Searching: 'Krampus' (2015) ‚úÖ Match found\n",
      "[94/755] Searching: 'The Tomorrow War' (2021) ‚úÖ Match found\n",
      "[93/755] Searching: 'tick, tick... BOOM!' (2021) ‚úÖ Match found\n",
      "[87/755] Searching: 'American Psycho' (2000) ‚úÖ Match found\n",
      "[83/755] Searching: 'Arrival' (2016) ‚úÖ Match found\n",
      "[86/755] Searching: 'The Lego Batman Movie' (2017) ‚úÖ Match found\n",
      "[85/755] Searching: 'We're the Millers' (2013) ‚úÖ Match found\n",
      "[80/755] Searching: 'The French Dispatch' (2021) ‚úÖ Match found\n",
      "[79/755] Searching: 'Love and Monsters' (2020) ‚úÖ Match found\n",
      "[78/755] Searching: 'War of the Worlds' (2005) ‚úÖ Match found\n",
      "[77/755] Searching: 'Hinterland' (2021) ‚úÖ Match found\n",
      "[76/755] Searching: 'Megamind' (2010) ‚úÖ Match found\n",
      "[75/755] Searching: 'Venom: Let There Be Carnage' (2021) ‚úÖ Match found\n",
      "[74/755] Searching: 'Venom' (2018) ‚úÖ Match found\n",
      "[71/755] Searching: 'Moonrise Kingdom' (2012) ‚úÖ Match found\n",
      "[73/755] Searching: 'Bottle Rocket' (1996) ‚úÖ Match found\n",
      "[72/755] Searching: 'Rushmore' (1998) ‚úÖ Match found\n",
      "[70/755] Searching: 'Dune' (2021) ‚úÖ Match found\n",
      "[69/755] Searching: 'Free Guy' (2021) ‚úÖ Match found\n",
      "[67/755] Searching: 'Hereditary' (2018) ‚úÖ Match found\n",
      "[61/755] Searching: 'Fantastic Mr. Fox' (2009) ‚úÖ Match found\n",
      "[60/755] Searching: 'The Fast and the Furious' (2001) ‚úÖ Match found\n",
      "[59/755] Searching: 'The Big Short' (2015) ‚úÖ Match found\n",
      "[58/755] Searching: 'Zootopia' (2016) ‚úÖ Match found\n",
      "[57/755] Searching: 'Bolt' (2008) ‚úÖ Match found\n",
      "[56/755] Searching: 'Neon Genesis Evangelion: The End of Evangelion' (1997) ‚úÖ Match found\n",
      "[36/755] Searching: 'Luca' (2021) ‚úÖ Match found\n",
      "[55/755] Searching: 'Neon Genesis Evangelion' (1995) ‚úÖ Match found\n",
      "[54/755] Searching: 'Tarzan' (1999) ‚úÖ Match found\n",
      "[53/755] Searching: 'Monster Hunter' (2020) ‚úÖ Match found\n",
      "[52/755] Searching: 'The Jungle Book' (2016) ‚úÖ Match found\n",
      "[51/755] Searching: 'The Lobster' (2015) ‚úÖ Match found\n",
      "[50/755] Searching: 'Jaws' (1975) ‚úÖ Match found\n",
      "[49/755] Searching: 'The Life Aquatic with Steve Zissou' (2004) ‚úÖ Match found\n",
      "[48/755] Searching: 'One Hour Photo' (2002) ‚úÖ Match found\n",
      "[47/755] Searching: 'The Village' (2004) ‚úÖ Match found\n",
      "[46/755] Searching: 'Starship Troopers' (1997) ‚úÖ Match found\n",
      "[45/755] Searching: 'Signs' (2002) ‚úÖ Match found\n",
      "[44/755] Searching: 'Kung Fu Panda 3' (2016) ‚úÖ Match found\n",
      "[43/755] Searching: 'Whiplash' (2014) ‚úÖ Match found\n",
      "[42/755] Searching: 'A Quiet Place Part II' (2020) ‚úÖ Match found\n",
      "[41/755] Searching: 'Kung Fu Panda 2' (2011) ‚úÖ Match found\n",
      "[40/755] Searching: 'Ex Machina' (2015) ‚úÖ Match found\n",
      "[39/755] Searching: 'Scott Pilgrim vs. the World' (2010) ‚úÖ Match found\n",
      "[38/755] Searching: 'Kung Fu Panda' (2008) ‚úÖ Match found\n",
      "[37/755] Searching: 'Fear Street: 1994' (2021) ‚úÖ Match found\n",
      "[33/755] Searching: 'Isle of Dogs' (2018) ‚úÖ Match found\n",
      "[31/755] Searching: 'Bo Burnham: Inside' (2021) ‚úÖ Match found\n",
      "[32/755] Searching: 'Army of the Dead' (2021) ‚úÖ Match found\n",
      "[30/755] Searching: 'Uncut Gems' (2019) ‚úÖ Match found\n",
      "[29/755] Searching: 'Steve Jobs' (2015) ‚úÖ Match found\n",
      "[28/755] Searching: 'Coraline' (2009) ‚úÖ Match found\n",
      "[26/755] Searching: 'Bram Stoker's Dracula' (1992) ‚úÖ Match found\n",
      "[25/755] Searching: 'Rio' (2011) ‚úÖ Match found\n",
      "[24/755] Searching: 'The Road to El Dorado' (2000) ‚úÖ Match found\n",
      "[23/755] Searching: 'The Cable Guy' (1996) ‚úÖ Match found\n",
      "[22/755] Searching: 'The King of Staten Island' (2020) ‚úÖ Match found\n",
      "[21/755] Searching: 'Saw' (2004) ‚úÖ Match found\n",
      "[20/755] Searching: 'Hostel' (2005) ‚úÖ Match found\n",
      "[19/755] Searching: 'Hamilton' (2020) ‚úÖ Match found\n",
      "[18/755] Searching: 'Mrs. Doubtfire' (1993) ‚úÖ Match found\n",
      "[17/755] Searching: 'The Cloverfield Paradox' (2018) ‚úÖ Match found\n",
      "[16/755] Searching: '10 Cloverfield Lane' (2016) ‚úÖ Match found\n",
      "[15/755] Searching: 'Cloverfield' (2008) ‚úÖ Match found\n",
      "[14/755] Searching: 'Da 5 Bloods' (2020) ‚úÖ Match found\n",
      "[12/755] Searching: 'Dead Poets Society' (1989) ‚úÖ Match found\n",
      "[11/755] Searching: 'The Breakfast Club' (1985) ‚úÖ Match found\n",
      "[10/755] Searching: 'Easy A' (2010) ‚úÖ Match found\n",
      "[7/755] Searching: 'Jojo Rabbit' (2019) ‚úÖ Match found\n",
      "[6/755] Searching: 'Knives Out' (2019) ‚úÖ Match found\n",
      "[8/755] Searching: 'Event Horizon' (1997) ‚úÖ Match found\n",
      "[5/755] Searching: 'Children of Men' (2006) ‚úÖ Match found\n",
      "[3/755] Searching: 'Marriage Story' (2019) ‚úÖ Match found\n",
      "[4/755] Searching: 'Zodiac' (2007) ‚úÖ Match found\n",
      "[2/755] Searching: 'The Meyerowitz Stories (New and Selected)' (2017) ‚úÖ Match found\n",
      "[1/755] Searching: 'Bird Box' (2018) ‚úÖ Match found\n",
      "[125/755] Searching: 'Frankenweenie' (2012) ‚úÖ Match found\n",
      "[62/755] Searching: 'The Lord of the Rings: The Fellowship of the Ring' (2001) ‚úÖ Match found\n",
      "[63/755] Searching: 'The Lord of the Rings: The Two Towers' (2002) ‚úÖ Match found\n",
      "[64/755] Searching: 'The Lord of the Rings: The Return of the King' (2003) ‚úÖ Match found\n",
      "[68/755] Searching: 'Lost in Translation' (2003) ‚úÖ Match found\n",
      "[82/755] Searching: 'First Man' (2018) ‚úÖ Match found\n",
      "[84/755] Searching: 'Annihilation' (2018) ‚úÖ Match found\n",
      "[88/755] Searching: 'The Fundamentals of Caring' (2016) ‚úÖ Match found\n",
      "[90/755] Searching: 'Get Out' (2017) ‚úÖ Match found\n",
      "[91/755] Searching: 'The Cabin in the Woods' (2011) ‚úÖ Match found\n",
      "[92/755] Searching: 'Godzilla: King of the Monsters' (2019) ‚úÖ Match found\n",
      "[95/755] Searching: 'Dawn of the Planet of the Apes' (2014) ‚úÖ Match found\n",
      "[96/755] Searching: 'War for the Planet of the Apes' (2017) ‚úÖ Match found\n",
      "[97/755] Searching: 'Planet of the Apes' (1968) ‚úÖ Match found\n",
      "[98/755] Searching: 'Rise of the Planet of the Apes' (2011) ‚úÖ Match found\n",
      "[101/755] Searching: 'Blade Runner 2049' (2017) ‚úÖ Match found\n",
      "[102/755] Searching: 'Blade Runner' (1982) ‚úÖ Match found\n",
      "[103/755] Searching: 'Predator' (1987) ‚úÖ Match found\n",
      "[104/755] Searching: 'Alien¬≥' (1992) ‚úÖ Match found\n",
      "[105/755] Searching: 'Predators' (2010) ‚úÖ Match found\n",
      "[106/755] Searching: 'The Predator' (2018) ‚úÖ Match found\n",
      "[107/755] Searching: 'Aliens vs Predator: Requiem' (2007) ‚úÖ Match found\n",
      "[108/755] Searching: 'Alien: Covenant' (2017) ‚úÖ Match found\n",
      "[110/755] Searching: 'Prometheus' (2012) ‚úÖ Match found\n",
      "[111/755] Searching: 'Predator 2' (1990) ‚úÖ Match found\n",
      "[112/755] Searching: 'Alien Resurrection' (1997) ‚úÖ Match found\n",
      "[113/755] Searching: 'Captain America: The First Avenger' (2011) ‚úÖ Match found\n",
      "[114/755] Searching: 'Surf's Up' (2007) ‚úÖ Match found\n",
      "[117/755] Searching: 'Pan's Labyrinth' (2006) ‚úÖ Match found\n",
      "[119/755] Searching: 'Crimson Peak' (2015) ‚úÖ Match found\n",
      "[120/755] Searching: 'Hellboy' (2004) ‚úÖ Match found\n",
      "[121/755] Searching: 'Hellboy II: The Golden Army' (2008) ‚úÖ Match found\n",
      "[122/755] Searching: 'The Devil's Backbone' (2001) ‚úÖ Match found\n",
      "[123/755] Searching: 'Pacific Rim: Uprising' (2018) ‚úÖ Match found\n",
      "[127/755] Searching: 'Making The Witcher' (2020) ‚úÖ Match found\n",
      "[135/755] Searching: 'The Lighthouse' (2019) ‚úÖ Match found\n",
      "[136/755] Searching: 'Finding Nemo' (2003) ‚úÖ Match found\n",
      "[137/755] Searching: 'Finding Dory' (2016) ‚úÖ Match found\n",
      "[140/755] Searching: 'John Wick: Chapter 3 ‚Äì Parabellum' (2019) ‚úÖ Match found\n",
      "[142/755] Searching: 'Shin Godzilla' (2016) ‚úÖ Match found\n",
      "[143/755] Searching: 'Titanic' (1997) ‚úÖ Match found\n",
      "[150/755] Searching: 'The Revenant' (2015) ‚úÖ Match found\n",
      "[151/755] Searching: 'Insidious: Chapter 2' (2013) ‚úÖ Match found\n",
      "[152/755] Searching: 'Insidious' (2010) ‚úÖ Match found\n",
      "[154/755] Searching: 'Harry Potter and the Philosopher's Stone' (2001) ‚úÖ Match found\n",
      "[155/755] Searching: 'Harry Potter and the Chamber of Secrets' (2002) ‚úÖ Match found\n",
      "[156/755] Searching: 'Harry Potter and the Goblet of Fire' (2005) ‚úÖ Match found\n",
      "[157/755] Searching: 'Harry Potter and the Deathly Hallows: Part 2' (2011) ‚úÖ Match found\n",
      "[158/755] Searching: 'Harry Potter and the Order of the Phoenix' (2007) ‚úÖ Match found\n",
      "[159/755] Searching: 'Harry Potter and the Deathly Hallows: Part 1' (2010) ‚úÖ Match found\n",
      "[160/755] Searching: 'Harry Potter and the Half-Blood Prince' (2009) ‚úÖ Match found\n",
      "[161/755] Searching: 'Harry Potter and the Prisoner of Azkaban' (2004) ‚úÖ Match found\n",
      "[162/755] Searching: 'Birdman or (The Unexpected Virtue of Ignorance)' (2014) ‚úÖ Match found\n",
      "[163/755] Searching: 'The Fly' (1986) ‚úÖ Match found\n",
      "[164/755] Searching: 'The Thing' (1982) ‚úÖ Match found\n",
      "[165/755] Searching: 'In the Mouth of Madness' (1994) ‚úÖ Match found\n",
      "[166/755] Searching: 'Prince of Darkness' (1987) ‚úÖ Match found\n",
      "[167/755] Searching: 'Re-Animator' (1985) ‚úÖ Match found\n",
      "[168/755] Searching: 'Bride of Re-Animator' (1990) ‚úÖ Match found\n",
      "[169/755] Searching: 'It' (2017) ‚úÖ Match found\n",
      "[170/755] Searching: 'It Chapter Two' (2019) ‚úÖ Match found\n",
      "[172/755] Searching: 'The House' (2022) ‚úÖ Match found\n",
      "[173/755] Searching: 'The Terminator' (1984) ‚úÖ Match found\n",
      "[174/755] Searching: 'Terminator 2: Judgment Day' (1991) ‚úÖ Match found\n",
      "[177/755] Searching: 'The Witch' (2015) ‚úÖ Match found\n",
      "[178/755] Searching: 'Veronica' (2017) ‚úÖ Match found\n",
      "[179/755] Searching: 'The Silence of the Lambs' (1991) ‚úÖ Match found\n",
      "[180/755] Searching: 'Midsommar' (2019) ‚úÖ Match found\n",
      "[181/755] Searching: 'Scary Stories to Tell in the Dark' (2019) ‚úÖ Match found\n",
      "[182/755] Searching: 'The Exorcist' (1973) ‚úÖ Match found\n",
      "[184/755] Searching: 'The Ritual' (2017) ‚úÖ Match found\n",
      "[185/755] Searching: 'The Meg' (2018) ‚úÖ Match found\n",
      "[186/755] Searching: 'It Comes at Night' (2017) ‚úÖ Match found\n",
      "[187/755] Searching: '28 Days Later' (2002) ‚úÖ Match found\n",
      "[188/755] Searching: 'The Hunt' (2012) ‚úÖ Match found\n",
      "[189/755] Searching: 'Memento' (2000) ‚úÖ Match found\n",
      "[190/755] Searching: 'Batman Begins' (2005) ‚úÖ Match found\n",
      "[191/755] Searching: 'The Dark Knight' (2008) ‚úÖ Match found\n",
      "[192/755] Searching: 'The Dark Knight Rises' (2012) ‚úÖ Match found\n",
      "[193/755] Searching: 'Interstellar' (2014) ‚úÖ Match found\n",
      "[194/755] Searching: 'Dunkirk' (2017) ‚úÖ Match found\n",
      "[195/755] Searching: 'Up' (2009) ‚úÖ Match found\n",
      "[197/755] Searching: 'Parasite' (2019) ‚úÖ Match found\n",
      "[198/755] Searching: 'Prisoners' (2013) ‚úÖ Match found\n",
      "[202/755] Searching: 'Star Wars' (1977) ‚úÖ Match found\n",
      "[203/755] Searching: 'The Empire Strikes Back' (1980) ‚úÖ Match found\n",
      "[204/755] Searching: 'Star Wars: The Last Jedi' (2017) ‚úÖ Match found\n",
      "[205/755] Searching: 'Return of the Jedi' (1983) ‚úÖ Match found\n",
      "[206/755] Searching: 'Star Wars: The Force Awakens' (2015) ‚úÖ Match found\n",
      "[207/755] Searching: 'Rogue One: A Star Wars Story' (2016) ‚úÖ Match found\n",
      "[208/755] Searching: 'Star Wars: Episode I ‚Äì The Phantom Menace' (1999) ‚úÖ Match found\n",
      "[209/755] Searching: 'Star Wars: Episode II ‚Äì¬†Attack of the Clones' (2002) ‚úÖ Match found\n",
      "[210/755] Searching: 'Star Wars: The Rise of Skywalker' (2019) ‚úÖ Match found\n",
      "[211/755] Searching: 'Star Wars: Episode III ‚Äì¬†Revenge of the Sith' (2005) ‚úÖ Match found\n",
      "[213/755] Searching: 'The SpongeBob SquarePants Movie' (2004) ‚úÖ Match found\n",
      "[215/755] Searching: 'Chernobyl' (2019) ‚úÖ Match found\n",
      "[216/755] Searching: 'Hannah Gadsby: Douglas' (2020) ‚úÖ Match found\n",
      "[217/755] Searching: 'Hannah Gadsby: Nanette' (2018) ‚úÖ Match found\n",
      "[218/755] Searching: 'Velvet Buzzsaw' (2019) ‚úÖ Match found\n",
      "[219/755] Searching: 'The Void' (2016) ‚úÖ Match found\n",
      "[222/755] Searching: 'Videodrome' (1983) ‚úÖ Match found\n",
      "[226/755] Searching: 'Mary and Max' (2009) ‚úÖ Match found\n",
      "[232/755] Searching: 'Kingsman: The Golden Circle' (2017) ‚úÖ Match found\n",
      "[233/755] Searching: 'Kingsman: The Secret Service' (2014) ‚úÖ Match found\n",
      "[234/755] Searching: 'Horns' (2013) ‚úÖ Match found\n",
      "[238/755] Searching: 'Color Out of Space' (2019) ‚úÖ Match found\n",
      "[243/755] Searching: 'Five Feet Apart' (2019) ‚úÖ Match found\n",
      "[244/755] Searching: 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb' (1964) ‚úÖ Match found\n",
      "[249/755] Searching: 'Rango' (2011) ‚úÖ Match found\n",
      "[250/755] Searching: 'Groundhog Day' (1993) ‚úÖ Match found\n",
      "[251/755] Searching: 'Shrek' (2001) ‚úÖ Match found\n",
      "[252/755] Searching: 'Shrek 2' (2004) ‚úÖ Match found\n",
      "[253/755] Searching: 'Shrek the Third' (2007) ‚úÖ Match found\n",
      "[254/755] Searching: 'Tangled' (2010) ‚úÖ Match found\n",
      "[255/755] Searching: 'The Emperor's New Groove' (2000) ‚úÖ Match found\n",
      "[256/755] Searching: 'Ice Age' (2002) ‚úÖ Match found\n",
      "[257/755] Searching: 'Ice Age: The Meltdown' (2006) ‚úÖ Match found\n",
      "[258/755] Searching: 'Ice Age: Dawn of the Dinosaurs' (2009) ‚úÖ Match found\n",
      "[259/755] Searching: 'Ice Age: Continental Drift' (2012) ‚úÖ Match found\n",
      "[260/755] Searching: 'Over the Hedge' (2006) ‚úÖ Match found\n",
      "[261/755] Searching: 'Monsters, Inc.' (2001) ‚úÖ Match found\n",
      "[262/755] Searching: 'The Croods' (2013) ‚úÖ Match found\n",
      "[263/755] Searching: 'The Secret Life of Pets' (2016) ‚úÖ Match found\n",
      "[265/755] Searching: 'Madagascar: Escape 2 Africa' (2008) ‚úÖ Match found\n",
      "[266/755] Searching: 'Madagascar 3: Europe's Most Wanted' (2012) ‚úÖ Match found\n",
      "[268/755] Searching: 'Horton Hears a Who!' (2008) ‚úÖ Match found\n",
      "[269/755] Searching: 'Shark Tale' (2004) ‚úÖ Match found\n",
      "[270/755] Searching: 'Penguins of Madagascar' (2014) ‚úÖ Match found\n",
      "[272/755] Searching: 'Minions' (2015) ‚úÖ Match found\n",
      "[273/755] Searching: 'Despicable Me' (2010) ‚úÖ Match found\n",
      "[274/755] Searching: 'Men in Black 3' (2012) ‚úÖ Match found\n",
      "[275/755] Searching: 'Men in Black' (1997) ‚úÖ Match found\n",
      "[276/755] Searching: 'Men in Black II' (2002) ‚úÖ Match found\n",
      "[277/755] Searching: 'Cats' (2019) ‚úÖ Match found\n",
      "[281/755] Searching: 'Sherlock: A Scandal in Belgravia' (2012) ‚ùå No exact match found\n",
      "[283/755] Searching: 'The Hobbit: An Unexpected Journey' (2012) ‚úÖ Match found\n",
      "[284/755] Searching: 'The Hobbit: The Desolation of Smaug' (2013) ‚úÖ Match found\n",
      "[285/755] Searching: 'The Hobbit: The Battle of the Five Armies' (2014) ‚úÖ Match found\n",
      "[286/755] Searching: 'Doctor Strange' (2016) ‚úÖ Match found\n",
      "[287/755] Searching: 'Avengers: Infinity War' (2018) ‚úÖ Match found\n",
      "[288/755] Searching: 'Avengers: Endgame' (2019) ‚úÖ Match found\n",
      "[289/755] Searching: 'Thor: Ragnarok' (2017) ‚úÖ Match found\n",
      "[290/755] Searching: 'Walking with Dinosaurs' (2013) ‚úÖ Match found\n",
      "[291/755] Searching: 'Dinosaur' (2000) ‚úÖ Match found\n",
      "[297/755] Searching: 'Dracula' (2020) ‚úÖ Match found\n",
      "[302/755] Searching: 'Downfalls High' (2021) ‚úÖ Match found\n",
      "[304/755] Searching: 'Alien Worlds' (2020) ‚úÖ Match found\n",
      "[305/755] Searching: 'Life' (2017) ‚úÖ Match found\n",
      "[306/755] Searching: 'Slither' (2006) ‚úÖ Match found\n",
      "[307/755] Searching: 'Jurassic Park' (1993) ‚úÖ Match found\n",
      "[308/755] Searching: 'Jurassic World' (2015) ‚úÖ Match found\n",
      "[309/755] Searching: 'The Lost World: Jurassic Park' (1997) ‚úÖ Match found\n",
      "[310/755] Searching: 'Jurassic Park III' (2001) ‚úÖ Match found\n",
      "[311/755] Searching: 'Jurassic World: Fallen Kingdom' (2018) ‚úÖ Match found\n",
      "[312/755] Searching: 'King Kong' (2005) ‚úÖ Match found\n",
      "[313/755] Searching: 'Journey to the Center of the Earth' (2008) ‚úÖ Match found\n",
      "[314/755] Searching: 'Kong: Skull Island' (2017) ‚úÖ Match found\n",
      "[315/755] Searching: 'Godzilla' (2014) ‚úÖ Match found\n",
      "[316/755] Searching: 'Good Will Hunting' (1997) ‚úÖ Match found\n",
      "[317/755] Searching: 'Night at the Museum' (2006) ‚úÖ Match found\n",
      "[318/755] Searching: 'Night at the Museum: Battle of the Smithsonian' (2009) ‚úÖ Match found\n",
      "[319/755] Searching: 'Patch Adams' (1998) ‚úÖ Match found\n",
      "[320/755] Searching: 'Joker' (2019) ‚úÖ Match found\n",
      "[321/755] Searching: 'Fat Albert' (2004) ‚úÖ Match found\n",
      "[322/755] Searching: 'Hubie Halloween' (2020) ‚úÖ Match found\n",
      "[323/755] Searching: 'Popstar: Never Stop Never Stopping' (2016) ‚úÖ Match found\n",
      "[324/755] Searching: 'Cloudy with a Chance of Meatballs' (2009) ‚úÖ Match found\n",
      "[325/755] Searching: 'Hotel Transylvania' (2012) ‚úÖ Match found\n",
      "[326/755] Searching: 'Grown Ups 2' (2013) ‚úÖ Match found\n",
      "[327/755] Searching: 'Grown Ups' (2010) ‚úÖ Match found\n",
      "[328/755] Searching: 'Jack and Jill' (2011) ‚úÖ Match found\n",
      "[329/755] Searching: 'That's My Boy' (2012) ‚úÖ Match found\n",
      "[330/755] Searching: 'The Watch' (2012) ‚úÖ Match found\n",
      "[331/755] Searching: 'Pixels' (2015) ‚úÖ Match found\n",
      "[332/755] Searching: 'You Don't Mess with the Zohan' (2008) ‚úÖ Match found\n",
      "[333/755] Searching: 'Bedtime Stories' (2008) ‚úÖ Match found\n",
      "[335/755] Searching: 'Seven Psychopaths' (2012) ‚úÖ Match found\n",
      "[336/755] Searching: 'Cube' (1997) ‚úÖ Match found\n",
      "[337/755] Searching: 'It Follows' (2014) ‚úÖ Match found\n",
      "[338/755] Searching: 'Godzilla' (1954) ‚úÖ Match found\n",
      "[339/755] Searching: 'Godzilla: The Planet Eater' (2018) ‚úÖ Match found\n",
      "[340/755] Searching: 'Godzilla: City on the Edge of Battle' (2018) ‚úÖ Match found\n",
      "[341/755] Searching: 'Godzilla: Planet of the Monsters' (2017) ‚úÖ Match found\n",
      "[342/755] Searching: 'Godzilla vs. Kong' (2021) ‚úÖ Match found\n",
      "[343/755] Searching: 'Pirates of the Caribbean: On Stranger Tides' (2011) ‚úÖ Match found\n",
      "[345/755] Searching: 'Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan' (2006) ‚úÖ Match found\n",
      "[346/755] Searching: 'Borat Subsequent Moviefilm' (2020) ‚úÖ Match found\n",
      "[347/755] Searching: 'The Dictator' (2012) ‚úÖ Match found\n",
      "[348/755] Searching: 'The Interview' (2014) ‚úÖ Match found\n",
      "[349/755] Searching: 'Sausage Party' (2016) ‚úÖ Match found\n",
      "[350/755] Searching: 'A Million Ways to Die in the West' (2014) ‚úÖ Match found\n",
      "[351/755] Searching: 'Anchorman 2: The Legend Continues' (2013) ‚úÖ Match found\n",
      "[352/755] Searching: 'Anchorman: The Legend of Ron Burgundy' (2004) ‚úÖ Match found\n",
      "[353/755] Searching: 'Christiane F.' (1981) ‚úÖ Match found\n",
      "[356/755] Searching: 'The Autopsy of Jane Doe' (2016) ‚úÖ Match found\n",
      "[357/755] Searching: 'Aladdin' (1992) ‚úÖ Match found\n",
      "[358/755] Searching: 'The Lion King' (1994) ‚úÖ Match found\n",
      "[359/755] Searching: 'The Lion King II: Simba's Pride' (1998) ‚úÖ Match found\n",
      "[360/755] Searching: 'The Mist' (2007) ‚úÖ Match found\n",
      "[361/755] Searching: 'The Dark Crystal' (1982) ‚úÖ Match found\n",
      "[363/755] Searching: 'Terminator 3: Rise of the Machines' (2003) ‚úÖ Match found\n",
      "[366/755] Searching: 'Johnny English' (2003) ‚úÖ Match found\n",
      "[367/755] Searching: 'Johnny English Reborn' (2011) ‚úÖ Match found\n",
      "[368/755] Searching: 'Love Actually' (2003) ‚úÖ Match found\n",
      "[369/755] Searching: '10,000 BC' (2008) ‚úÖ Match found\n",
      "[370/755] Searching: 'Independence Day' (1996) ‚úÖ Match found\n",
      "[371/755] Searching: '2012' (2009) ‚úÖ Match found\n",
      "[372/755] Searching: 'The Day After Tomorrow' (2004) ‚úÖ Match found\n",
      "[374/755] Searching: 'Fantastic Beasts and Where to Find Them' (2016) ‚úÖ Match found\n",
      "[375/755] Searching: 'Schindler's List' (1993) ‚úÖ Match found\n",
      "[376/755] Searching: 'Skyfall' (2012) ‚úÖ Match found\n",
      "[377/755] Searching: 'Casino Royale' (2006) ‚úÖ Match found\n",
      "[378/755] Searching: 'Spectre' (2015) ‚úÖ Match found\n",
      "[379/755] Searching: 'Quantum of Solace' (2008) ‚úÖ Match found\n",
      "[381/755] Searching: 'The Skin I Live In' (2011) ‚úÖ Match found\n",
      "[382/755] Searching: 'Iron Man 2' (2010) ‚úÖ Match found\n",
      "[383/755] Searching: 'Iron Man' (2008) ‚úÖ Match found\n",
      "[384/755] Searching: 'The Incredible Hulk' (2008) ‚úÖ Match found\n",
      "[385/755] Searching: 'Thor' (2011) ‚úÖ Match found\n",
      "[386/755] Searching: 'The Avengers' (2012) ‚úÖ Match found\n",
      "[387/755] Searching: 'Iron Man 3' (2013) ‚úÖ Match found\n",
      "[388/755] Searching: 'Thor: The Dark World' (2013) ‚úÖ Match found\n",
      "[389/755] Searching: 'Captain America: The Winter Soldier' (2014) ‚úÖ Match found\n",
      "[390/755] Searching: 'Guardians of the Galaxy' (2014) ‚úÖ Match found\n",
      "[391/755] Searching: 'Avengers: Age of Ultron' (2015) ‚úÖ Match found\n",
      "[392/755] Searching: 'Ant-Man' (2015) ‚úÖ Match found\n",
      "[393/755] Searching: 'Captain America: Civil War' (2016) ‚úÖ Match found\n",
      "[394/755] Searching: 'Black Panther' (2018) ‚úÖ Match found\n",
      "[395/755] Searching: 'Guardians of the Galaxy Vol. 2' (2017) ‚úÖ Match found\n",
      "[396/755] Searching: 'Spider-Man: Homecoming' (2017) ‚úÖ Match found\n",
      "[397/755] Searching: 'Ant-Man and the Wasp' (2018) ‚úÖ Match found\n",
      "[398/755] Searching: 'Captain Marvel' (2019) ‚úÖ Match found\n",
      "[399/755] Searching: 'Deadpool' (2016) ‚úÖ Match found\n",
      "[400/755] Searching: 'Deadpool 2' (2018) ‚úÖ Match found\n",
      "[402/755] Searching: 'Marvel One-Shot: A Funny Thing Happened on the Way to Thor's Hammer' (2011) ‚úÖ Match found\n",
      "[403/755] Searching: 'Marvel One-Shot: Item 47' (2012) ‚úÖ Match found\n",
      "[405/755] Searching: 'A Bug's Life' (1998) ‚úÖ Match found\n",
      "[406/755] Searching: 'Teenage Mutant Ninja Turtles' (2014) ‚úÖ Match found\n",
      "[407/755] Searching: 'Cars' (2006) ‚úÖ Match found\n",
      "[408/755] Searching: 'Cars 2' (2011) ‚úÖ Match found\n",
      "[409/755] Searching: 'The Water Horse' (2007) ‚úÖ Match found\n",
      "[411/755] Searching: 'Monsters University' (2013) ‚úÖ Match found\n",
      "[412/755] Searching: 'Chicken Little' (2005) ‚úÖ Match found\n",
      "[414/755] Searching: 'Incredibles 2' (2018) ‚úÖ Match found\n",
      "[415/755] Searching: 'Frozen' (2013) ‚úÖ Match found\n",
      "[416/755] Searching: 'Transformers' (2007) ‚úÖ Match found\n",
      "[417/755] Searching: 'Transformers: Revenge of the Fallen' (2009) ‚úÖ Match found\n",
      "[418/755] Searching: 'Bohemian Rhapsody' (2018) ‚úÖ Match found\n",
      "[419/755] Searching: 'Inside Out' (2015) ‚úÖ Match found\n",
      "[420/755] Searching: 'E.T. the Extra-Terrestrial' (1982) ‚úÖ Match found\n",
      "[421/755] Searching: 'Toy Story' (1995) ‚úÖ Match found\n",
      "[422/755] Searching: 'The Incredibles' (2004) ‚úÖ Match found\n",
      "[423/755] Searching: 'Nosferatu' (1922) ‚úÖ Match found\n",
      "[424/755] Searching: 'A Quiet Place' (2018) ‚úÖ Match found\n",
      "[425/755] Searching: 'The Host' (2006) ‚úÖ Match found\n",
      "[426/755] Searching: 'Forrest Gump' (1994) ‚úÖ Match found\n",
      "[427/755] Searching: 'The Hateful Eight' (2015) ‚úÖ Match found\n",
      "[428/755] Searching: 'The Death of David Cronenberg' (2021) ‚úÖ Match found\n",
      "[429/755] Searching: 'Love, Death & Robots: When the Yogurt Took Over' (2019) ‚ùå No exact match found\n",
      "[430/755] Searching: 'Love, Death & Robots: Ice Age' (2019) ‚ùå No exact match found\n",
      "[431/755] Searching: 'Love, Death & Robots: Three Robots' (2019) ‚ùå No exact match found\n",
      "[432/755] Searching: 'Love, Death & Robots: Fish Night' (2019) ‚ùå No exact match found\n",
      "[433/755] Searching: 'Love, Death & Robots: Sonnie's Edge' (2019) ‚ùå No exact match found\n",
      "[434/755] Searching: 'Love, Death & Robots: Good Hunting' (2019) ‚ùå No exact match found\n",
      "[435/755] Searching: 'Love, Death & Robots: Beyond the Aquila Rift' (2019) ‚ùå No exact match found\n",
      "[436/755] Searching: 'Love, Death & Robots: Sucker of Souls' (2019) ‚ùå No exact match found\n",
      "[437/755] Searching: 'Love, Death & Robots: Suits' (2019) ‚ùå No exact match found\n",
      "[438/755] Searching: 'Love, Death & Robots: The Witness' (2019) ‚ùå No exact match found\n",
      "[439/755] Searching: 'Love, Death & Robots: The Dump' (2019) ‚ùå No exact match found\n",
      "[440/755] Searching: 'Love, Death & Robots: Shape-Shifters' (2019) ‚ùå No exact match found\n",
      "[441/755] Searching: 'Love, Death & Robots: Helping Hand' (2019) ‚ùå No exact match found\n",
      "[442/755] Searching: 'Love, Death & Robots: Lucky 13' (2019) ‚ùå No exact match found\n",
      "[443/755] Searching: 'Love, Death & Robots: Zima Blue' (2019) ‚ùå No exact match found\n",
      "[444/755] Searching: 'Love, Death & Robots: The Secret War' (2019) ‚ùå No exact match found\n",
      "[445/755] Searching: 'Love, Death & Robots: Alternate Histories' (2019) ‚ùå No exact match found\n",
      "[446/755] Searching: 'Love, Death & Robots: Blindspot' (2019) ‚ùå No exact match found\n",
      "[448/755] Searching: 'Downfall' (2004) ‚úÖ Match found\n",
      "[449/755] Searching: 'The Wave' (2008) ‚úÖ Match found\n",
      "[450/755] Searching: 'Cowboys & Aliens' (2011) ‚úÖ Match found\n",
      "[451/755] Searching: 'Animals United' (2010) ‚úÖ Match found\n",
      "[452/755] Searching: '7 Dwarves - Men Alone in the Woods' (2004) ‚úÖ Match found\n",
      "[453/755] Searching: '7 Dwarves: The Forest Is Not Enough' (2006) ‚úÖ Match found\n",
      "[454/755] Searching: 'Otto's Eleven' (2010) ‚úÖ Match found\n",
      "[455/755] Searching: 'Manitou's Shoe' (2001) ‚úÖ Match found\n",
      "[456/755] Searching: '(T)Raumschiff Surprise - Periode 1' (2004) ‚úÖ Match found\n",
      "[457/755] Searching: 'Bullyparade: The Movie' (2017) ‚úÖ Match found\n",
      "[458/755] Searching: 'Lissi and the Wild Emperor' (2007) ‚úÖ Match found\n",
      "[459/755] Searching: 'Wickie the Mighty Viking' (2009) ‚úÖ Match found\n",
      "[460/755] Searching: 'Buddy' (2013) ‚úÖ Match found\n",
      "[461/755] Searching: 'Laura's Star' (2004) ‚úÖ Match found\n",
      "[462/755] Searching: 'Zoolander' (2001) ‚úÖ Match found\n",
      "[464/755] Searching: 'Back to the Future' (1985) ‚úÖ Match found\n",
      "[465/755] Searching: 'Back to the Future Part III' (1990) ‚úÖ Match found\n",
      "[466/755] Searching: 'Back to the Future Part II' (1989) ‚úÖ Match found\n",
      "[467/755] Searching: 'The Lego Movie' (2014) ‚úÖ Match found\n",
      "[468/755] Searching: 'Beetlejuice' (1988) ‚úÖ Match found\n",
      "[469/755] Searching: 'The Emoji Movie' (2017) ‚úÖ Match found\n",
      "[470/755] Searching: 'Batman & Robin' (1997) ‚úÖ Match found\n",
      "[471/755] Searching: 'Batman' (1989) ‚úÖ Match found\n",
      "[472/755] Searching: 'Batman Returns' (1992) ‚úÖ Match found\n",
      "[473/755] Searching: 'Batman Forever' (1995) ‚úÖ Match found\n",
      "[474/755] Searching: 'Modern Times' (1936) ‚úÖ Match found\n",
      "[475/755] Searching: 'Moana' (2016) ‚úÖ Match found\n",
      "[476/755] Searching: 'Raiders of the Lost Ark' (1981) ‚úÖ Match found\n",
      "[477/755] Searching: 'Indiana Jones and the Temple of Doom' (1984) ‚úÖ Match found\n",
      "[479/755] Searching: 'Pok√©mon Detective Pikachu' (2019) ‚úÖ Match found\n",
      "[480/755] Searching: 'Rudolph the Red-Nosed Reindeer: The Movie' (1998) ‚úÖ Match found\n",
      "[482/755] Searching: 'Rudolph the Red-Nosed Reindeer & the Island of Misfit Toys' (2001) ‚úÖ Match found\n",
      "[483/755] Searching: 'Wach' (2018) ‚úÖ Match found\n",
      "[484/755] Searching: 'The Naked Gun: From the Files of Police Squad!' (1988) ‚úÖ Match found\n",
      "[485/755] Searching: 'The Naked Gun 2¬Ω: The Smell of Fear' (1991) ‚úÖ Match found\n",
      "[486/755] Searching: 'Naked Gun 33‚Öì: The Final Insult' (1994) ‚úÖ Match found\n",
      "[488/755] Searching: 'Les Mis√©rables' (2012) ‚úÖ Match found\n",
      "[489/755] Searching: 'Zygote' (2017) ‚úÖ Match found\n",
      "[490/755] Searching: 'District 9' (2009) ‚úÖ Match found\n",
      "[491/755] Searching: 'Evolution' (2001) ‚úÖ Match found\n",
      "[493/755] Searching: 'Armageddon' (1998) ‚úÖ Match found\n",
      "[494/755] Searching: 'Triumph of the Will' (1935) ‚úÖ Match found\n",
      "[496/755] Searching: 'Sweeney Todd: The Demon Barber of Fleet Street' (2007) ‚úÖ Match found\n",
      "[497/755] Searching: 'The Trial of the Chicago 7' (2020) ‚úÖ Match found\n",
      "[498/755] Searching: 'Good Omens: Lockdown' (2020) ‚úÖ Match found\n",
      "[499/755] Searching: 'Saw' (2003) ‚úÖ Match found\n",
      "[500/755] Searching: 'Treasure Planet' (2002) ‚úÖ Match found\n",
      "[503/755] Searching: 'The Intouchables' (2011) ‚úÖ Match found\n",
      "[504/755] Searching: 'Serial (Bad) Weddings' (2014) ‚úÖ Match found\n",
      "[506/755] Searching: 'Noah' (2014) ‚úÖ Match found\n",
      "[507/755] Searching: 'Errementari: The Blacksmith and the Devil' (2017) ‚úÖ Match found\n",
      "[508/755] Searching: 'The Two Popes' (2019) ‚úÖ Match found\n",
      "[509/755] Searching: 'Luther' (2003) ‚úÖ Match found\n",
      "[510/755] Searching: 'The Passion of the Christ' (2004) ‚úÖ Match found\n",
      "[511/755] Searching: 'Hop' (2011) ‚úÖ Match found\n",
      "[512/755] Searching: 'Alvin and the Chipmunks' (2007) ‚úÖ Match found\n",
      "[513/755] Searching: 'Apollo 18' (2011) ‚úÖ Match found\n",
      "[514/755] Searching: 'The Chronicles of Narnia: The Lion, the Witch and the Wardrobe' (2005) ‚úÖ Match found\n",
      "[515/755] Searching: 'Edward Scissorhands' (1990) ‚úÖ Match found\n",
      "[516/755] Searching: 'Corpse Bride' (2005) ‚úÖ Match found\n",
      "[517/755] Searching: 'Dark Shadows' (2012) ‚úÖ Match found\n",
      "[518/755] Searching: 'The Martian' (2015) ‚úÖ Match found\n",
      "[519/755] Searching: 'Ghostbusters' (1984) ‚úÖ Match found\n",
      "[520/755] Searching: 'Ghostbusters' (2016) ‚úÖ Match found\n",
      "[521/755] Searching: 'Ghostbusters II' (1989) ‚úÖ Match found\n",
      "[524/755] Searching: 'The Smurfs' (2011) ‚úÖ Match found\n",
      "[525/755] Searching: 'The Smurfs 2' (2013) ‚úÖ Match found\n",
      "[526/755] Searching: 'Garfield' (2004) ‚úÖ Match found\n",
      "[529/755] Searching: 'Stromberg ‚Äì The Movie' (2014) ‚úÖ Match found\n",
      "[530/755] Searching: 'Look Who's Back' (2015) ‚úÖ Match found\n",
      "[531/755] Searching: 'Manhunt' (2017) ‚úÖ Match found\n",
      "[543/755] Searching: 'Hairspray' (2007) ‚úÖ Match found\n",
      "[553/755] Searching: 'Kung Fury' (2015) ‚úÖ Match found\n",
      "[554/755] Searching: 'Iron Sky' (2012) ‚úÖ Match found\n",
      "[557/755] Searching: 'Werner and the Wizard of Booze' (1990) ‚úÖ Match found\n",
      "[558/755] Searching: 'Werner: Eat My Dust!!!' (1996) ‚úÖ Match found\n",
      "[559/755] Searching: 'Werner - Volles Roo√§√§√§!!!' (1999) ‚úÖ Match found\n",
      "[560/755] Searching: 'Werner - Gekotzt wird sp√§ter!' (2003) ‚úÖ Match found\n",
      "[561/755] Searching: 'Werner - Eiskalt!' (2011) ‚úÖ Match found\n",
      "[564/755] Searching: 'Big Time Adolescence' (2019) ‚úÖ Match found\n",
      "[565/755] Searching: 'Project Power' (2020) ‚úÖ Match found\n",
      "[571/755] Searching: 'Home Alone' (1990) ‚úÖ Match found\n",
      "[572/755] Searching: 'Home Alone 2: Lost in New York' (1992) ‚úÖ Match found\n",
      "[578/755] Searching: 'Puss in Boots' (2011) ‚úÖ Match found\n",
      "[581/755] Searching: 'The Great Wall' (2016) ‚úÖ Match found\n",
      "[582/755] Searching: 'Gerald's Game' (2017) ‚úÖ Match found\n",
      "[583/755] Searching: 'Erin Brockovich' (2000) ‚úÖ Match found\n",
      "[584/755] Searching: 'Wonder' (2017) ‚úÖ Match found\n",
      "[588/755] Searching: 'Alex Strangelove' (2018) ‚úÖ Match found\n",
      "[590/755] Searching: 'history of the entire world, i guess' (2017) ‚ùå No exact match found\n",
      "[592/755] Searching: 'Sex Tape' (2014) ‚úÖ Match found\n",
      "[593/755] Searching: 'Apostle' (2018) ‚úÖ Match found\n",
      "[595/755] Searching: 'Monsters' (2010) ‚úÖ Match found\n",
      "[599/755] Searching: 'Once' (2007) ‚úÖ Match found\n",
      "[600/755] Searching: 'My Octopus Teacher' (2020) ‚úÖ Match found\n",
      "[601/755] Searching: 'Gone Girl' (2014) ‚úÖ Match found\n",
      "[602/755] Searching: 'Watchmen' (2009) ‚úÖ Match found\n",
      "[603/755] Searching: 'Phase IV' (1974) ‚úÖ Match found\n",
      "[606/755] Searching: 'Trio' (2019) ‚úÖ Match found\n",
      "[609/755] Searching: 'The Perfect Secret' (2019) ‚úÖ Match found\n",
      "[610/755] Searching: 'Turkish for Beginners' (2012) ‚úÖ Match found\n",
      "[611/755] Searching: 'Suck Me Shakespeer' (2013) ‚úÖ Match found\n",
      "[612/755] Searching: 'Suck Me Shakespeer 2' (2015) ‚úÖ Match found\n",
      "[613/755] Searching: 'Suck Me Shakespeer 3' (2017) ‚úÖ Match found\n",
      "[614/755] Searching: 'Girls on Top' (2001) ‚úÖ Match found\n",
      "[615/755] Searching: 'The NeverEnding Story' (1984) ‚úÖ Match found\n",
      "[619/755] Searching: 'Edge of Tomorrow' (2014) ‚úÖ Match found\n",
      "[620/755] Searching: 'The Wrong Trousers' (1993) ‚úÖ Match found\n",
      "[622/755] Searching: 'The Hangover' (2009) ‚úÖ Match found\n",
      "[623/755] Searching: '21 Jump Street' (2012) ‚úÖ Match found\n",
      "[624/755] Searching: 'The Lorax' (2012) ‚úÖ Match found\n",
      "[625/755] Searching: 'Monsters vs Aliens' (2009) ‚úÖ Match found\n",
      "[627/755] Searching: 'Grease' (1978) ‚úÖ Match found\n",
      "[636/755] Searching: 'Skyscraper' (2018) ‚úÖ Match found\n",
      "[640/755] Searching: 'Hulk' (2003) ‚úÖ Match found\n",
      "[641/755] Searching: 'The Greatest Showman' (2017) ‚úÖ Match found\n",
      "[643/755] Searching: 'Rubber' (2010) ‚úÖ Match found\n",
      "[645/755] Searching: 'Mowgli: Legend of the Jungle' (2018) ‚úÖ Match found\n",
      "[651/755] Searching: 'Dumbo' (2019) ‚úÖ Match found\n",
      "[652/755] Searching: 'Miss Peregrine's Home for Peculiar Children' (2016) ‚úÖ Match found\n",
      "[657/755] Searching: '28 Weeks Later' (2007) ‚úÖ Match found\n",
      "[659/755] Searching: 'The Land Before Time II: The Great Valley Adventure' (1994) ‚úÖ Match found\n",
      "[660/755] Searching: 'The Land Before Time III: The Time of the Great Giving' (1995) ‚úÖ Match found\n",
      "[661/755] Searching: 'The Land Before Time IV: Journey Through the Mists' (1996) ‚úÖ Match found\n",
      "[662/755] Searching: 'The Land Before Time V: The Mysterious Island' (1997) ‚úÖ Match found\n",
      "[663/755] Searching: 'The Land Before Time VI: The Secret of Saurus Rock' (1998) ‚úÖ Match found\n",
      "[664/755] Searching: 'The Land Before Time VII: The Stone of Cold Fire' (2000) ‚úÖ Match found\n",
      "[665/755] Searching: 'The Land Before Time VIII: The Big Freeze' (2001) ‚úÖ Match found\n",
      "[666/755] Searching: 'The Land Before Time IX: Journey to Big Water' (2002) ‚úÖ Match found\n",
      "[667/755] Searching: 'The Land Before Time X: The Great Longneck Migration' (2003) ‚úÖ Match found\n",
      "[668/755] Searching: 'The Land Before Time XI: Invasion of the Tinysauruses' (2005) ‚úÖ Match found\n",
      "[669/755] Searching: 'The Land Before Time XII: The Great Day of the Flyers' (2006) ‚úÖ Match found\n",
      "[675/755] Searching: 'The Bisexual' (2018) ‚úÖ Match found\n",
      "[708/755] Searching: 'Scrooged' (1988) ‚úÖ Match found\n",
      "[709/755] Searching: 'Don't Hug Me I'm Scared' (2011) ‚úÖ Match found\n",
      "[710/755] Searching: 'Don't Hug Me I'm Scared 2' (2014) ‚úÖ Match found\n",
      "[711/755] Searching: 'Don't Hug Me I'm Scared 6' (2016) ‚úÖ Match found\n",
      "[712/755] Searching: 'Don't Hug Me I'm Scared 4' (2015) ‚úÖ Match found\n",
      "[713/755] Searching: 'Don't Hug Me I'm Scared 3' (2014) ‚úÖ Match found\n",
      "[714/755] Searching: 'Don't Hug Me I'm Scared 5' (2015) ‚úÖ Match found\n",
      "[721/755] Searching: 'The Last Unicorn' (1982) ‚úÖ Match found\n",
      "[723/755] Searching: 'Land of the Lost' (2009) ‚úÖ Match found\n",
      "[726/755] Searching: 'Spaceballs' (1987) ‚úÖ Match found\n",
      "[727/755] Searching: 'WHAT DID JACK DO?' (2017) ‚úÖ Match found\n",
      "[731/755] Searching: 'Cruella' (2021) ‚úÖ Match found\n",
      "[749/755] Searching: 'A Very Sunny Christmas' (2009) ‚úÖ Match found\n",
      "[754/755] Searching: 'Cargo' (2017) ‚úÖ Match found\n",
      "[755/755] Searching: 'Evan Almighty' (2007) ‚úÖ Match found\n",
      "\n",
      "‚úîÔ∏è  Enrichment completed.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:55:51.617402Z",
     "start_time": "2025-09-14T15:55:51.593037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "\n",
    "    \"ich baue einen Knowledge graph basierend auf Letterboxd User-Daten in einem Jupyter Notebook. Diese m√∂chte ich jetzt mit TMDB Daten anreichern. Dazu habe ich mir gerade API Zugang verschafft, ich habe jetzt einen API Read Access Token.\n",
    "        Wie beginne ich, meine bestehende Tabelle mit weitern Film-Daten anzureichern?\"\n",
    "\n",
    "\n",
    "    \"def enrich_dataframe(df):\n",
    "            new_data = []\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                result = search_movie(title, year)\n",
    "                if result:\n",
    "                    details = get_movie_details(result[\"id\"])\n",
    "                    if details:\n",
    "                        new_data.append({\n",
    "                            \"title\": title,\n",
    "                            \"year\": year,\n",
    "                            \"tmdb_id\": result[\"id\"],\n",
    "                            \"overview\": details.get(\"overview\"),\n",
    "                            \"genres\": [g[\"name\"] for g in details.get(\"genres\", [])],\n",
    "                            \"runtime\": details.get(\"runtime\"),\n",
    "                            \"vote_average\": details.get(\"vote_average\"),\n",
    "                            \"poster_path\": details.get(\"poster_path\")\n",
    "                        })\n",
    "                else:\n",
    "                    new_data.append({\n",
    "                        \"title\": title,\n",
    "                        \"year\": year,\n",
    "                        \"tmdb_id\": None,\n",
    "                        \"overview\": None,\n",
    "                        \"genres\": None,\n",
    "                        \"runtime\": None,\n",
    "                        \"vote_average\": None,\n",
    "                        \"poster_path\": None\n",
    "                    })\n",
    "\n",
    "                sleep(0.25)  # Vermeide Rate-Limits\n",
    "\n",
    "            return pd.DataFrame(new_data)\n",
    "\n",
    "        add print statements to this method so the user gets feedback while the method is running\"\n",
    "\n",
    "\n",
    "    \"bitte englische kommentare stattdessen\"\n",
    "\n",
    "\n",
    "    \"Gerade erstellt die Funktion \"enrich_dataframe\" eine v√∂llig neue Tabelle mit TMDB daten und beinhaltet aber keine meiner Daten die ich bereits in einer Tabelle habe.\n",
    "\n",
    "        Das ist mein Code bisher:\n",
    "        merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add \"Last Watched\" information\n",
    "\n",
    "        # Step 1: Group diary entries by movie and take the latest date as a string\n",
    "        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "        # Step 2: Merge with the existing merged DataFrame\n",
    "        merged = pd.merge(\n",
    "            merged,\n",
    "            last_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Add \"Times watched\" information\n",
    "\n",
    "        # Step 1: Sort diary by \"Watched Date\" so earliest watches come first\n",
    "        diary_sorted = diary.sort_values(\"Watched Date\")\n",
    "\n",
    "        # Step 2: Define a function to count watches and check if first was a rewatch\n",
    "        def get_times_watched(group):\n",
    "            count = len(group)\n",
    "            first_rewatch = group.iloc[0][\"Rewatch\"] == \"Yes\"\n",
    "            return f\"{count}+\" if first_rewatch else str(count)\n",
    "\n",
    "        # Step 3: Group and apply, excluding group keys from the inner DataFrame\n",
    "        times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\n",
    "            get_times_watched\n",
    "        ).reset_index(name=\"Times watched\")\n",
    "\n",
    "\n",
    "        # Step 4: Merge into the main DataFrame\n",
    "        merged = (pd.merge(\n",
    "            merged,\n",
    "            times_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        ).rename(columns={'Name': 'Title'}).sort_values(by=\"Last watched date\", ascending=False))\n",
    "\n",
    "        merged.head(15)\n",
    "\n",
    "        Meine Tabelle beinhaltet die Spalten:\n",
    "        Title, Year, Letterboxd URI, Rating, Liked, Last watched date, Times watched\n",
    "\n",
    "        Nun m√∂chte ich die TMDB daten an diese tabelle anheften.\"\n",
    "\n",
    "\n",
    "    \"automatically include the full poster path in the table. use size \"original\"\"\n",
    "\n",
    "\n",
    "    \"def search_movie(title, year=None):\n",
    "            url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"year\"] = year\n",
    "            response = requests.get(url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\")\n",
    "                return results[0] if results else None\n",
    "            else:\n",
    "                print(f\"Error at {title}: {response.status_code}\")\n",
    "                return None\n",
    "\n",
    "        def get_movie_details(tmdb_id):\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def enrich_dataframe(df):\n",
    "            tmdb_base_url = \"https://www.themoviedb.org/movie/\"\n",
    "            poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "            size = \"original\"  # Use original size for poster images\n",
    "\n",
    "            # Create empty columns for TMDB data\n",
    "            df[\"tmdb_url\"] = None\n",
    "            df[\"overview\"] = None\n",
    "            df[\"genres\"] = None\n",
    "            df[\"runtime\"] = None\n",
    "            df[\"vote_average\"] = None\n",
    "            df[\"poster_url\"] = None\n",
    "\n",
    "            total = len(df)\n",
    "            print(f\"Starting enrichment for {total} films using TMDB data...\\n\")\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "                result = search_movie(title, year)\n",
    "                if result:\n",
    "                    print(\" ‚úÖ Match found\")\n",
    "                    details = get_movie_details(result[\"id\"])\n",
    "                    if details:\n",
    "                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + str(result[\"id\"])\n",
    "                        df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\n",
    "                        df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "\n",
    "                        poster_path = details.get(\"poster_path\")\n",
    "                        if poster_path:\n",
    "                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\n",
    "                        else:\n",
    "                            df.at[idx, \"poster_url\"] = None\n",
    "                    else:\n",
    "                        print(\" ‚ö†Ô∏è  Details not found\")\n",
    "                else:\n",
    "                    print(\" ‚ùå No match\")\n",
    "\n",
    "                sleep(0.25)  # Avoid rate limit\n",
    "\n",
    "            print(\"\\n‚úîÔ∏è  Enrichment completed.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "        i want the following changes to this code:\n",
    "\n",
    "        instead of only searching for movies, it searches for movies and tv shows.\n",
    "        the algorithm works like this:\n",
    "        first it searches for it in the movie database always with title and year. from the list of search results it will try to match the title EXACTLY. only if an exact title match fails, it then tries to search for it in the tv show database. again, it tries to match the title exactly.\"\n",
    "\n",
    "\n",
    "    \"if it is a TV show, i dont want a runtime in my results\"\n",
    "\n",
    "\n",
    "    \"add a new column that includes info on whether the list entry is a TV show or a Movie\"\n",
    "\n",
    "\n",
    "    \"this works well! I have an additional problem:\n",
    "\n",
    "        in my testing i found problems with for example these two movies:\n",
    "\n",
    "        In my letterboxd data, the movie is called\n",
    "        The Hunger Games: Mockingjay ‚Äì Part 1\n",
    "\n",
    "        TMDB has it listed as\n",
    "        The Hunger Games: Mockingjay - Part 1\n",
    "\n",
    "        the algorithm is unable to match it, i suspect because of the two different hypthens.\n",
    "\n",
    "        another case is this, letterboxd has the movie as\n",
    "        Godzilla √ó Kong: The New Empire\n",
    "\n",
    "        whereas TMDB lists it as\n",
    "        Godzilla x Kong: The New Empire\n",
    "\n",
    "        the algorithm is unable to match it, again likely because the \"x\" is different.\n",
    "\n",
    "        how can I solve this problem?\"\n",
    "\n",
    "\n",
    "    \"import requests\n",
    "        from time import sleep\n",
    "        import os\n",
    "        from dotenv import load_dotenv\n",
    "        import unicodedata\n",
    "        import re\n",
    "\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')\n",
    "\n",
    "        HEADERS = {\n",
    "            \"Authorization\": \"Bearer \" + TMDB_API_TOKEN,\n",
    "            \"Content-Type\": \"application/json;charset=utf-8\"\n",
    "        }\n",
    "\n",
    "        def normalize_title(title):\n",
    "            if not title:\n",
    "                return \"\"\n",
    "            # Unicode normalize, convert to ASCII-compatible\n",
    "            title = unicodedata.normalize(\"NFKC\", title)\n",
    "\n",
    "            # Replace common visually similar characters\n",
    "            substitutions = {\n",
    "                \"‚Äì\": \"-\",  # en dash\n",
    "                \"‚Äî\": \"-\",  # em dash\n",
    "                \"‚àí\": \"-\",  # minus\n",
    "                \"√ó\": \"x\",  # multiplication sign\n",
    "                \"‚Äô\": \"'\",  # curly apostrophe\n",
    "                \"‚Äú\": '\"',\n",
    "                \"‚Äù\": '\"',\n",
    "                \"‚Ä¶\": \"...\",\n",
    "                \"&\": \"and\",  # optional\n",
    "            }\n",
    "\n",
    "            for orig, repl in substitutions.items():\n",
    "                title = title.replace(orig, repl)\n",
    "\n",
    "            # Collapse multiple spaces and lowercase\n",
    "            title = re.sub(r\"\\s+\", \" \", title).strip().lower()\n",
    "            return title\n",
    "\n",
    "        def search_exact_match(results, search_title):\n",
    "            norm_search = normalize_title(search_title)\n",
    "            for r in results:\n",
    "                tmdb_title = r.get(\"title\") or r.get(\"name\") or \"\"\n",
    "                if normalize_title(tmdb_title) == norm_search:\n",
    "                    return r\n",
    "            return None\n",
    "\n",
    "        def search_movie_or_tv(title, year=None):\n",
    "            # First: try movie search\n",
    "            movie_url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"year\"] = year\n",
    "\n",
    "            response = requests.get(movie_url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\", [])\n",
    "                match = search_exact_match(results, title)\n",
    "                if match:\n",
    "                    match[\"media_type\"] = \"movie\"\n",
    "                    return match\n",
    "\n",
    "            # Second: try TV search\n",
    "            tv_url = \"https://api.themoviedb.org/3/search/tv\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"first_air_date_year\"] = year\n",
    "\n",
    "            response = requests.get(tv_url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\", [])\n",
    "                match = search_exact_match(results, title)\n",
    "                if match:\n",
    "                    match[\"media_type\"] = \"tv\"\n",
    "                    return match\n",
    "\n",
    "            # print(f\" Error or no match for '{title}' \")\n",
    "            return None\n",
    "\n",
    "        def get_movie_details(tmdb_id):\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_details(tmdb_id, media_type):\n",
    "            url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            return response.json() if response.status_code == 200 else None\n",
    "\n",
    "        def enrich_dataframe(df):\n",
    "            tmdb_base_url = \"https://www.themoviedb.org/\"\n",
    "            poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "            size = \"original\"\n",
    "\n",
    "            # Create empty columns for TMDB data\n",
    "            df[\"tmdb_url\"] = None\n",
    "            df[\"overview\"] = None\n",
    "            df[\"genres\"] = None\n",
    "            df[\"runtime\"] = None\n",
    "            df[\"vote_average\"] = None\n",
    "            df[\"poster_url\"] = None\n",
    "            df[\"media_type\"] = None  # New column: \"movie\" or \"tv\"\n",
    "\n",
    "            total = len(df)\n",
    "            print(f\"Starting enrichment for {total} titles using TMDB data...\\n\")\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "                result = search_movie_or_tv(title, year)\n",
    "                if result:\n",
    "                    print(\" ‚úÖ Match found\")\n",
    "                    media_type = result[\"media_type\"]\n",
    "                    details = get_details(result[\"id\"], media_type)\n",
    "\n",
    "                    if details:\n",
    "                        df.at[idx, \"media_type\"] = media_type\n",
    "                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + f\"{media_type}/\" + str(result[\"id\"])\n",
    "                        df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\n",
    "                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "\n",
    "                        # Only add runtime for movies\n",
    "                        if media_type == \"movie\":\n",
    "                            df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                        else:\n",
    "                            df.at[idx, \"runtime\"] = None\n",
    "\n",
    "                        poster_path = details.get(\"poster_path\")\n",
    "                        if poster_path:\n",
    "                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\n",
    "                else:\n",
    "                    print(\" ‚ùå No exact match found\")\n",
    "\n",
    "                sleep(0.25)\n",
    "\n",
    "            print(\"\\n‚úîÔ∏è  Enrichment completed.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "        enriched_merged = enrich_dataframe(merged)\n",
    "        enriched_merged.to_csv(\"data/enriched_merged.csv\", index=False)\n",
    "\n",
    "        this is my code. Adapt it so that it includes a new column that includes the directors name. another column should include an array of actor names, and another column should include an array of character names.\"\n",
    "\n",
    "\n",
    "    \"i want to store them like the genres:\n",
    "        ['Adventure', 'Drama', 'Science Fiction']\n",
    "        also, it should be possible to have multiple directors.\n",
    "        please give me an updated \"enrich dataframe\" function\"\n",
    "\n",
    "'''"
   ],
   "id": "12c58eab9013249b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\\nThese following prompts were used:\\n\\n\\n    \"ich baue einen Knowledge graph basierend auf Letterboxd User-Daten in einem Jupyter Notebook. Diese m√∂chte ich jetzt mit TMDB Daten anreichern. Dazu habe ich mir gerade API Zugang verschafft, ich habe jetzt einen API Read Access Token.\\n        Wie beginne ich, meine bestehende Tabelle mit weitern Film-Daten anzureichern?\"\\n\\n\\n    \"def enrich_dataframe(df):\\n            new_data = []\\n            for idx, row in df.iterrows():\\n                title = row[\"Title\"]\\n                year = row.get(\"Year\", None)\\n\\n                result = search_movie(title, year)\\n                if result:\\n                    details = get_movie_details(result[\"id\"])\\n                    if details:\\n                        new_data.append({\\n                            \"title\": title,\\n                            \"year\": year,\\n                            \"tmdb_id\": result[\"id\"],\\n                            \"overview\": details.get(\"overview\"),\\n                            \"genres\": [g[\"name\"] for g in details.get(\"genres\", [])],\\n                            \"runtime\": details.get(\"runtime\"),\\n                            \"vote_average\": details.get(\"vote_average\"),\\n                            \"poster_path\": details.get(\"poster_path\")\\n                        })\\n                else:\\n                    new_data.append({\\n                        \"title\": title,\\n                        \"year\": year,\\n                        \"tmdb_id\": None,\\n                        \"overview\": None,\\n                        \"genres\": None,\\n                        \"runtime\": None,\\n                        \"vote_average\": None,\\n                        \"poster_path\": None\\n                    })\\n\\n                sleep(0.25)  # Vermeide Rate-Limits\\n\\n            return pd.DataFrame(new_data)\\n\\n        add print statements to this method so the user gets feedback while the method is running\"\\n\\n\\n    \"bitte englische kommentare stattdessen\"\\n\\n\\n    \"Gerade erstellt die Funktion \"enrich_dataframe\" eine v√∂llig neue Tabelle mit TMDB daten und beinhaltet aber keine meiner Daten die ich bereits in einer Tabelle habe.\\n\\n        Das ist mein Code bisher:\\n        merged = pd.merge(\\n            watched[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\']],\\n            ratings[[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\', \\'Rating\\']],\\n            on=[\\'Name\\', \\'Year\\', \\'Letterboxd URI\\'],\\n            how=\\'left\\'\\n        )\\n\\n        # Add \"Liked\" information\\n        liked_uris = set(likes[\"Letterboxd URI\"])\\n\\n        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\\n            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\\n        )\\n\\n\\n        # Add \"Last Watched\" information\\n\\n        # Step 1: Group diary entries by movie and take the latest date as a string\\n        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\\n        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\\n\\n        # Step 2: Merge with the existing merged DataFrame\\n        merged = pd.merge(\\n            merged,\\n            last_watched,\\n            on=[\"Name\", \"Year\"],\\n            how=\"left\"\\n        )\\n\\n        # Add \"Times watched\" information\\n\\n        # Step 1: Sort diary by \"Watched Date\" so earliest watches come first\\n        diary_sorted = diary.sort_values(\"Watched Date\")\\n\\n        # Step 2: Define a function to count watches and check if first was a rewatch\\n        def get_times_watched(group):\\n            count = len(group)\\n            first_rewatch = group.iloc[0][\"Rewatch\"] == \"Yes\"\\n            return f\"{count}+\" if first_rewatch else str(count)\\n\\n        # Step 3: Group and apply, excluding group keys from the inner DataFrame\\n        times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\\n            get_times_watched\\n        ).reset_index(name=\"Times watched\")\\n\\n\\n        # Step 4: Merge into the main DataFrame\\n        merged = (pd.merge(\\n            merged,\\n            times_watched,\\n            on=[\"Name\", \"Year\"],\\n            how=\"left\"\\n        ).rename(columns={\\'Name\\': \\'Title\\'}).sort_values(by=\"Last watched date\", ascending=False))\\n\\n        merged.head(15)\\n\\n        Meine Tabelle beinhaltet die Spalten:\\n        Title, Year, Letterboxd URI, Rating, Liked, Last watched date, Times watched\\n\\n        Nun m√∂chte ich die TMDB daten an diese tabelle anheften.\"\\n\\n\\n    \"automatically include the full poster path in the table. use size \"original\"\"\\n\\n\\n    \"def search_movie(title, year=None):\\n            url = \"https://api.themoviedb.org/3/search/movie\"\\n            params = {\"query\": title}\\n            if year:\\n                params[\"year\"] = year\\n            response = requests.get(url, headers=HEADERS, params=params)\\n            if response.status_code == 200:\\n                results = response.json().get(\"results\")\\n                return results[0] if results else None\\n            else:\\n                print(f\"Error at {title}: {response.status_code}\")\\n                return None\\n\\n        def get_movie_details(tmdb_id):\\n            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\\n            response = requests.get(url, headers=HEADERS)\\n            if response.status_code == 200:\\n                return response.json()\\n            else:\\n                return None\\n\\n\\n        def enrich_dataframe(df):\\n            tmdb_base_url = \"https://www.themoviedb.org/movie/\"\\n            poster_base_url = \"https://image.tmdb.org/t/p/\"\\n            size = \"original\"  # Use original size for poster images\\n\\n            # Create empty columns for TMDB data\\n            df[\"tmdb_url\"] = None\\n            df[\"overview\"] = None\\n            df[\"genres\"] = None\\n            df[\"runtime\"] = None\\n            df[\"vote_average\"] = None\\n            df[\"poster_url\"] = None\\n\\n            total = len(df)\\n            print(f\"Starting enrichment for {total} films using TMDB data...\\n\")\\n\\n            for idx, row in df.iterrows():\\n                title = row[\"Title\"]\\n                year = row.get(\"Year\", None)\\n\\n                print(f\"[{idx+1}/{total}] Searching: \\'{title}\\' ({year})\", end=\"\")\\n\\n                result = search_movie(title, year)\\n                if result:\\n                    print(\" ‚úÖ Match found\")\\n                    details = get_movie_details(result[\"id\"])\\n                    if details:\\n                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + str(result[\"id\"])\\n                        df.at[idx, \"overview\"] = details.get(\"overview\")\\n                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\\n                        df.at[idx, \"runtime\"] = details.get(\"runtime\")\\n                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\\n\\n                        poster_path = details.get(\"poster_path\")\\n                        if poster_path:\\n                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\\n                        else:\\n                            df.at[idx, \"poster_url\"] = None\\n                    else:\\n                        print(\" ‚ö†Ô∏è  Details not found\")\\n                else:\\n                    print(\" ‚ùå No match\")\\n\\n                sleep(0.25)  # Avoid rate limit\\n\\n            print(\"\\n‚úîÔ∏è  Enrichment completed.\")\\n            return df\\n\\n\\n        i want the following changes to this code:\\n\\n        instead of only searching for movies, it searches for movies and tv shows.\\n        the algorithm works like this:\\n        first it searches for it in the movie database always with title and year. from the list of search results it will try to match the title EXACTLY. only if an exact title match fails, it then tries to search for it in the tv show database. again, it tries to match the title exactly.\"\\n\\n\\n    \"if it is a TV show, i dont want a runtime in my results\"\\n\\n\\n    \"add a new column that includes info on whether the list entry is a TV show or a Movie\"\\n\\n\\n    \"this works well! I have an additional problem:\\n\\n        in my testing i found problems with for example these two movies:\\n\\n        In my letterboxd data, the movie is called\\n        The Hunger Games: Mockingjay ‚Äì Part 1\\n\\n        TMDB has it listed as\\n        The Hunger Games: Mockingjay - Part 1\\n\\n        the algorithm is unable to match it, i suspect because of the two different hypthens.\\n\\n        another case is this, letterboxd has the movie as\\n        Godzilla √ó Kong: The New Empire\\n\\n        whereas TMDB lists it as\\n        Godzilla x Kong: The New Empire\\n\\n        the algorithm is unable to match it, again likely because the \"x\" is different.\\n\\n        how can I solve this problem?\"\\n\\n\\n    \"import requests\\n        from time import sleep\\n        import os\\n        from dotenv import load_dotenv\\n        import unicodedata\\n        import re\\n\\n        # Load environment variables\\n        load_dotenv()\\n\\n        TMDB_API_TOKEN = os.getenv(\\'TMDB_API_TOKEN\\')\\n\\n        HEADERS = {\\n            \"Authorization\": \"Bearer \" + TMDB_API_TOKEN,\\n            \"Content-Type\": \"application/json;charset=utf-8\"\\n        }\\n\\n        def normalize_title(title):\\n            if not title:\\n                return \"\"\\n            # Unicode normalize, convert to ASCII-compatible\\n            title = unicodedata.normalize(\"NFKC\", title)\\n\\n            # Replace common visually similar characters\\n            substitutions = {\\n                \"‚Äì\": \"-\",  # en dash\\n                \"‚Äî\": \"-\",  # em dash\\n                \"‚àí\": \"-\",  # minus\\n                \"√ó\": \"x\",  # multiplication sign\\n                \"‚Äô\": \"\\'\",  # curly apostrophe\\n                \"‚Äú\": \\'\"\\',\\n                \"‚Äù\": \\'\"\\',\\n                \"‚Ä¶\": \"...\",\\n                \"&\": \"and\",  # optional\\n            }\\n\\n            for orig, repl in substitutions.items():\\n                title = title.replace(orig, repl)\\n\\n            # Collapse multiple spaces and lowercase\\n            title = re.sub(r\"\\\\s+\", \" \", title).strip().lower()\\n            return title\\n\\n        def search_exact_match(results, search_title):\\n            norm_search = normalize_title(search_title)\\n            for r in results:\\n                tmdb_title = r.get(\"title\") or r.get(\"name\") or \"\"\\n                if normalize_title(tmdb_title) == norm_search:\\n                    return r\\n            return None\\n\\n        def search_movie_or_tv(title, year=None):\\n            # First: try movie search\\n            movie_url = \"https://api.themoviedb.org/3/search/movie\"\\n            params = {\"query\": title}\\n            if year:\\n                params[\"year\"] = year\\n\\n            response = requests.get(movie_url, headers=HEADERS, params=params)\\n            if response.status_code == 200:\\n                results = response.json().get(\"results\", [])\\n                match = search_exact_match(results, title)\\n                if match:\\n                    match[\"media_type\"] = \"movie\"\\n                    return match\\n\\n            # Second: try TV search\\n            tv_url = \"https://api.themoviedb.org/3/search/tv\"\\n            params = {\"query\": title}\\n            if year:\\n                params[\"first_air_date_year\"] = year\\n\\n            response = requests.get(tv_url, headers=HEADERS, params=params)\\n            if response.status_code == 200:\\n                results = response.json().get(\"results\", [])\\n                match = search_exact_match(results, title)\\n                if match:\\n                    match[\"media_type\"] = \"tv\"\\n                    return match\\n\\n            # print(f\" Error or no match for \\'{title}\\' \")\\n            return None\\n\\n        def get_movie_details(tmdb_id):\\n            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\\n            response = requests.get(url, headers=HEADERS)\\n            if response.status_code == 200:\\n                return response.json()\\n            else:\\n                return None\\n\\n\\n        def get_details(tmdb_id, media_type):\\n            url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}\"\\n            response = requests.get(url, headers=HEADERS)\\n            return response.json() if response.status_code == 200 else None\\n\\n        def enrich_dataframe(df):\\n            tmdb_base_url = \"https://www.themoviedb.org/\"\\n            poster_base_url = \"https://image.tmdb.org/t/p/\"\\n            size = \"original\"\\n\\n            # Create empty columns for TMDB data\\n            df[\"tmdb_url\"] = None\\n            df[\"overview\"] = None\\n            df[\"genres\"] = None\\n            df[\"runtime\"] = None\\n            df[\"vote_average\"] = None\\n            df[\"poster_url\"] = None\\n            df[\"media_type\"] = None  # New column: \"movie\" or \"tv\"\\n\\n            total = len(df)\\n            print(f\"Starting enrichment for {total} titles using TMDB data...\\n\")\\n\\n            for idx, row in df.iterrows():\\n                title = row[\"Title\"]\\n                year = row.get(\"Year\", None)\\n\\n                print(f\"[{idx+1}/{total}] Searching: \\'{title}\\' ({year})\", end=\"\")\\n\\n                result = search_movie_or_tv(title, year)\\n                if result:\\n                    print(\" ‚úÖ Match found\")\\n                    media_type = result[\"media_type\"]\\n                    details = get_details(result[\"id\"], media_type)\\n\\n                    if details:\\n                        df.at[idx, \"media_type\"] = media_type\\n                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + f\"{media_type}/\" + str(result[\"id\"])\\n                        df.at[idx, \"overview\"] = details.get(\"overview\")\\n                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\\n                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\\n\\n                        # Only add runtime for movies\\n                        if media_type == \"movie\":\\n                            df.at[idx, \"runtime\"] = details.get(\"runtime\")\\n                        else:\\n                            df.at[idx, \"runtime\"] = None\\n\\n                        poster_path = details.get(\"poster_path\")\\n                        if poster_path:\\n                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\\n                else:\\n                    print(\" ‚ùå No exact match found\")\\n\\n                sleep(0.25)\\n\\n            print(\"\\n‚úîÔ∏è  Enrichment completed.\")\\n            return df\\n\\n\\n\\n        enriched_merged = enrich_dataframe(merged)\\n        enriched_merged.to_csv(\"data/enriched_merged.csv\", index=False)\\n\\n        this is my code. Adapt it so that it includes a new column that includes the directors name. another column should include an array of actor names, and another column should include an array of character names.\"\\n\\n\\n    \"i want to store them like the genres:\\n        [\\'Adventure\\', \\'Drama\\', \\'Science Fiction\\']\\n        also, it should be possible to have multiple directors.\\n        please give me an updated \"enrich dataframe\" function\"\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Knowledge Graph",
   "id": "5f0d3d84584b841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T20:59:42.682994Z",
     "start_time": "2025-09-16T20:59:41.865972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "# Load the CSV\n",
    "file_path = \"../data/enriched_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Helper: convert TMDB URLs to local IDs like movie123, person456\n",
    "def get_local_id(tmdb_url):\n",
    "    if not isinstance(tmdb_url, str):\n",
    "        return None  # Invalid or missing\n",
    "    match = re.search(r'themoviedb\\.org/(movie|tv|person|genre|company)/(\\d+)', tmdb_url)\n",
    "    if match:\n",
    "        entity_type, entity_id = match.groups()\n",
    "        return f\"{entity_type}{entity_id}\"\n",
    "    return None\n",
    "\n",
    "triples = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tmdb_url = row.get('tmdb_url')\n",
    "    if not isinstance(tmdb_url, str) or not tmdb_url.startswith(\"http\"):\n",
    "        continue  # Skip rows with invalid or missing TMDB URL\n",
    "\n",
    "    # Extract ID and media type\n",
    "    try:\n",
    "        tmdb_id = tmdb_url.rstrip('/').split(\"/\")[-1]\n",
    "        media_type = str(row.get(\"media_type\", \"movie\")).strip().lower()\n",
    "        if media_type not in [\"movie\", \"tv\"]:\n",
    "            media_type = \"movie\"  # default fallback\n",
    "        subj = f\"{media_type}{tmdb_id}\"\n",
    "    except Exception as e:\n",
    "        continue  # Skip this row if any error occurs\n",
    "\n",
    "    # Basic movie/tv info\n",
    "    triples.append((subj, \"rdf:type\", f\"schema:{media_type.capitalize()}\"))\n",
    "    triples.append((subj, \"schema:name\", row[\"Title\"]))\n",
    "    triples.append((subj, \"schema:datePublished\", \"published_\" + str(row[\"Year\"])))\n",
    "    triples.append((subj, \"schema:aggregateRating\", \"avgVote_\" + str(row[\"vote_average\"])))\n",
    "    triples.append((subj, \"schema:review\", \"personalVote_\" + str(row[\"Rating\"])))\n",
    "    triples.append((subj, \"ex:liked\", \"liked_\" + str(row[\"Liked\"])))\n",
    "    triples.append((subj, \"ex:lastWatched\", \"lastWatched_\" + str(row[\"Last watched date\"])))\n",
    "    triples.append((subj, \"ex:timesWatched\", \"timesWatched_\" + str(row[\"Times watched\"])))\n",
    "    triples.append((subj, \"ex:originalLanguage\", str(row[\"original_language\"])))\n",
    "    triples.append((subj, \"ex:popularity\", \"popul_\" + str(row[\"popularity\"])))\n",
    "\n",
    "\n",
    "    if not pd.isna(row[\"runtime\"]):\n",
    "        triples.append((subj, \"schema:duration\", \"runtime\" + str(int(row[\"runtime\"]))))\n",
    "\n",
    "    # Directors\n",
    "    try:\n",
    "        directors_raw = literal_eval(row[\"director\"])\n",
    "        for entry in directors_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            director_id = get_local_id(url)  # e.g. person1673654\n",
    "            if not director_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:director\", director_id))\n",
    "            triples.append((director_id, \"rdf:type\", \"schema:Person\"))\n",
    "            triples.append((director_id, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Actors\n",
    "    try:\n",
    "        actors_raw = literal_eval(row[\"actors\"])\n",
    "        for entry in actors_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            actor_id = get_local_id(url)  # e.g. person12345\n",
    "            if not actor_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:actor\", actor_id))\n",
    "            triples.append((actor_id, \"rdf:type\", \"schema:Person\"))\n",
    "            triples.append((actor_id, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Characters\n",
    "    try:\n",
    "        characters = literal_eval(row[\"characters\"])\n",
    "        for character in characters:\n",
    "            triples.append((subj, \"schema:character\", character))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Genres\n",
    "    try:\n",
    "        genres_raw = literal_eval(row[\"genres\"])\n",
    "        for entry in genres_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            genre_id = get_local_id(url)  # e.g. genre18\n",
    "            if not genre_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:genre\", genre_id))\n",
    "            triples.append((genre_id, \"rdf:type\", \"schema:Text\"))\n",
    "            triples.append((genre_id, \"ex:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Countries Of Origin\n",
    "    try:\n",
    "        countriesOfOrigin = literal_eval(row[\"origin_country\"])\n",
    "        for entry in countriesOfOrigin:\n",
    "            triples.append((subj, \"schema:countryOfOrigin\", entry))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Production Companies\n",
    "    try:\n",
    "        companies_raw = literal_eval(row[\"production_companies\"])\n",
    "        for entry in companies_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, rest = entry.split(\":\", 1)\n",
    "            url, country = rest.rsplit(\":\", 1)\n",
    "            company_id = get_local_id(url)\n",
    "            if not company_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:productionCompany\", company_id))\n",
    "            triples.append((company_id, \"rdf:type\", \"schema:Company\"))\n",
    "            triples.append((company_id, \"schema:name\", name.strip()))\n",
    "            triples.append((company_id, \"ex:country\", country))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Production Countries\n",
    "    try:\n",
    "        countries_raw = literal_eval(row[\"production_countries\"])\n",
    "        for entry in countries_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue\n",
    "            name, code = entry.split(\":\", 1)\n",
    "            triples.append((subj, \"ex:productionCountry\", code))\n",
    "            triples.append((code, \"rdf:type\", \"schema:Country\"))\n",
    "            triples.append((code, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Spoken Languages\n",
    "    try:\n",
    "        languages_raw = literal_eval(row[\"spoken_languages\"])\n",
    "        for entry in languages_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue\n",
    "            name, code = entry.split(\":\", 1)\n",
    "            triples.append((subj, \"schema:inLanguage\", code))\n",
    "            triples.append((code, \"rdf:type\", \"schema:Language\"))\n",
    "            triples.append((code, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame and export for PyKEEN\n",
    "triples_df = pd.DataFrame(triples, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "triples_df.to_csv(\"../data/kg/triples/movie_kg_triples.tsv\", sep=\"\\t\", index=False, header=False)\n"
   ],
   "id": "2960040f242b15de",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:55:52.024594Z",
     "start_time": "2025-09-14T15:55:51.972946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cells is in big parts AI generated by the free and paid version of ChatGPT and was afterwards heavily adapted by me. Since it is not possible to accurately say which parts were originaly AI generated by wich promt, I have included all prompts that were used on this file here.\n",
    "These following prompts were used:\n",
    "\n",
    "\n",
    "    \"i now want to build a knowledge graph from all my watched movies using RDF\"\n",
    "\n",
    "\n",
    "    \"erstellt das triple in der SVO struktur, um diese sp√§ter mit PyKEEN weiter zu verarbeiten?\"\n",
    "\n",
    "\n",
    "    \"verwende TMDB url anstelle von Letterboxd url als subjekt\"\n",
    "\n",
    "\n",
    "    \"if that is turtles syntax, do I need to import anything?\n",
    "\n",
    "        please write me all the code in block again\"\n",
    "\n",
    "\n",
    "    \"import pandas as pd\n",
    "        from ast import literal_eval\n",
    "\n",
    "        # Load the CSV\n",
    "        file_path = \"data/enriched_merged.csv\"\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Build triple list\n",
    "        triples = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            movie_uri = row['tmdb_url']\n",
    "\n",
    "            # Basic movie info\n",
    "            triples.append((movie_uri, \"rdf:type\", \"schema:Movie\"))\n",
    "            triples.append((movie_uri, \"schema:name\", row[\"Title\"]))\n",
    "            triples.append((movie_uri, \"schema:datePublished\", str(row[\"Year\"])))\n",
    "            triples.append((movie_uri, \"schema:rating\", str(row[\"vote_average\"])))\n",
    "\n",
    "            if not pd.isna(row[\"runtime\"]):\n",
    "                triples.append((movie_uri, \"schema:duration\", str(int(row[\"runtime\"]))))\n",
    "\n",
    "            # Directors\n",
    "            try:\n",
    "                directors = literal_eval(row[\"director\"])\n",
    "                for director in directors:\n",
    "                    dir_uri = director\n",
    "                    triples.append((movie_uri, \"schema:director\", dir_uri))\n",
    "                    triples.append((dir_uri, \"rdf:type\", \"schema:Person\"))\n",
    "                    triples.append((dir_uri, \"schema:name\", director))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Actors\n",
    "            try:\n",
    "                actors = literal_eval(row[\"actors\"])\n",
    "                for actor in actors:\n",
    "                    actor_uri = actor\n",
    "                    triples.append((movie_uri, \"schema:actor\", actor_uri))\n",
    "                    triples.append((actor_uri, \"rdf:type\", \"schema:Person\"))\n",
    "                    triples.append((actor_uri, \"schema:name\", actor))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Genres\n",
    "            try:\n",
    "                genres = literal_eval(row[\"genres\"])\n",
    "                for genre in genres:\n",
    "                    genre_uri = genre\n",
    "                    triples.append((movie_uri, \"schema:genre\", genre_uri))\n",
    "                    triples.append((genre_uri, \"rdf:type\", \"schema:Genre\"))\n",
    "                    triples.append((genre_uri, \"schema:name\", genre))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Convert to DataFrame or export for PyKEEN\n",
    "        triples_df = pd.DataFrame(triples, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "\n",
    "        # Optional: save to TSV for PyKEEN\n",
    "        triples_df.to_csv(\"data/movie_kg_triples.tsv\", sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "        this is my current code. make the following changes:\n",
    "        subject should use the following coding:\n",
    "        movies get \"movie<tmdbNr>\"\n",
    "        genres get \"genre<tmdbNr>\"\n",
    "        people get \"person<tmdbNr>\"\n",
    "        tv shows get \"tv<tmdbNr>\"\n",
    "\n",
    "        for example, instead of using \"https://www.themoviedb.org/person/77870\" as a subject, instead use \"person77870\"\"\n",
    "\n",
    "\n",
    "    \"---------------------------------------------------------------------------\n",
    "        AttributeError                            Traceback (most recent call last)\n",
    "        Cell In[28], line 25\n",
    "             23 for _, row in df.iterrows():\n",
    "             24     tmdb_url = row['tmdb_url']\n",
    "        ---> 25     tmdb_id = tmdb_url.rstrip('/').split(\"/\")[-1]\n",
    "             27     media_type = row.get(\"media_type\", \"movie\")\n",
    "             28     subj = f\"{media_type}{tmdb_id}\"\n",
    "\n",
    "        AttributeError: 'float' object has no attribute 'rstrip'\"\n",
    "\n",
    "\n",
    "    \"# Directors\n",
    "            try:\n",
    "                directors = literal_eval(row[\"director\"])\n",
    "                for director in directors:\n",
    "                    director_id = get_local_id(director)\n",
    "                    if not director_id:\n",
    "                        continue\n",
    "                    triples.append((subj, \"schema:director\", director_id))\n",
    "                    triples.append((director_id, \"rdf:type\", \"schema:Person\"))\n",
    "                    triples.append((director_id, \"schema:name\", director))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        alter this code.\n",
    "        the row in my CSV look like this now, it includes name AND id.\n",
    "        I want the subject of the director to be person<tmdbNr> like it already was, and I want the name attributed changed to \"person1673654      schema:name      Jon Nguyen\n",
    "        [Jon Nguyen:http://www.themoviedb.org/person/1673654, Rick Barnes:http://www.themoviedb.org/person/1756857, Olivia Neergaard-Holm:http://www.themoviedb.org/person/1425110]\"\n",
    "\n",
    "\n",
    "    \"now do the same for actors\"\n",
    "\n",
    "\n",
    "    \"the same for genres\"\n",
    "\n",
    "\n",
    "    \"[Thunder Road:https://www.themoviedb.org/company/3528:US, 87Eleven:https://www.themoviedb.org/company/23008:US, Lionsgate:https://www.themoviedb.org/company/1632:US]\n",
    "\n",
    "        try:\n",
    "                companies_raw = literal_eval(row[\"production_companies\"])\n",
    "                for entry in companies_raw:\n",
    "                    if \":\" not in entry:\n",
    "                        continue # skip malformed\n",
    "                    name, url, country = entry.split(\":\", 2)\n",
    "                    print(\"Name:\" + name + \" URL:\" + url + \" Country:\" + country)\n",
    "                    company_id = get_local_id(url)\n",
    "                    if not company_id:\n",
    "                        continue\n",
    "                    triples.append((subj, \"schema:productionCompany\", company_id))\n",
    "                    triples.append((company_id, \"rdf:type\", \"schema:Company\"))\n",
    "                    triples.append((company_id, \"schema:name\", name.strip()))\n",
    "                    triples.append((company_id, \"ex:country\", country))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        this obviously does not work, since the link itself contains a \":\" after the https\n",
    "\n",
    "        how can I fix this? i know that the link will always start with \"https://\"\"\n",
    "\n",
    "\n",
    "'''"
   ],
   "id": "cf518c84366b6a98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code in the previous cells is in big parts AI generated by the free and paid version of ChatGPT and was afterwards heavily adapted by me. Since it is not possible to accurately say which parts were originaly AI generated by wich promt, I have included all prompts that were used on this file here.\\nThese following prompts were used:\\n\\n\\n    \"i now want to build a knowledge graph from all my watched movies using RDF\"\\n\\n\\n    \"erstellt das triple in der SVO struktur, um diese sp√§ter mit PyKEEN weiter zu verarbeiten?\"\\n\\n\\n    \"verwende TMDB url anstelle von Letterboxd url als subjekt\"\\n\\n\\n    \"if that is turtles syntax, do I need to import anything?\\n\\n        please write me all the code in block again\"\\n\\n\\n    \"import pandas as pd\\n        from ast import literal_eval\\n\\n        # Load the CSV\\n        file_path = \"data/enriched_merged.csv\"\\n        df = pd.read_csv(file_path)\\n\\n        # Build triple list\\n        triples = []\\n\\n        for _, row in df.iterrows():\\n            movie_uri = row[\\'tmdb_url\\']\\n\\n            # Basic movie info\\n            triples.append((movie_uri, \"rdf:type\", \"schema:Movie\"))\\n            triples.append((movie_uri, \"schema:name\", row[\"Title\"]))\\n            triples.append((movie_uri, \"schema:datePublished\", str(row[\"Year\"])))\\n            triples.append((movie_uri, \"schema:rating\", str(row[\"vote_average\"])))\\n\\n            if not pd.isna(row[\"runtime\"]):\\n                triples.append((movie_uri, \"schema:duration\", str(int(row[\"runtime\"]))))\\n\\n            # Directors\\n            try:\\n                directors = literal_eval(row[\"director\"])\\n                for director in directors:\\n                    dir_uri = director\\n                    triples.append((movie_uri, \"schema:director\", dir_uri))\\n                    triples.append((dir_uri, \"rdf:type\", \"schema:Person\"))\\n                    triples.append((dir_uri, \"schema:name\", director))\\n            except:\\n                pass\\n\\n            # Actors\\n            try:\\n                actors = literal_eval(row[\"actors\"])\\n                for actor in actors:\\n                    actor_uri = actor\\n                    triples.append((movie_uri, \"schema:actor\", actor_uri))\\n                    triples.append((actor_uri, \"rdf:type\", \"schema:Person\"))\\n                    triples.append((actor_uri, \"schema:name\", actor))\\n            except:\\n                pass\\n\\n            # Genres\\n            try:\\n                genres = literal_eval(row[\"genres\"])\\n                for genre in genres:\\n                    genre_uri = genre\\n                    triples.append((movie_uri, \"schema:genre\", genre_uri))\\n                    triples.append((genre_uri, \"rdf:type\", \"schema:Genre\"))\\n                    triples.append((genre_uri, \"schema:name\", genre))\\n            except:\\n                pass\\n\\n        # Convert to DataFrame or export for PyKEEN\\n        triples_df = pd.DataFrame(triples, columns=[\"subject\", \"predicate\", \"object\"])\\n\\n        # Optional: save to TSV for PyKEEN\\n        triples_df.to_csv(\"data/movie_kg_triples.tsv\", sep=\"\\t\", index=False, header=False)\\n\\n        this is my current code. make the following changes:\\n        subject should use the following coding:\\n        movies get \"movie<tmdbNr>\"\\n        genres get \"genre<tmdbNr>\"\\n        people get \"person<tmdbNr>\"\\n        tv shows get \"tv<tmdbNr>\"\\n\\n        for example, instead of using \"https://www.themoviedb.org/person/77870\" as a subject, instead use \"person77870\"\"\\n\\n\\n    \"---------------------------------------------------------------------------\\n        AttributeError                            Traceback (most recent call last)\\n        Cell In[28], line 25\\n             23 for _, row in df.iterrows():\\n             24     tmdb_url = row[\\'tmdb_url\\']\\n        ---> 25     tmdb_id = tmdb_url.rstrip(\\'/\\').split(\"/\")[-1]\\n             27     media_type = row.get(\"media_type\", \"movie\")\\n             28     subj = f\"{media_type}{tmdb_id}\"\\n\\n        AttributeError: \\'float\\' object has no attribute \\'rstrip\\'\"\\n\\n\\n    \"# Directors\\n            try:\\n                directors = literal_eval(row[\"director\"])\\n                for director in directors:\\n                    director_id = get_local_id(director)\\n                    if not director_id:\\n                        continue\\n                    triples.append((subj, \"schema:director\", director_id))\\n                    triples.append((director_id, \"rdf:type\", \"schema:Person\"))\\n                    triples.append((director_id, \"schema:name\", director))\\n            except:\\n                pass\\n\\n\\n        alter this code.\\n        the row in my CSV look like this now, it includes name AND id.\\n        I want the subject of the director to be person<tmdbNr> like it already was, and I want the name attributed changed to \"person1673654      schema:name      Jon Nguyen\\n        [Jon Nguyen:http://www.themoviedb.org/person/1673654, Rick Barnes:http://www.themoviedb.org/person/1756857, Olivia Neergaard-Holm:http://www.themoviedb.org/person/1425110]\"\\n\\n\\n    \"now do the same for actors\"\\n\\n\\n    \"the same for genres\"\\n\\n\\n    \"[Thunder Road:https://www.themoviedb.org/company/3528:US, 87Eleven:https://www.themoviedb.org/company/23008:US, Lionsgate:https://www.themoviedb.org/company/1632:US]\\n\\n        try:\\n                companies_raw = literal_eval(row[\"production_companies\"])\\n                for entry in companies_raw:\\n                    if \":\" not in entry:\\n                        continue # skip malformed\\n                    name, url, country = entry.split(\":\", 2)\\n                    print(\"Name:\" + name + \" URL:\" + url + \" Country:\" + country)\\n                    company_id = get_local_id(url)\\n                    if not company_id:\\n                        continue\\n                    triples.append((subj, \"schema:productionCompany\", company_id))\\n                    triples.append((company_id, \"rdf:type\", \"schema:Company\"))\\n                    triples.append((company_id, \"schema:name\", name.strip()))\\n                    triples.append((company_id, \"ex:country\", country))\\n            except:\\n                pass\\n\\n        this obviously does not work, since the link itself contains a \":\" after the https\\n\\n        how can I fix this? i know that the link will always start with \"https://\"\"\\n\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert to .ttl (Delete this again, as it is not necessary)",
   "id": "d7b7c368676c50bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:55:53.727452Z",
     "start_time": "2025-09-14T15:55:52.060890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# new\n",
    "\n",
    "# Re-run code after environment reset\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, XSD\n",
    "\n",
    "# Reload the uploaded TSV file\n",
    "file_path = \"../data/kg/triples/movie_kg_triples.tsv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"subject\", \"predicate\", \"object\"])\n",
    "\n",
    "# Define namespaces\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "EX = Namespace(\"http://example.org/\")\n",
    "BASE = Namespace(\"http://your-kg.org/resource/\")\n",
    "\n",
    "# Initialize RDF graph\n",
    "g = Graph()\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"ex\", EX)\n",
    "g.bind(\"rdf\", RDF)\n",
    "\n",
    "# Add triples to RDF graph\n",
    "for s, p, o in df.itertuples(index=False):\n",
    "    subj = URIRef(BASE + s)\n",
    "\n",
    "    # Determine predicate namespace\n",
    "    if p.startswith(\"schema:\"):\n",
    "        pred = SCHEMA[p.split(\"schema:\")[1]]\n",
    "    elif p.startswith(\"rdf:\"):\n",
    "        pred = RDF[p.split(\"rdf:\")[1]]\n",
    "    elif p.startswith(\"ex:\"):\n",
    "        pred = EX[p.split(\"ex:\")[1]]\n",
    "    else:\n",
    "        pred = URIRef(BASE + p)\n",
    "\n",
    "    # Decide whether object is URI or Literal\n",
    "    if isinstance(o, str) and re.match(r'^(movie|tv|person|genre|company)\\d+$', o):\n",
    "        obj = URIRef(BASE + o)\n",
    "    elif isinstance(o, str) and o.startswith(\"http\"):\n",
    "        obj = URIRef(o)\n",
    "    else:\n",
    "        try:\n",
    "            if \".\" in o:\n",
    "                obj = Literal(float(o))\n",
    "            else:\n",
    "                obj = Literal(int(o))\n",
    "        except:\n",
    "            obj = Literal(o)\n",
    "\n",
    "    g.add((subj, pred, obj))\n",
    "\n",
    "# Serialize to Turtle format\n",
    "ttl_path = \"data/kg/movie_kg.ttl\"\n",
    "g.serialize(destination=ttl_path, format=\"turtle\")\n",
    "ttl_path\n"
   ],
   "id": "9e5bce77e6e8dd48",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/kg/movie_kg.ttl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 57\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Serialize to Turtle format\u001B[39;00m\n\u001B[1;32m     56\u001B[0m ttl_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata/kg/movie_kg.ttl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 57\u001B[0m g\u001B[38;5;241m.\u001B[39mserialize(destination\u001B[38;5;241m=\u001B[39mttl_path, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mturtle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     58\u001B[0m ttl_path\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.13/site-packages/rdflib/graph.py:1407\u001B[0m, in \u001B[0;36mGraph.serialize\u001B[0;34m(self, destination, format, base, encoding, **args)\u001B[0m\n\u001B[1;32m   1405\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1406\u001B[0m             os_path \u001B[38;5;241m=\u001B[39m location\n\u001B[0;32m-> 1407\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m stream:\n\u001B[1;32m   1408\u001B[0m         serializer\u001B[38;5;241m.\u001B[39mserialize(stream, base\u001B[38;5;241m=\u001B[39mbase, encoding\u001B[38;5;241m=\u001B[39mencoding, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/kg/movie_kg.ttl'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# old\n",
    "\n",
    "\n",
    "# convert_tsv_to_ttl.py\n",
    "\n",
    "INPUT_FILE = \"../data/kg/triples/movie_kg_triples.tsv\"\n",
    "OUTPUT_FILE = \"../to_delete/triples.ttl\"\n",
    "\n",
    "# Prefixes\n",
    "prefixes = [\n",
    "    \"@prefix schema: <http://schema.org/> .\",\n",
    "    \"@prefix ex: <http://example.org/> .\",\n",
    "    \"@prefix tmdb: <https://www.themoviedb.org/> .\",\n",
    "    \"@prefix : <http://example.org/> .\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "def escape(term):\n",
    "    # Basic escaping for URIs (if needed)\n",
    "    return term.replace(\" \", \"_\")\n",
    "\n",
    "def convert_tsv_to_ttl(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    triples = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue  # Skip malformed lines\n",
    "\n",
    "        subj, pred, obj = map(escape, parts)\n",
    "\n",
    "        # Expand schema: to full URI\n",
    "        if pred.startswith(\"schema:\"):\n",
    "            predicate = pred  # already prefixed\n",
    "        else:\n",
    "            predicate = f\":{pred}\"\n",
    "\n",
    "        triple = f\":{subj} {predicate} :{obj} .\"\n",
    "        triples.append(triple)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(prefixes + triples))\n",
    "\n",
    "    print(f\"‚úÖ Converted {len(triples)} triples to {output_file}\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    convert_tsv_to_ttl(INPUT_FILE, OUTPUT_FILE)\n"
   ],
   "id": "f38f971e3ba8a31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
