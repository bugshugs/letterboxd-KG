{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bffa82c",
   "metadata": {},
   "source": [
    "# Film-Recommender — LO3 mit echten Labels & Zeit-Split\n",
    "*Generated: 2025-09-12T15:52:40 UTC*\n",
    "\n",
    "Dieses Notebook erweitert dein GNN-Reranking um:\n",
    "- **Echte Labels** aus dem Letterboxd-Export (Ratings / Watchlist) statt Pseudo-Labels\n",
    "- **Zeitbasierten Split** (Train auf älteren Interaktionen, Test auf späteren)\n",
    "- **Evaluation** (Hit@K, NDCG@K, Recall@K) für Baseline (`final`), **GNN**, und **Ensemble**\n",
    "- **Top-K-Exporte** (CSV) pro Seed für Baseline/GNN/Ensemble\n",
    "\n",
    "> Es bleibt kompatibel mit deinem CSV **`../data/kg/rerank_by_logical_rules.csv`** (aus Sicht dieses Notebooks in `gnn/`).  \n",
    "> PyTorch/pyG-Installationszelle ist enthalten (optional lokal ausführen).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b820298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.086629Z",
     "start_time": "2025-09-14T11:16:51.060874Z"
    }
   },
   "source": [
    "# === Konfiguration ===\n",
    "CSV_PATH = \"rerank_by_logical_rules.csv\"\n",
    "LETTERBOXD_DIR = \"../data/letterboxd_export\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "SPLIT_DATE = None   # z.B. \"2024-01-01\"\n",
    "SEED = 42\n",
    "TOPK = 10\n",
    "NEG_PER_POS = 3\n",
    "LAMBDA = 0.6\n",
    "\n",
    "import os, re, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"CSV_PATH:\", CSV_PATH)\n",
    "print(\"LETTERBOXD_DIR:\", LETTERBOXD_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_PATH: ../data/kg/rerank_by_logical_rules.csv\n",
      "LETTERBOXD_DIR: ../data/letterboxd_export\n",
      "OUTPUT_DIR: ../data/kg/outputs\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "b07649e0",
   "metadata": {},
   "source": [
    "## (Optional) Installationen"
   ]
  },
  {
   "cell_type": "code",
   "id": "52a2616b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.093639Z",
     "start_time": "2025-09-14T11:16:51.091907Z"
    }
   },
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-$(python -c \"import torch;print(torch.__version__.split('+')[0])\").html\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "23090cea",
   "metadata": {},
   "source": [
    "## 1) Rerank-CSV laden & vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "id": "62e7730d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.136248Z",
     "start_time": "2025-09-14T11:16:51.115200Z"
    }
   },
   "source": [
    "assert Path(CSV_PATH).exists(), f\"CSV nicht gefunden: {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "df.head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 41)\n",
      "['candidate_id', 'candidate_title', 'year', 'cos', 'meta', 'final', 'seed', 'comp_genres', 'comp_keywords', 'comp_cast', 'comp_director', 'comp_runtime', 'comp_language', 'comp_popularity', 'comp_vote', 'tmdb_url', 'overview', 'genres', 'runtime', 'vote_average', 'poster_url', 'media_type', 'director', 'actors', 'characters', 'origin_country', 'original_language', 'popularity', 'production_companies', 'production_countries', 'spoken_languages', 'name_norm', 'year_str', 'genre_list', 'director_list', 'watchlist_priority', 'genre_boost', 'director_boost', 'genre_penalty', 'director_penalty', 'score']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   candidate_id        candidate_title    year  cos    meta   final  \\\n",
       "0          2756              The Abyss  1989.0  0.0  0.4592  0.1837   \n",
       "1          1991            Death Proof  2007.0  0.0  0.5010  0.2004   \n",
       "2         28387  Kicking and Screaming  1995.0  0.0  0.5109  0.2044   \n",
       "\n",
       "                                        seed  comp_genres  comp_keywords  \\\n",
       "0                                     Aliens       0.5000         0.0000   \n",
       "1                          Kill Bill: Vol. 2       0.6667         0.0179   \n",
       "2  The Meyerowitz Stories (New and Selected)       0.6667         0.0000   \n",
       "\n",
       "   comp_cast  ...              name_norm  year_str  \\\n",
       "0     0.0256  ...              the abyss    1989.0   \n",
       "1     0.0526  ...            death proof    2007.0   \n",
       "2     0.0256  ...  kicking and screaming    1995.0   \n",
       "\n",
       "                                     genre_list          director_list  \\\n",
       "0  ['Adventure', 'Thriller', 'Science Fiction']      ['James Cameron']   \n",
       "1                        ['Action', 'Thriller']  ['Quentin Tarantino']   \n",
       "2                ['Comedy', 'Drama', 'Romance']      ['Noah Baumbach']   \n",
       "\n",
       "   watchlist_priority genre_boost director_boost genre_penalty  \\\n",
       "0                True        True           True         False   \n",
       "1               False        True           True         False   \n",
       "2               False        True           True         False   \n",
       "\n",
       "   director_penalty  score  \n",
       "0             False      5  \n",
       "1             False      3  \n",
       "2             False      3  \n",
       "\n",
       "[3 rows x 41 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>candidate_title</th>\n",
       "      <th>year</th>\n",
       "      <th>cos</th>\n",
       "      <th>meta</th>\n",
       "      <th>final</th>\n",
       "      <th>seed</th>\n",
       "      <th>comp_genres</th>\n",
       "      <th>comp_keywords</th>\n",
       "      <th>comp_cast</th>\n",
       "      <th>...</th>\n",
       "      <th>name_norm</th>\n",
       "      <th>year_str</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>director_list</th>\n",
       "      <th>watchlist_priority</th>\n",
       "      <th>genre_boost</th>\n",
       "      <th>director_boost</th>\n",
       "      <th>genre_penalty</th>\n",
       "      <th>director_penalty</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2756</td>\n",
       "      <td>The Abyss</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4592</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>Aliens</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>...</td>\n",
       "      <td>the abyss</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>['Adventure', 'Thriller', 'Science Fiction']</td>\n",
       "      <td>['James Cameron']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>Death Proof</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>Kill Bill: Vol. 2</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>...</td>\n",
       "      <td>death proof</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>['Action', 'Thriller']</td>\n",
       "      <td>['Quentin Tarantino']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28387</td>\n",
       "      <td>Kicking and Screaming</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5109</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>The Meyerowitz Stories (New and Selected)</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>...</td>\n",
       "      <td>kicking and screaming</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance']</td>\n",
       "      <td>['Noah Baumbach']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "7e1f0736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.194050Z",
     "start_time": "2025-09-14T11:16:51.188112Z"
    }
   },
   "source": [
    "num_like = ['cos','final','score','comp_genres','comp_keywords','comp_cast','comp_director',\n",
    "            'comp_runtime','comp_language','comp_popularity','comp_vote']\n",
    "for c in num_like:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "def minmax(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mn, mx = np.nanmin(x), np.nanmax(x)\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx) or mx<=mn:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "df['seed'] = df['seed'].astype(str)\n",
    "df['candidate_title'] = df['candidate_title'].astype(str)\n",
    "print(\"Zeilen:\", len(df))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen: 200\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "aeb5b03a",
   "metadata": {},
   "source": [
    "## 2) Graph-Kanten aus CSV bauen"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b1eb385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.284994Z",
     "start_time": "2025-09-14T11:16:51.274222Z"
    }
   },
   "source": [
    "movies = pd.unique(pd.concat([df['seed'], df['candidate_title']], ignore_index=True))\n",
    "movie2id = {m:i for i,m in enumerate(movies)}\n",
    "id2movie = {i:m for m,i in movie2id.items()}\n",
    "\n",
    "rel_cols = [c for c in ['cos','final','comp_genres','comp_keywords','comp_cast','comp_director',\n",
    "                        'comp_runtime','comp_language','comp_popularity','comp_vote'] if c in df.columns]\n",
    "\n",
    "edges = {c: [] for c in rel_cols}\n",
    "for _, row in df.iterrows():\n",
    "    s = row['seed']; c = row['candidate_title']\n",
    "    if pd.isna(s) or pd.isna(c): \n",
    "        continue\n",
    "    u, v = movie2id[s], movie2id[c]\n",
    "    for rc in rel_cols:\n",
    "        edges[rc].append((u, v, float(row.get(rc, 0.0))))\n",
    "\n",
    "norm_edges = {}\n",
    "for rc, lst in edges.items():\n",
    "    if not lst:\n",
    "        continue\n",
    "    w = np.array([w for (_,_,w) in lst], dtype=float)\n",
    "    wn = minmax(w)\n",
    "    norm_edges[rc] = [(u,v,float(wn[i])) for i,(u,v,_) in enumerate(lst)]\n",
    "\n",
    "for rc, lst in norm_edges.items():\n",
    "    print(rc, \"Edges:\", len(lst))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos Edges: 200\n",
      "final Edges: 200\n",
      "comp_genres Edges: 200\n",
      "comp_keywords Edges: 200\n",
      "comp_cast Edges: 200\n",
      "comp_director Edges: 200\n",
      "comp_runtime Edges: 200\n",
      "comp_language Edges: 200\n",
      "comp_popularity Edges: 200\n",
      "comp_vote Edges: 200\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "1512c0af",
   "metadata": {},
   "source": [
    "## 3) Letterboxd-Labels laden (Ratings / Watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "id": "83180ad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.365768Z",
     "start_time": "2025-09-14T11:16:51.325172Z"
    }
   },
   "source": [
    "def smart_find(base_dir, primary_name_patterns, fallback_exts=('csv', 'CSV')):\n",
    "    base = Path(base_dir)\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    for pat in primary_name_patterns:\n",
    "        for ext in fallback_exts:\n",
    "            cand = base / f\"{pat}.{ext}\"\n",
    "            if cand.exists():\n",
    "                return str(cand)\n",
    "    for p in base.rglob(\"*\"):\n",
    "        name = p.name.lower()\n",
    "        for pat in primary_name_patterns:\n",
    "            if pat.lower() in name and p.suffix.lower() in ('.csv',):\n",
    "                return str(p)\n",
    "    return None\n",
    "\n",
    "ratings_path   = smart_find(LETTERBOXD_DIR, ['ratings', 'ratings-export', 'ratings-2'])\n",
    "watched_path   = smart_find(LETTERBOXD_DIR, ['watched', 'diary'])\n",
    "watchlist_path = smart_find(LETTERBOXD_DIR, ['watchlist'])\n",
    "\n",
    "print(\"ratings_path:\", ratings_path)\n",
    "print(\"watched_path:\", watched_path)\n",
    "print(\"watchlist_path:\", watchlist_path)\n",
    "\n",
    "assert ratings_path or watched_path or watchlist_path, \"Keine Letterboxd-CSV gefunden. Prüfe LETTERBOXD_DIR.\"\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "def normalize_title(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"[^a-z0-9]+\", \" \", t)\n",
    "    t = re.sub(r\"\\b(the|a|an)\\b\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def parse_date_series(s):\n",
    "    if s is None:\n",
    "        return pd.Series(dtype='datetime64[ns]')\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s.astype(str), errors='coerce', utc=True)\n",
    "\n",
    "def parse_rating_series(s):\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=float)\n",
    "    def to_num(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            txt = str(x)\n",
    "            stars = txt.count('★')\n",
    "            half  = '½' in txt\n",
    "            return stars + (0.5 if half else 0.0)\n",
    "    return s.apply(to_num).astype(float)\n",
    "\n",
    "frames = []\n",
    "if ratings_path:\n",
    "    r = pd.read_csv(ratings_path)\n",
    "    title = r.get('Name', r.get('Title'))\n",
    "    year  = r.get('Year')\n",
    "    rating= parse_rating_series(r.get('Rating'))\n",
    "    date  = parse_date_series(r.get('Date', r.get('WatchedDate')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': rating, 'date': date, 'watchlist': False}))\n",
    "if watched_path:\n",
    "    w = pd.read_csv(watched_path)\n",
    "    title = w.get('Name', w.get('Title'))\n",
    "    year  = w.get('Year')\n",
    "    rating= parse_rating_series(w.get('Rating'))\n",
    "    date  = parse_date_series(w.get('Date', w.get('WatchedDate')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': rating, 'date': date, 'watchlist': False}))\n",
    "if watchlist_path:\n",
    "    wl = pd.read_csv(watchlist_path)\n",
    "    title = wl.get('Name', wl.get('Title'))\n",
    "    year  = wl.get('Year')\n",
    "    date  = parse_date_series(wl.get('AddedDate', wl.get('Date')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': np.nan, 'date': date, 'watchlist': True}))\n",
    "\n",
    "inter = pd.concat(frames, ignore_index=True).dropna(subset=['title'])\n",
    "inter['title_norm'] = inter['title'].apply(normalize_title)\n",
    "inter['year'] = pd.to_numeric(inter['year'], errors='coerce')\n",
    "inter['date'] = parse_date_series(inter['date'])\n",
    "inter['is_positive'] = (inter['rating'] >= 4.0) | (inter['watchlist'] == True)\n",
    "\n",
    "print(\"Interaktionen (gesamt):\", len(inter))\n",
    "print(\"Davon positive:\", int(inter['is_positive'].sum()))\n",
    "inter.head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_path: ../data/letterboxd_export/ratings.csv\n",
      "watched_path: ../data/letterboxd_export/watched.csv\n",
      "watchlist_path: ../data/letterboxd_export/watchlist.csv\n",
      "Interaktionen (gesamt): 1816\n",
      "Davon positive: 903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                       title    year  rating  \\\n",
       "0                                   Bird Box  2018.0     3.0   \n",
       "1  The Meyerowitz Stories (New and Selected)  2017.0     5.0   \n",
       "2                             Marriage Story  2019.0     5.0   \n",
       "\n",
       "                       date  watchlist                           title_norm  \\\n",
       "0 2020-06-08 00:00:00+00:00      False                             bird box   \n",
       "1 2020-06-08 00:00:00+00:00      False  meyerowitz stories new and selected   \n",
       "2 2020-06-08 00:00:00+00:00      False                       marriage story   \n",
       "\n",
       "   is_positive  \n",
       "0        False  \n",
       "1         True  \n",
       "2         True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>watchlist</th>\n",
       "      <th>title_norm</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bird Box</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020-06-08 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>bird box</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Meyerowitz Stories (New and Selected)</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2020-06-08 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>meyerowitz stories new and selected</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marriage Story</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2020-06-08 00:00:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>marriage story</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "a0ccc5b1",
   "metadata": {},
   "source": [
    "### 3.1 Mapping: Letterboxd-Titel → Kandidaten"
   ]
  },
  {
   "cell_type": "code",
   "id": "061a594a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.412206Z",
     "start_time": "2025-09-14T11:16:51.404939Z"
    }
   },
   "source": [
    "cand_norm = {normalize_title(t): t for t in pd.unique(df['candidate_title'])}\n",
    "seed_norm = {normalize_title(t): t for t in pd.unique(df['seed'])}\n",
    "all_norm  = {**seed_norm, **cand_norm}\n",
    "\n",
    "inter['cand_match'] = inter['title_norm'].map(all_norm)\n",
    "mapped = inter.dropna(subset=['cand_match']).copy()\n",
    "print(\"Gemappte Interaktionen:\", len(mapped), \"von\", len(inter))\n",
    "\n",
    "mapped_pos = mapped[(mapped['is_positive']) & mapped['date'].notna()].copy()\n",
    "assert len(mapped_pos) > 0, \"Keine positiven Interaktionen gemappt. Prüfe Normalisierung/Titel.\"\n",
    "split_date = pd.to_datetime(\"2024-01-01\", utc=True) if False else mapped_pos['date'].quantile(0.8)\n",
    "print(\"Split-Datum:\", split_date)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemappte Interaktionen: 229 von 1816\n",
      "Split-Datum: 2024-05-21 00:00:00+00:00\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "1cb54fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.477187Z",
     "start_time": "2025-09-14T11:16:51.473130Z"
    }
   },
   "source": [
    "train_pos = mapped_pos[mapped_pos['date'] <= split_date]['cand_match'].tolist()\n",
    "test_pos  = mapped_pos[mapped_pos['date'] >  split_date]['cand_match'].tolist()\n",
    "\n",
    "train_pos_ids = [movie2id[t] for t in train_pos if t in movie2id]\n",
    "test_pos_set  = set([t for t in test_pos if t in movie2id])\n",
    "print(\"Train positive IDs:\", len(train_pos_ids), \"| Test positive unique:\", len(test_pos_set))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positive IDs: 49 | Test positive unique: 12\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "e973ec56",
   "metadata": {},
   "source": [
    "## 4) Evaluation-Helper"
   ]
  },
  {
   "cell_type": "code",
   "id": "64e9e12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.521878Z",
     "start_time": "2025-09-14T11:16:51.517165Z"
    }
   },
   "source": [
    "# ✅ NumPy 2.0 kompatible Eval-Helper\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def _to_float_array(x):\n",
    "    return np.asarray(x, dtype=float)\n",
    "\n",
    "def dcg_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    # DCG: (2^rel - 1) / log2(2..k+1)\n",
    "    return float(np.sum((np.power(2.0, r) - 1.0) / np.log2(np.arange(2, r.size + 2))))\n",
    "\n",
    "def ndcg_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    ideal = np.sort(r)[::-1]  # absteigend\n",
    "    idcg = dcg_at_k(ideal, k)\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def hit_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    return float(np.any(r > 0))\n",
    "\n",
    "def recall_at_k(rel, total_pos, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    found = int(np.sum(r))\n",
    "    return float(found / total_pos) if total_pos > 0 else 0.0\n",
    "\n",
    "def eval_grouped_with_test(df, score_col, k=10, test_pos_set=None):\n",
    "    hits, ndcgs, recalls, cnt = [], [], [], 0\n",
    "    for seed, g in df.groupby('seed', sort=False):\n",
    "        g = g.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
    "        rel = (g['candidate_title'].isin(test_pos_set)).astype(int).to_numpy()\n",
    "        total_pos = int(rel.sum())\n",
    "        hits.append(hit_at_k(rel, k))\n",
    "        ndcgs.append(ndcg_at_k(rel, k))\n",
    "        recalls.append(recall_at_k(rel, total_pos, k))\n",
    "        cnt += 1\n",
    "    return float(np.mean(hits)), float(np.mean(ndcgs)), float(np.mean(recalls)), cnt\n"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "c6423e6b",
   "metadata": {},
   "source": [
    "## 5) Baseline (final) — Eval & Export"
   ]
  },
  {
   "cell_type": "code",
   "id": "c004d365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.617853Z",
     "start_time": "2025-09-14T11:16:51.542700Z"
    }
   },
   "source": [
    "if 'final' in df.columns:\n",
    "    df['final_norm'] = df.groupby('seed')['final'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "    h, n, r, cnt = eval_grouped_with_test(df, 'final_norm', k=TOPK, test_pos_set=test_pos_set)\n",
    "    print(f\"Baseline (`final_norm`) — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "    topk_baseline = df.sort_values(['seed','final_norm'], ascending=[True, False]).groupby('seed').head(TOPK)\n",
    "    topk_baseline['test_relevant'] = topk_baseline['candidate_title'].isin(test_pos_set)\n",
    "    out_path = f\"{OUTPUT_DIR}/top{TOPK}_baseline.csv\"\n",
    "    topk_baseline.to_csv(out_path, index=False)\n",
    "    print(\"Export:\", out_path)\n",
    "else:\n",
    "    print(\"Keine 'final'-Spalte gefunden — Baseline übersprungen.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (`final_norm`) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031 (Seeds: 130)\n",
      "Export: ../data/kg/outputs/top10_baseline.csv\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "a438ef88",
   "metadata": {},
   "source": [
    "## 6) GNN-Training (HeteroConv, echte Labels)"
   ]
  },
  {
   "cell_type": "code",
   "id": "78f43d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:51.654009Z",
     "start_time": "2025-09-14T11:16:51.650267Z"
    }
   },
   "source": [
    "import importlib, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "has_torch = importlib.util.find_spec('torch') is not None\n",
    "has_pyg   = importlib.util.find_spec('torch_geometric') is not None\n",
    "print(\"Torch installiert:\", has_torch, \"| PyG installiert:\", has_pyg)\n",
    "if not (has_torch and has_pyg):\n",
    "    print(\"GNN-Teil wird übersprungen (Pakete fehlen).\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch installiert: True | PyG installiert: True\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "722f967c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:55.693837Z",
     "start_time": "2025-09-14T11:16:51.678615Z"
    }
   },
   "source": [
    "if has_torch and has_pyg:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.data import HeteroData\n",
    "    from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    data = HeteroData()\n",
    "    num_movies = len(movie2id)\n",
    "    d = 64\n",
    "\n",
    "    for rc, lst in norm_edges.items():\n",
    "        if not lst: continue\n",
    "        src = torch.tensor([u for (u,_,_) in lst], dtype=torch.long)\n",
    "        dst = torch.tensor([v for (_,v,_) in lst], dtype=torch.long)\n",
    "        data['movie', rc, 'movie'].edge_index = torch.stack([src, dst], dim=0)\n",
    "\n",
    "    pos_ids = [movie2id[t] for t in train_pos if t in movie2id]\n",
    "    assert len(pos_ids) > 0, \"Keine Trainings-Positives im CSV-Kandidatenraum gefunden.\"\n",
    "    u_src = torch.zeros(len(pos_ids), dtype=torch.long)\n",
    "    m_dst = torch.tensor(pos_ids, dtype=torch.long)\n",
    "    data['user','likes','movie'].edge_index = torch.stack([u_src, m_dst], dim=0)\n",
    "\n",
    "    all_movie_ids = torch.arange(num_movies, dtype=torch.long)\n",
    "    pos_set = set(m_dst.tolist())\n",
    "    neg_pool = [int(i) for i in all_movie_ids.tolist() if i not in pos_set]\n",
    "    NEG_PER_POS = 3\n",
    "    neg_pairs = [(0, random.choice(neg_pool)) for _ in range(len(pos_ids) * NEG_PER_POS)]\n",
    "    un_src = torch.tensor([p[0] for p in neg_pairs], dtype=torch.long)\n",
    "    mn_dst = torch.tensor([p[1] for p in neg_pairs], dtype=torch.long)\n",
    "\n",
    "    conv_edge_types = [et for et in data.edge_types if et[2] == 'movie']\n",
    "    edge_index_dict = {et: data[et].edge_index for et in conv_edge_types}\n",
    "\n",
    "    class HeteroRecommender(nn.Module):\n",
    "        def __init__(self, num_movies, dim=64, layers=2):\n",
    "            super().__init__()\n",
    "            self.movie_emb = nn.Embedding(num_movies, dim)\n",
    "            self.user_emb  = nn.Embedding(1, dim)\n",
    "            self.layers = nn.ModuleList([\n",
    "                HeteroConv({ et: GATv2Conv((-1, -1), dim, add_self_loops=False) for et in conv_edge_types }, aggr='sum')\n",
    "                for _ in range(layers)\n",
    "            ])\n",
    "        def forward(self, edge_index_dict):\n",
    "            x = {'movie': self.movie_emb.weight, 'user': self.user_emb.weight}\n",
    "            for conv in self.layers:\n",
    "                out = conv(x, edge_index_dict)\n",
    "                out = {k: F.relu(v) for k, v in out.items()}\n",
    "                x.update(out)\n",
    "            return x\n",
    "        @staticmethod\n",
    "        def score(user_vec, item_vec):\n",
    "            return (user_vec * item_vec).sum(dim=-1)\n",
    "\n",
    "    model = HeteroRecommender(num_movies=num_movies, dim=d, layers=2)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        x_out = model(edge_index_dict)\n",
    "        user_pos = x_out['user'][u_src]\n",
    "        item_pos = x_out['movie'][m_dst]\n",
    "        user_neg = x_out['user'][un_src]\n",
    "        item_neg = x_out['movie'][mn_dst]\n",
    "        pos_logit = HeteroRecommender.score(user_pos, item_pos)\n",
    "        neg_logit = HeteroRecommender.score(user_neg, item_neg)\n",
    "        loss = bce(pos_logit, torch.ones_like(pos_logit)) + bce(neg_logit, torch.zeros_like(neg_logit))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_out = model(edge_index_dict)\n",
    "        user_vec = x_out['user'][0:1]\n",
    "\n",
    "    gnn_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        cand = row['candidate_title']\n",
    "        mid = movie2id.get(cand, None)\n",
    "        if mid is None:\n",
    "            gnn_scores.append(np.nan); continue\n",
    "        item_vec = x_out['movie'][mid:mid+1]\n",
    "        s = float((user_vec * item_vec).sum(dim=-1))\n",
    "        gnn_scores.append(s)\n",
    "\n",
    "    df['s_gnn'] = gnn_scores\n",
    "    df['s_gnn_norm'] = df.groupby('seed')['s_gnn'].transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9))\n",
    "    print(\"GNN-Scoring hinzugefügt: 's_gnn'/'s_gnn_norm'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 0.0047\n",
      "Epoch 100 | Loss 0.0023\n",
      "Epoch 150 | Loss 0.0014\n",
      "Epoch 200 | Loss 0.0010\n",
      "GNN-Scoring hinzugefügt: 's_gnn'/'s_gnn_norm'\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "4dbbd6d8",
   "metadata": {},
   "source": [
    "## 7) Ensemble & finale Evaluation + Top-K-Exporte"
   ]
  },
  {
   "cell_type": "code",
   "id": "86cd9792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:55.938612Z",
     "start_time": "2025-09-14T11:16:55.754243Z"
    }
   },
   "source": [
    "if 'final' in df.columns and 'final_norm' not in df.columns:\n",
    "    df['final_norm'] = df.groupby('seed')['final'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "\n",
    "if 's_gnn_norm' in df.columns:\n",
    "    if 'final_norm' in df.columns:\n",
    "        df['score_ensemble'] = LAMBDA * df['s_gnn_norm'] + (1.0 - LAMBDA) * df['final_norm']\n",
    "    else:\n",
    "        df['score_ensemble'] = df['s_gnn_norm']\n",
    "else:\n",
    "    print(\"Warnung: Kein GNN-Score vorhanden; Ensemble entspricht Baseline.\")\n",
    "    df['score_ensemble'] = df.get('final_norm', 0.0)\n",
    "\n",
    "def eval_grouped_with_test(df, score_col, k=10, test_pos_set=None):\n",
    "    hits, ndcgs, recalls, cnt = [], [], [], 0\n",
    "    for seed, g in df.groupby('seed', sort=False):\n",
    "        g = g.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
    "        rel = (g['candidate_title'].isin(test_pos_set)).astype(int).values\n",
    "        total_pos = int(rel.sum())\n",
    "        hits.append(float(np.any(rel[:k] > 0)))\n",
    "        # NDCG:\n",
    "        rel_k = rel[:k]\n",
    "        dcg = np.sum((2**rel_k - 1) / np.log2(np.arange(2, len(rel_k) + 2)))\n",
    "        ideal = np.sort(rel)[::-1][:k]\n",
    "        idcg = np.sum((2**ideal - 1) / np.log2(np.arange(2, len(ideal) + 2)))\n",
    "        ndcgs.append((dcg / idcg) if idcg > 0 else 0.0)\n",
    "        # Recall@K:\n",
    "        found = rel[:k].sum()\n",
    "        recalls.append(float(found / total_pos) if total_pos > 0 else 0.0)\n",
    "        cnt += 1\n",
    "    return float(np.mean(hits)), float(np.mean(ndcgs)), float(np.mean(recalls)), cnt\n",
    "\n",
    "for name, col in [('Baseline(final_norm)', 'final_norm'), ('GNN(s_gnn_norm)', 's_gnn_norm'), ('Ensemble', 'score_ensemble')]:\n",
    "    if col in df.columns:\n",
    "        h, n, r, cnt = eval_grouped_with_test(df, col, k=TOPK, test_pos_set=test_pos_set)\n",
    "        print(f\"{name:>20} — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "\n",
    "def export_topk(df, col, fname):\n",
    "    if col not in df.columns: \n",
    "        return None\n",
    "    out = df.sort_values(['seed', col], ascending=[True, False]).groupby('seed').head(TOPK).copy()\n",
    "    out['test_relevant'] = out['candidate_title'].isin(test_pos_set)\n",
    "    path = f\"{OUTPUT_DIR}/{fname}\"\n",
    "    out.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "p1 = export_topk(df, 'final_norm',     f\"top{TOPK}_baseline.csv\")\n",
    "p2 = export_topk(df, 's_gnn_norm',     f\"top{TOPK}_gnn.csv\")\n",
    "p3 = export_topk(df, 'score_ensemble', f\"top{TOPK}_ensemble.csv\")\n",
    "print(\"Exporte:\", p1, p2, p3)\n",
    "\n",
    "df.sort_values(['seed','final_norm'], ascending=[True, False]).to_csv(f\"{OUTPUT_DIR}/baseline_rerank.csv\", index=False)\n",
    "if 's_gnn_norm' in df.columns:\n",
    "    df.sort_values(['seed','s_gnn_norm'], ascending=[True, False]).to_csv(f\"{OUTPUT_DIR}/gnn_rerank.csv\", index=False)\n",
    "df.sort_values(['seed','score_ensemble'], ascending=[True, False]).to_csv(f\"{OUTPUT_DIR}/ensemble_rerank.csv\", index=False)\n",
    "print(\"Fertige Exporte im Ordner:\", OUTPUT_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline(final_norm) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031 (Seeds: 130)\n",
      "     GNN(s_gnn_norm) — Hit@10: 0.031 | NDCG@10: 0.031 | Recall@10: 0.031 (Seeds: 130)\n",
      "            Ensemble — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031 (Seeds: 130)\n",
      "Exporte: ../data/kg/outputs/top10_baseline.csv ../data/kg/outputs/top10_gnn.csv ../data/kg/outputs/top10_ensemble.csv\n",
      "Fertige Exporte im Ordner: ../data/kg/outputs\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "68595830",
   "metadata": {},
   "source": [
    "## 8) Qualitative Fallstudie (2 Seeds)"
   ]
  },
  {
   "cell_type": "code",
   "id": "736b5a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:55.966014Z",
     "start_time": "2025-09-14T11:16:55.945467Z"
    }
   },
   "source": [
    "seeds_with_pos = []\n",
    "for seed, g in df.groupby('seed'):\n",
    "    if (g['candidate_title'].isin(test_pos_set)).any():\n",
    "        seeds_with_pos.append(seed)\n",
    "sample_seeds = seeds_with_pos[:2] if len(seeds_with_pos)>=2 else df['seed'].unique()[:2]\n",
    "for s in sample_seeds:\n",
    "    print(\"\\n=== Seed:\", s, \"===\")\n",
    "    g = df[df['seed'] == s].copy()\n",
    "    for name, col in [('Baseline', 'final_norm'), ('GNN', 's_gnn_norm'), ('Ensemble','score_ensemble')]:\n",
    "        if col in g.columns:\n",
    "            top = g.sort_values(col, ascending=False).head(5)[['candidate_title', col]].copy()\n",
    "            top['is_test_rel'] = top['candidate_title'].isin(test_pos_set)\n",
    "            print(f\"{name} Top-5:\")\n",
    "            print(top.to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed: Godzilla ===\n",
      "Baseline Top-5:\n",
      "          candidate_title  final_norm  is_test_rel\n",
      "The War of the Gargantuas         0.0         True\n",
      "GNN Top-5:\n",
      "          candidate_title  s_gnn_norm  is_test_rel\n",
      "The War of the Gargantuas         0.0         True\n",
      "Ensemble Top-5:\n",
      "          candidate_title  score_ensemble  is_test_rel\n",
      "The War of the Gargantuas             0.0         True\n",
      "\n",
      "=== Seed: Good Time ===\n",
      "Baseline Top-5:\n",
      " candidate_title  final_norm  is_test_rel\n",
      "We Own the Night    1.000000        False\n",
      "         Chopper    0.133333        False\n",
      "           Bound    0.000000         True\n",
      "GNN Top-5:\n",
      " candidate_title  s_gnn_norm  is_test_rel\n",
      "           Bound         0.0         True\n",
      "We Own the Night         0.0        False\n",
      "         Chopper         0.0        False\n",
      "Ensemble Top-5:\n",
      " candidate_title  score_ensemble  is_test_rel\n",
      "We Own the Night        0.400000        False\n",
      "         Chopper        0.053333        False\n",
      "           Bound        0.000000         True\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
