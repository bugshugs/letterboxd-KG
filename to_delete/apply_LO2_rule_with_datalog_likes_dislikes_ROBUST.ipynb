{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c0a904",
   "metadata": {},
   "source": [
    "\n",
    "# LO2 with **Datalog** â€” Watchlist + Likes/Dislikes (Robust)\n",
    "\n",
    "This notebook is a **robust** version that:\n",
    "- recomputes `liked_genres/dirs` and `disliked_genres/dirs` **before** using them,\n",
    "- safely coerces `genre_list` / `director_list` to Python lists (no NaNs/floats),\n",
    "- avoids `film_genre/3` or `director_fact/3` predicates inside Datalog by asserting the\n",
    "  boost/penalty flags as facts.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d2694f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:22:09.843219Z",
     "start_time": "2025-09-06T20:22:09.526163Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detect project root\n",
    "here = Path.cwd()\n",
    "candidate = here\n",
    "while candidate != candidate.parent and not (candidate / \"data\").exists():\n",
    "    candidate = candidate.parent\n",
    "project_root = candidate if (candidate / \"data\").exists() else Path(\"../logical\")\n",
    "print(\"Detected project_root:\", project_root.resolve())\n",
    "\n",
    "# Paths\n",
    "watched_path   = project_root / \"data\" / \"letterboxd_export\" / \"watched.csv\"\n",
    "watchlist_path = project_root / \"data\" / \"letterboxd_export\" / \"watchlist.csv\"\n",
    "candidates_path= project_root / \"data\" / \"kg\" / \"tmdb_rerank_with_embedding_results_movies_only.csv\"\n",
    "\n",
    "# enriched-merged uploaded in this session or local fallback\n",
    "enriched_uploaded = Path(\"/mnt/data/0bf2f757-dc8c-43d8-9f82-d2705737b4fe.csv\")\n",
    "enriched_local = project_root / \"data\" / \"enriched_merged.csv\"\n",
    "enriched_path = enriched_uploaded if enriched_uploaded.exists() else enriched_local\n",
    "print(\"Using enriched file:\", enriched_path)\n",
    "\n",
    "out_csv = project_root / \"data\" / \"kg\" / \"rerank_LO2_watchlist_likes_dislikes_ROBUST.csv\"\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected project_root: /Users/tschaffel/PycharmProjects/letterboxd-KG\n",
      "Using enriched file: /Users/tschaffel/PycharmProjects/letterboxd-KG/data/enriched_merged.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "05e36b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:22:12.454825Z",
     "start_time": "2025-09-06T20:22:12.373081Z"
    }
   },
   "source": [
    "\n",
    "# Load data\n",
    "watched_df = pd.read_csv(watched_path)\n",
    "watchlist_df = pd.read_csv(watchlist_path)\n",
    "recs_df = pd.read_csv(candidates_path)\n",
    "enriched_df = pd.read_csv(enriched_path)\n",
    "\n",
    "# Normalize column names\n",
    "for df in (watched_df, watchlist_df, recs_df, enriched_df):\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "def pick(colnames, options):\n",
    "    for o in options:\n",
    "        if o in colnames:\n",
    "            return o\n",
    "    return None\n",
    "\n",
    "# Columns\n",
    "watched_name_col = pick(watched_df.columns, [\"name\",\"film name\",\"title\"])\n",
    "watched_year_col = pick(watched_df.columns, [\"year\",\"release year\",\"release_year\"])\n",
    "watch_name_col   = pick(watchlist_df.columns, [\"name\",\"film name\",\"title\",\"candidate_title\",\"movie\",\"movie_title\",\"original_title\"])\n",
    "watch_year_col   = pick(watchlist_df.columns, [\"year\",\"release year\",\"release_year\",\"releaseyear\"])\n",
    "recs_name_col    = pick(recs_df.columns, [\"candidate_title\",\"name\",\"title\",\"movie_title\",\"original_title\"])\n",
    "recs_year_col    = pick(recs_df.columns, [\"year\",\"release_year\",\"candidate_year\",\"releaseyear\",\"year_x\",\"year_y\"])\n",
    "\n",
    "en_title_col = pick(enriched_df.columns, [\"title\",\"name\"])\n",
    "en_year_col  = pick(enriched_df.columns, [\"year\",\"release_year\"])\n",
    "en_rating_col= pick(enriched_df.columns, [\"rating\",\"myrating\",\"rating10\",\"rating_10\"])\n",
    "en_genres_col= pick(enriched_df.columns, [\"genres\",\"genre\",\"tmdb_genres\"])\n",
    "en_dir_col   = pick(enriched_df.columns, [\"director\",\"directors\",\"tmdb_directors\"])\n",
    "\n",
    "assert all([watched_name_col, watched_year_col, watch_name_col, watch_year_col, recs_name_col, recs_year_col, en_title_col, en_year_col, en_rating_col, en_genres_col, en_dir_col]), \"Missing required columns\"\n",
    "\n",
    "def norm_name(s): return s.astype(str).str.strip().str.lower()\n",
    "def norm_year(s): \n",
    "    return s.astype(str).str.extract(r\"(\\d{4})\", expand=False).fillna(s.astype(str).str.strip())\n",
    "\n",
    "for df_, ncol, ycol in [(watched_df, watched_name_col, watched_year_col),\n",
    "                        (watchlist_df, watch_name_col, watch_year_col),\n",
    "                        (recs_df, recs_name_col, recs_year_col),\n",
    "                        (enriched_df, en_title_col, en_year_col)]:\n",
    "    df_[\"name_norm\"] = norm_name(df_[ncol])\n",
    "    df_[\"year_str\"]  = norm_year(df_[ycol])\n",
    "\n",
    "# Parse list-like columns robustly\n",
    "def parse_list_of_colon_pairs(cell):\n",
    "    if pd.isna(cell): return []\n",
    "    txt = str(cell)\n",
    "    # 1) try literal_eval\n",
    "    try:\n",
    "        lst = ast.literal_eval(txt)\n",
    "        out = []\n",
    "        if isinstance(lst, list):\n",
    "            for it in lst:\n",
    "                if isinstance(it, str):\n",
    "                    out.append(it.split(':',1)[0].strip())\n",
    "                else:\n",
    "                    out.append(str(it))\n",
    "        return out\n",
    "    except Exception:\n",
    "        # 2) regex fallback\n",
    "        hits = re.findall(r\"'([^':]+):\", txt)\n",
    "        return [h.strip() for h in hits]\n",
    "\n",
    "enriched_df[\"genre_list\"] = enriched_df[en_genres_col].apply(parse_list_of_colon_pairs)\n",
    "enriched_df[\"director_list\"] = enriched_df[en_dir_col].apply(parse_list_of_colon_pairs)\n",
    "\n",
    "# Aggregate per (name_norm, year_str) with set-union\n",
    "def set_union(series_of_lists):\n",
    "    s = set()\n",
    "    for lst in series_of_lists:\n",
    "        if isinstance(lst, list):\n",
    "            s.update(lst)\n",
    "        elif pd.isna(lst):\n",
    "            continue\n",
    "        else:\n",
    "            # single string -> treat as one item\n",
    "            s.add(str(lst))\n",
    "    return sorted(s)\n",
    "\n",
    "agg_meta = (enriched_df\n",
    "            .groupby([\"name_norm\",\"year_str\"], as_index=False)\n",
    "            .agg(genre_list=(\"genre_list\", set_union),\n",
    "                 director_list=(\"director_list\", set_union)))\n",
    "\n",
    "recs_df = recs_df.merge(agg_meta, on=[\"name_norm\",\"year_str\"], how=\"left\")\n",
    "\n",
    "# Force to list (no floats/NaNs)\n",
    "def ensure_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if pd.isna(x): return []\n",
    "    return [str(x)]\n",
    "\n",
    "recs_df[\"genre_list\"] = recs_df[\"genre_list\"].apply(ensure_list)\n",
    "recs_df[\"director_list\"] = recs_df[\"director_list\"].apply(ensure_list)\n",
    "\n",
    "watched_pairs   = set(zip(watched_df[\"name_norm\"], watched_df[\"year_str\"]))\n",
    "watchlist_pairs = set(zip(watchlist_df[\"name_norm\"], watchlist_df[\"year_str\"]))\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2be72f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:22:16.481328Z",
     "start_time": "2025-09-06T20:22:16.465017Z"
    }
   },
   "source": [
    "\n",
    "# Compute likes & dislikes from ratings (recomputed here to avoid NameError)\n",
    "rated = enriched_df.dropna(subset=[en_rating_col]).copy()\n",
    "rmax = rated[en_rating_col].max()\n",
    "scale = 10.0 if rmax > 5 else 5.0\n",
    "like_threshold = 7.0 if scale == 10.0 else 3.5\n",
    "dislike_threshold = 3.0 if scale == 10.0 else 1.5\n",
    "min_count = 2\n",
    "\n",
    "genres_long = rated.explode(\"genre_list\").dropna(subset=[\"genre_list\"])\n",
    "dirs_long   = rated.explode(\"director_list\").dropna(subset=[\"director_list\"])\n",
    "\n",
    "g_stats = (genres_long.groupby(\"genre_list\")[en_rating_col]\n",
    "           .agg(['mean','count']).reset_index().rename(columns={'genre_list':'genre'}))\n",
    "d_stats = (dirs_long.groupby(\"director_list\")[en_rating_col]\n",
    "           .agg(['mean','count']).reset_index().rename(columns={'director_list':'director'}))\n",
    "\n",
    "liked_genres    = set(g_stats[(g_stats['mean']>=like_threshold)    & (g_stats['count']>=min_count)]['genre'])\n",
    "disliked_genres = set(g_stats[(g_stats['mean']<=dislike_threshold) & (g_stats['count']>=min_count)]['genre'])\n",
    "\n",
    "liked_dirs      = set(d_stats[(d_stats['mean']>=like_threshold)    & (d_stats['count']>=min_count)]['director'])\n",
    "disliked_dirs   = set(d_stats[(d_stats['mean']<=dislike_threshold) & (d_stats['count']>=min_count)]['director'])\n",
    "\n",
    "print(\"Scale:\", scale, \"| like_threshold:\", like_threshold, \"| dislike_threshold:\", dislike_threshold)\n",
    "print(\"liked genres (n):\", len(liked_genres), \"| disliked genres (n):\", len(disliked_genres))\n",
    "print(\"liked directors (n):\", len(liked_dirs), \"| disliked directors (n):\", len(disliked_dirs))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale: 5.0 | like_threshold: 3.5 | dislike_threshold: 1.5\n",
      "liked genres (n): 17 | disliked genres (n): 0\n",
      "liked directors (n): 40 | disliked directors (n): 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c32c4d1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:22:19.519501Z",
     "start_time": "2025-09-06T20:22:19.513578Z"
    }
   },
   "source": [
    "\n",
    "# Precompute boosts/penalties per candidate (safe iteration)\n",
    "def has_any(lst, S):\n",
    "    try:\n",
    "        return any(x in S for x in (lst or []))\n",
    "    except TypeError:\n",
    "        # lst not iterable -> treat as no features\n",
    "        return False\n",
    "\n",
    "recs_df[\"genre_boost\"]      = recs_df[\"genre_list\"].apply(lambda lst: has_any(lst, liked_genres))\n",
    "recs_df[\"director_boost\"]   = recs_df[\"director_list\"].apply(lambda lst: has_any(lst, liked_dirs))\n",
    "recs_df[\"genre_penalty\"]    = recs_df[\"genre_list\"].apply(lambda lst: has_any(lst, disliked_genres))\n",
    "recs_df[\"director_penalty\"] = recs_df[\"director_list\"].apply(lambda lst: has_any(lst, disliked_dirs))\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "a31a0534",
   "metadata": {},
   "source": [
    "## Datalog core: assert facts and query sets"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b668027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:22:22.624695Z",
     "start_time": "2025-09-06T20:22:22.048700Z"
    }
   },
   "source": [
    "\n",
    "use_pyDatalog = False\n",
    "try:\n",
    "    from pyDatalog import pyDatalog\n",
    "    use_pyDatalog = True\n",
    "    print(\"pyDatalog is available â€” using it.\")\n",
    "except Exception as e:\n",
    "    print(\"pyDatalog not available, fallback will be used:\", e)\n",
    "\n",
    "USER = \"tobias\"\n",
    "\n",
    "if use_pyDatalog:\n",
    "    pyDatalog.clear()\n",
    "    pyDatalog.create_terms('watched_fact, candidateFor, onWatchlist, '\n",
    "                           'genreBoost, dirBoost, genrePenalty, dirPenalty, '\n",
    "                           'recommendedBase, recommended, watchBoost, U,N,Y')\n",
    "\n",
    "    for n,y in watched_pairs:   +watched_fact(USER,n,y)\n",
    "    for n,y in watchlist_pairs: +onWatchlist(USER,n,y)\n",
    "    for _,row in recs_df.iterrows():\n",
    "        +candidateFor(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "\n",
    "    for _,row in recs_df[recs_df[\"genre_boost\"]==True].iterrows():\n",
    "        +genreBoost(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "    for _,row in recs_df[recs_df[\"director_boost\"]==True].iterrows():\n",
    "        +dirBoost(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "    for _,row in recs_df[recs_df[\"genre_penalty\"]==True].iterrows():\n",
    "        +genrePenalty(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "    for _,row in recs_df[recs_df[\"director_penalty\"]==True].iterrows():\n",
    "        +dirPenalty(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "\n",
    "    recommendedBase(U,N,Y) <= candidateFor(U,N,Y) & ~watched_fact(U,N,Y)\n",
    "    recommended(U,N,Y)     <= recommendedBase(U,N,Y)\n",
    "    watchBoost(U,N,Y)      <= recommendedBase(U,N,Y) & onWatchlist(U,N,Y)\n",
    "\n",
    "    def qset(s):\n",
    "        ans = pyDatalog.ask(s)\n",
    "        return set(tuple(x) for x in (ans.answers if ans else []))\n",
    "\n",
    "    all_pairs   = qset(f'recommended(\"{USER}\", N, Y)')\n",
    "    watch_pairs = qset(f'watchBoost(\"{USER}\", N, Y)')\n",
    "    g_like      = qset(f'genreBoost(\"{USER}\", N, Y)')\n",
    "    d_like      = qset(f'dirBoost(\"{USER}\", N, Y)')\n",
    "    g_bad       = qset(f'genrePenalty(\"{USER}\", N, Y)')\n",
    "    d_bad       = qset(f'dirPenalty(\"{USER}\", N, Y)')\n",
    "\n",
    "    if all_pairs:\n",
    "        rec_df = pd.DataFrame(list(all_pairs), columns=[\"name_norm\",\"year_str\"])\n",
    "        out = recs_df.merge(rec_df, on=[\"name_norm\",\"year_str\"], how=\"inner\")\n",
    "    else:\n",
    "        out = recs_df.iloc[0:0].copy()\n",
    "\n",
    "    def flag(df, S, col):\n",
    "        df[col] = list(map(lambda p: p in S, zip(df[\"name_norm\"], df[\"year_str\"])))\n",
    "    flag(out, watch_pairs, \"watchlist_priority\")\n",
    "    flag(out, g_like, \"genre_boost\")\n",
    "    flag(out, d_like, \"director_boost\")\n",
    "    flag(out, g_bad,  \"genre_penalty\")\n",
    "    flag(out, d_bad,  \"director_penalty\")\n",
    "\n",
    "else:\n",
    "    # Pure pandas fallback\n",
    "    cand_pairs = list(zip(recs_df[\"name_norm\"], recs_df[\"year_str\"]))\n",
    "    keep_mask = [pair not in watched_pairs for pair in cand_pairs]\n",
    "    out = recs_df.loc[keep_mask].copy()\n",
    "    out[\"watchlist_priority\"] = list(map(lambda p: p in watchlist_pairs, zip(out[\"name_norm\"], out[\"year_str\"])))\n",
    "    out[\"genre_boost\"]      = recs_df[\"genre_boost\"]\n",
    "    out[\"director_boost\"]   = recs_df[\"director_boost\"]\n",
    "    out[\"genre_penalty\"]    = recs_df[\"genre_penalty\"]\n",
    "    out[\"director_penalty\"] = recs_df[\"director_penalty\"]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyDatalog is available â€” using it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyParser.py:771: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  r = re.compile('^\\s*')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Predicate without definition (or error in resolver): genreBoost/3",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 41\u001B[39m\n\u001B[32m     39\u001B[39m all_pairs   = qset(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mrecommended(\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mUSER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m, N, Y)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     40\u001B[39m watch_pairs = qset(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mwatchBoost(\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mUSER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m, N, Y)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m g_like      = qset(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mgenreBoost(\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mUSER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m, N, Y)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     42\u001B[39m d_like      = qset(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mdirBoost(\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mUSER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m, N, Y)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     43\u001B[39m g_bad       = qset(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mgenrePenalty(\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mUSER\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m, N, Y)\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 36\u001B[39m, in \u001B[36mqset\u001B[39m\u001B[34m(s)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mqset\u001B[39m(s):\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m     ans = pyDatalog.ask(s)\n\u001B[32m     37\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mtuple\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m (ans.answers \u001B[38;5;28;01mif\u001B[39;00m ans \u001B[38;5;28;01melse\u001B[39;00m []))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyDatalog.py:111\u001B[39m, in \u001B[36mask\u001B[39m\u001B[34m(code)\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mask\u001B[39m(code):\n\u001B[32m    110\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"returns the result of the query contained in the code string\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m111\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pyParser.ask(code)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyParser.py:841\u001B[39m, in \u001B[36mask\u001B[39m\u001B[34m(code)\u001B[39m\n\u001B[32m    839\u001B[39m add_symbols(code.co_names, newglobals)\n\u001B[32m    840\u001B[39m parsed_code = \u001B[38;5;28meval\u001B[39m(code, newglobals)\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m a = parsed_code.ask()\n\u001B[32m    842\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Answer.make(a)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyParser.py:577\u001B[39m, in \u001B[36mQuery.ask\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    576\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mask\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m577\u001B[39m     \u001B[38;5;28mself\u001B[39m._data = Body(\u001B[38;5;28mself\u001B[39m.pre_calculations, \u001B[38;5;28mself\u001B[39m).ask()\n\u001B[32m    578\u001B[39m     \u001B[38;5;28mself\u001B[39m.todo = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    579\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyParser.py:693\u001B[39m, in \u001B[36mBody.ask\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    691\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\" resolve the query and determine the values of its variables\"\"\"\u001B[39;00m\n\u001B[32m    692\u001B[39m literal = \u001B[38;5;28mself\u001B[39m.literal()\n\u001B[32m--> \u001B[39m\u001B[32m693\u001B[39m \u001B[38;5;28mself\u001B[39m._data = literal.lua.ask()\n\u001B[32m    694\u001B[39m literal.todo, \u001B[38;5;28mself\u001B[39m.todo = \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    695\u001B[39m - (literal <= \u001B[38;5;28mself\u001B[39m) \u001B[38;5;66;03m# delete the temporary clause\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyEngine.py:513\u001B[39m, in \u001B[36mLiteral.ask\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    511\u001B[39m todo, arg = (SEARCH, (Ts.Goal, ))\n\u001B[32m    512\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m todo:\n\u001B[32m--> \u001B[39m\u001B[32m513\u001B[39m     todo, arg = todo(*arg)\n\u001B[32m    515\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m Ts.Goal.facts \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    516\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pyDatalog/pyEngine.py:812\u001B[39m, in \u001B[36mSubgoal.search\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    808\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m util.DatalogError(\u001B[33m\"\u001B[39m\u001B[33mError: right hand side of comparison must be bound: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m \n\u001B[32m    809\u001B[39m                             % literal.pred.id, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    810\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.next_step()\n\u001B[32m--> \u001B[39m\u001B[32m812\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mPredicate without definition (or error in resolver): \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m % literal.pred.id)\n",
      "\u001B[31mAttributeError\u001B[39m: Predicate without definition (or error in resolver): genreBoost/3"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7f7d7fdc",
   "metadata": {},
   "source": [
    "## Score, Sort, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83503c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weights\n",
    "w_watch, w_glike, w_dlike, w_gbad, w_dbad = 2, 1, 2, 1, 2\n",
    "\n",
    "out[\"score\"] = (out[\"watchlist_priority\"].astype(int)*w_watch +\n",
    "                out[\"genre_boost\"].astype(int)*w_glike +\n",
    "                out[\"director_boost\"].astype(int)*w_dlike -\n",
    "                out[\"genre_penalty\"].astype(int)*w_gbad -\n",
    "                out[\"director_penalty\"].astype(int)*w_dbad)\n",
    "\n",
    "sort_cols, ascending = [\"score\"], [False]\n",
    "if \"rank\" in out.columns: sort_cols.append(\"rank\"); ascending.append(True)\n",
    "\n",
    "out_sorted = out.sort_values(by=sort_cols, ascending=ascending).reset_index(drop=True)\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_sorted.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv.resolve())\n",
    "\n",
    "summary = {\n",
    "    \"candidates_total\": int(len(recs_df)),\n",
    "    \"recommended_total\": int(len(out_sorted)),\n",
    "    \"watchlist_priority_true\": int(out_sorted[\"watchlist_priority\"].sum()),\n",
    "    \"genre_boost_true\": int(out_sorted[\"genre_boost\"].sum()),\n",
    "    \"director_boost_true\": int(out_sorted[\"director_boost\"].sum()),\n",
    "    \"genre_penalty_true\": int(out_sorted[\"genre_penalty\"].sum()),\n",
    "    \"director_penalty_true\": int(out_sorted[\"director_penalty\"].sum()),\n",
    "}\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
