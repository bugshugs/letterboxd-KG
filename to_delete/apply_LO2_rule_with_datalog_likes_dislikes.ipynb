{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608c135e",
   "metadata": {},
   "source": [
    "\n",
    "# LO2 with **Datalog**: Watchlist + Genre/Director Likes & **Dislikes**\n",
    "\n",
    "We extend the rules to include **negative preferences**:\n",
    "- `dislikesGenreFact(U,G)` from low-rated genres\n",
    "- `dislikesDirectorFact(U,D)` from low-rated directors\n",
    "\n",
    "**Core rules**\n",
    "```\n",
    "recommendedBase(U,N,Y) :- candidateFor(U,N,Y) ∧ not watched_fact(U,N,Y).\n",
    "watchBoost(U,N,Y)      :- recommendedBase(U,N,Y) ∧ onWatchlist(U,N,Y).\n",
    "\n",
    "genreBoost(U,N,Y)      :- recommendedBase(U,N,Y) ∧ film_genre(N,Y,G) ∧ likesGenreFact(U,G).\n",
    "dirBoost(U,N,Y)        :- recommendedBase(U,N,Y) ∧ director_fact(N,Y,D) ∧ likesDirectorFact(U,D).\n",
    "\n",
    "genrePenalty(U,N,Y)    :- recommendedBase(U,N,Y) ∧ film_genre(N,Y,G) ∧ dislikesGenreFact(U,G).\n",
    "dirPenalty(U,N,Y)      :- recommendedBase(U,N,Y) ∧ director_fact(N,Y,D) ∧ dislikesDirectorFact(U,D).\n",
    "\n",
    "recommended(U,N,Y)     :- recommendedBase(U,N,Y).\n",
    "```\n",
    "**Scoring (default weights, adjust to taste):**\n",
    "```\n",
    "score = 2*watchBoost + 1*genreBoost + 2*dirBoost - 1*genrePenalty - 2*dirPenalty\n",
    "```\n",
    "\n",
    "**Inputs**\n",
    "- `data/letterboxd_export/watched.csv`\n",
    "- `data/letterboxd_export/watchlist.csv`\n",
    "- `data/kg/tmdb_rerank_with_embedding_results_movies_only.csv`\n",
    "- `enriched-merged.csv` (uploaded in this session)\n",
    "\n",
    "**Outputs**\n",
    "- `data/kg/rerank_LO2_watchlist_genre_dir_likes_dislikes.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a898c522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:09:57.505106Z",
     "start_time": "2025-09-06T20:09:57.169906Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast, re\n",
    "from pathlib import Path\n",
    "\n",
    "# Auto-detect project root\n",
    "here = Path.cwd()\n",
    "candidate = here\n",
    "while candidate != candidate.parent and not (candidate / \"data\").exists():\n",
    "    candidate = candidate.parent\n",
    "project_root = candidate if (candidate / \"data\").exists() else Path(\"../logical\")\n",
    "print(\"Detected project_root:\", project_root.resolve())\n",
    "\n",
    "# Paths\n",
    "watched_path   = project_root / \"data\" / \"letterboxd_export\" / \"watched.csv\"\n",
    "watchlist_path = project_root / \"data\" / \"letterboxd_export\" / \"watchlist.csv\"\n",
    "candidates_path= project_root / \"data\" / \"kg\" / \"tmdb_rerank_with_embedding_results_movies_only.csv\"\n",
    "\n",
    "# Uploaded enriched-merged.csv from chat session:\n",
    "enriched_uploaded = Path(\"/mnt/data/0bf2f757-dc8c-43d8-9f82-d2705737b4fe.csv\")\n",
    "enriched_local = project_root / \"data\" / \"enriched_merged.csv\"\n",
    "enriched_path = enriched_uploaded if enriched_uploaded.exists() else enriched_local\n",
    "print(\"Using enriched file:\", enriched_path)\n",
    "\n",
    "out_csv = project_root / \"data\" / \"kg\" / \"rerank_LO2_watchlist_genre_dir_likes_dislikes.csv\"\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected project_root: /Users/tschaffel/PycharmProjects/letterboxd-KG\n",
      "Using enriched file: /Users/tschaffel/PycharmProjects/letterboxd-KG/data/enriched_merged.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "75ca6467",
   "metadata": {},
   "source": [
    "## Load & Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "3add00ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T20:10:22.831029Z",
     "start_time": "2025-09-06T20:10:22.132921Z"
    }
   },
   "source": [
    "\n",
    "watched_df = pd.read_csv(watched_path)\n",
    "watchlist_df = pd.read_csv(watchlist_path)\n",
    "recs_df = pd.read_csv(candidates_path)\n",
    "enriched_df = pd.read_csv(enriched_path)\n",
    "\n",
    "# Normalize column names\n",
    "for df in (watched_df, watchlist_df, recs_df, enriched_df):\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "def pick(colnames, options):\n",
    "    for o in options:\n",
    "        if o in colnames:\n",
    "            return o\n",
    "    return None\n",
    "\n",
    "# Columns\n",
    "watched_name_col = pick(watched_df.columns, [\"name\",\"film name\",\"title\"])\n",
    "watched_year_col = pick(watched_df.columns, [\"year\",\"release year\",\"release_year\"])\n",
    "\n",
    "watch_name_col = pick(watchlist_df.columns, [\"name\",\"film name\",\"title\",\"candidate_title\",\"movie\",\"movie_title\",\"original_title\"])\n",
    "watch_year_col = pick(watchlist_df.columns, [\"year\",\"release year\",\"release_year\",\"releaseyear\"])\n",
    "\n",
    "recs_name_col = pick(recs_df.columns, [\"candidate_title\",\"name\",\"title\",\"movie_title\",\"original_title\"])\n",
    "recs_year_col = pick(recs_df.columns, [\"year\",\"release_year\",\"candidate_year\",\"releaseyear\",\"year_x\",\"year_y\"])\n",
    "\n",
    "en_title_col = pick(enriched_df.columns, [\"title\",\"name\"])\n",
    "en_year_col  = pick(enriched_df.columns, [\"year\",\"release_year\"])\n",
    "en_rating_col= pick(enriched_df.columns, [\"rating\",\"myrating\",\"rating10\",\"rating_10\"])\n",
    "en_genres_col= pick(enriched_df.columns, [\"genres\",\"genre\",\"tmdb_genres\"])\n",
    "en_dir_col   = pick(enriched_df.columns, [\"director\",\"directors\",\"tmdb_directors\"])\n",
    "\n",
    "assert watched_name_col and watched_year_col\n",
    "assert watch_name_col  and watch_year_col\n",
    "assert recs_name_col   and recs_year_col\n",
    "assert en_title_col and en_year_col and en_rating_col and en_genres_col and en_dir_col\n",
    "\n",
    "# Normalizers\n",
    "def norm_name(s): return s.astype(str).str.strip().str.lower()\n",
    "def norm_year(s): \n",
    "    return s.astype(str).str.extract(r\"(\\d{4})\", expand=False).fillna(s.astype(str).str.strip())\n",
    "\n",
    "for df, ncol, ycol in [(watched_df, watched_name_col, watched_year_col),\n",
    "                       (watchlist_df, watch_name_col, watch_year_col),\n",
    "                       (recs_df, recs_name_col, recs_year_col),\n",
    "                       (enriched_df, en_title_col, en_year_col)]:\n",
    "    df[\"name_norm\"] = norm_name(df[ncol])\n",
    "    df[\"year_str\"]  = norm_year(df[ycol])\n",
    "\n",
    "# Parse list-like columns (e.g., \"['Action:url','Sci-Fi:url']\" -> [\"Action\",\"Sci-Fi\"] ; \"['Ridley Scott:url']\" -> [\"Ridley Scott\"])\n",
    "def parse_list_of_colon_pairs(cell):\n",
    "    if pd.isna(cell):\n",
    "        return []\n",
    "    txt = str(cell)\n",
    "    try:\n",
    "        lst = ast.literal_eval(txt)\n",
    "        out = []\n",
    "        for it in lst if isinstance(lst, list) else []:\n",
    "            if isinstance(it, str):\n",
    "                out.append(it.split(':',1)[0].strip())\n",
    "        return out\n",
    "    except Exception:\n",
    "        return re.findall(r\"'([^':]+):\", txt)\n",
    "\n",
    "enriched_df[\"genre_list\"] = enriched_df[en_genres_col].apply(parse_list_of_colon_pairs)\n",
    "enriched_df[\"director_list\"] = enriched_df[en_dir_col].apply(parse_list_of_colon_pairs)\n",
    "\n",
    "# Map candidates to metadata\n",
    "cand_meta = enriched_df[[\"name_norm\",\"year_str\",\"genre_list\",\"director_list\"]].drop_duplicates()\n",
    "recs_df = recs_df.merge(cand_meta, on=[\"name_norm\",\"year_str\"], how=\"left\")\n",
    "\n",
    "watched_pairs   = set(zip(watched_df[\"name_norm\"], watched_df[\"year_str\"]))\n",
    "watchlist_pairs = set(zip(watchlist_df[\"name_norm\"], watchlist_df[\"year_str\"]))\n",
    "\n",
    "print(\"Watched pairs:\", len(watched_pairs))\n",
    "print(\"Watchlist pairs:\", len(watchlist_pairs))\n",
    "print(\"Candidates:\", len(recs_df))\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 68\u001B[39m\n\u001B[32m     65\u001B[39m enriched_df[\u001B[33m\"\u001B[39m\u001B[33mdirector_list\u001B[39m\u001B[33m\"\u001B[39m] = enriched_df[en_dir_col].apply(parse_list_of_colon_pairs)\n\u001B[32m     67\u001B[39m \u001B[38;5;66;03m# Map candidates to metadata\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m cand_meta = enriched_df[[\u001B[33m\"\u001B[39m\u001B[33mname_norm\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33myear_str\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mgenre_list\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33mdirector_list\u001B[39m\u001B[33m\"\u001B[39m]].drop_duplicates()\n\u001B[32m     69\u001B[39m recs_df = recs_df.merge(cand_meta, on=[\u001B[33m\"\u001B[39m\u001B[33mname_norm\u001B[39m\u001B[33m\"\u001B[39m,\u001B[33m\"\u001B[39m\u001B[33myear_str\u001B[39m\u001B[33m\"\u001B[39m], how=\u001B[33m\"\u001B[39m\u001B[33mleft\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     71\u001B[39m watched_pairs   = \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mzip\u001B[39m(watched_df[\u001B[33m\"\u001B[39m\u001B[33mname_norm\u001B[39m\u001B[33m\"\u001B[39m], watched_df[\u001B[33m\"\u001B[39m\u001B[33myear_str\u001B[39m\u001B[33m\"\u001B[39m]))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pandas/core/frame.py:6818\u001B[39m, in \u001B[36mDataFrame.drop_duplicates\u001B[39m\u001B[34m(self, subset, keep, inplace, ignore_index)\u001B[39m\n\u001B[32m   6815\u001B[39m inplace = validate_bool_kwarg(inplace, \u001B[33m\"\u001B[39m\u001B[33minplace\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   6816\u001B[39m ignore_index = validate_bool_kwarg(ignore_index, \u001B[33m\"\u001B[39m\u001B[33mignore_index\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m6818\u001B[39m result = \u001B[38;5;28mself\u001B[39m[-\u001B[38;5;28mself\u001B[39m.duplicated(subset, keep=keep)]\n\u001B[32m   6819\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ignore_index:\n\u001B[32m   6820\u001B[39m     result.index = default_index(\u001B[38;5;28mlen\u001B[39m(result))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pandas/core/frame.py:6958\u001B[39m, in \u001B[36mDataFrame.duplicated\u001B[39m\u001B[34m(self, subset, keep)\u001B[39m\n\u001B[32m   6956\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   6957\u001B[39m     vals = (col.values \u001B[38;5;28;01mfor\u001B[39;00m name, col \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.items() \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m subset)\n\u001B[32m-> \u001B[39m\u001B[32m6958\u001B[39m     labels, shape = \u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mzip\u001B[39m(*\u001B[38;5;28mmap\u001B[39m(f, vals)))\n\u001B[32m   6960\u001B[39m     ids = get_group_index(labels, \u001B[38;5;28mtuple\u001B[39m(shape), sort=\u001B[38;5;28;01mFalse\u001B[39;00m, xnull=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   6961\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._constructor_sliced(duplicated(ids, keep), index=\u001B[38;5;28mself\u001B[39m.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pandas/core/frame.py:6926\u001B[39m, in \u001B[36mDataFrame.duplicated.<locals>.f\u001B[39m\u001B[34m(vals)\u001B[39m\n\u001B[32m   6925\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mf\u001B[39m(vals) -> \u001B[38;5;28mtuple\u001B[39m[np.ndarray, \u001B[38;5;28mint\u001B[39m]:\n\u001B[32m-> \u001B[39m\u001B[32m6926\u001B[39m     labels, shape = algorithms.factorize(vals, size_hint=\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n\u001B[32m   6927\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m labels.astype(\u001B[33m\"\u001B[39m\u001B[33mi8\u001B[39m\u001B[33m\"\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m), \u001B[38;5;28mlen\u001B[39m(shape)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pandas/core/algorithms.py:795\u001B[39m, in \u001B[36mfactorize\u001B[39m\u001B[34m(values, sort, use_na_sentinel, size_hint)\u001B[39m\n\u001B[32m    792\u001B[39m             \u001B[38;5;66;03m# Don't modify (potentially user-provided) array\u001B[39;00m\n\u001B[32m    793\u001B[39m             values = np.where(null_mask, na_value, values)\n\u001B[32m--> \u001B[39m\u001B[32m795\u001B[39m     codes, uniques = factorize_array(\n\u001B[32m    796\u001B[39m         values,\n\u001B[32m    797\u001B[39m         use_na_sentinel=use_na_sentinel,\n\u001B[32m    798\u001B[39m         size_hint=size_hint,\n\u001B[32m    799\u001B[39m     )\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) > \u001B[32m0\u001B[39m:\n\u001B[32m    802\u001B[39m     uniques, codes = safe_sort(\n\u001B[32m    803\u001B[39m         uniques,\n\u001B[32m    804\u001B[39m         codes,\n\u001B[32m   (...)\u001B[39m\u001B[32m    807\u001B[39m         verify=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    808\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/pandas/core/algorithms.py:595\u001B[39m, in \u001B[36mfactorize_array\u001B[39m\u001B[34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[39m\n\u001B[32m    592\u001B[39m hash_klass, values = _get_hashtable_algo(values)\n\u001B[32m    594\u001B[39m table = hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[32m--> \u001B[39m\u001B[32m595\u001B[39m uniques, codes = table.factorize(\n\u001B[32m    596\u001B[39m     values,\n\u001B[32m    597\u001B[39m     na_sentinel=-\u001B[32m1\u001B[39m,\n\u001B[32m    598\u001B[39m     na_value=na_value,\n\u001B[32m    599\u001B[39m     mask=mask,\n\u001B[32m    600\u001B[39m     ignore_na=use_na_sentinel,\n\u001B[32m    601\u001B[39m )\n\u001B[32m    603\u001B[39m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n\u001B[32m    604\u001B[39m uniques = _reconstruct_data(uniques, original.dtype, original)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7281\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas/_libs/hashtable_class_helper.pxi:7195\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable._unique\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mTypeError\u001B[39m: unhashable type: 'list'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "cc6ac203",
   "metadata": {},
   "source": [
    "## Compute Likes & Dislikes (Genres/Directors) from Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bf0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rated = enriched_df.dropna(subset=[en_rating_col]).copy()\n",
    "rmax = rated[en_rating_col].max()\n",
    "scale = 10.0 if rmax > 5 else 5.0\n",
    "like_threshold = 7.0 if scale == 10.0 else 3.5   # >=70% of scale\n",
    "dislike_threshold = 3.0 if scale == 10.0 else 1.5 # <=30% of scale\n",
    "min_count = 2  # require at least 2 rated films per category\n",
    "\n",
    "genres_long = rated.explode(\"genre_list\").dropna(subset=[\"genre_list\"])\n",
    "dirs_long   = rated.explode(\"director_list\").dropna(subset=[\"director_list\"])\n",
    "\n",
    "g_stats = (genres_long.groupby(\"genre_list\")[en_rating_col]\n",
    "           .agg(['mean','count']).reset_index().rename(columns={'genre_list':'genre'}))\n",
    "d_stats = (dirs_long.groupby(\"director_list\")[en_rating_col]\n",
    "           .agg(['mean','count']).reset_index().rename(columns={'director_list':'director'}))\n",
    "\n",
    "liked_genres    = set(g_stats[(g_stats['mean']>=like_threshold)    & (g_stats['count']>=min_count)]['genre'])\n",
    "disliked_genres = set(g_stats[(g_stats['mean']<=dislike_threshold) & (g_stats['count']>=min_count)]['genre'])\n",
    "\n",
    "liked_dirs      = set(d_stats[(d_stats['mean']>=like_threshold)    & (d_stats['count']>=min_count)]['director'])\n",
    "disliked_dirs   = set(d_stats[(d_stats['mean']<=dislike_threshold) & (d_stats['count']>=min_count)]['director'])\n",
    "\n",
    "print(\"Scale:\", scale, \"| like_threshold:\", like_threshold, \"| dislike_threshold:\", dislike_threshold)\n",
    "print(\"Liked genres:\", sorted(list(liked_genres))[:10], \"... (n=\", len(liked_genres), \")\")\n",
    "print(\"Disliked genres:\", sorted(list(disliked_genres))[:10], \"... (n=\", len(disliked_genres), \")\")\n",
    "print(\"Liked directors:\", sorted(list(liked_dirs))[:10], \"... (n=\", len(liked_dirs), \")\")\n",
    "print(\"Disliked directors:\", sorted(list(disliked_dirs))[:10], \"... (n=\", len(disliked_dirs), \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f338c8",
   "metadata": {},
   "source": [
    "## Datalog Rules (with dislikes) — with fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a692b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "use_pyDatalog = False\n",
    "try:\n",
    "    from pyDatalog import pyDatalog\n",
    "    use_pyDatalog = True\n",
    "    print(\"pyDatalog is available — using it.\")\n",
    "except Exception as e:\n",
    "    print(\"pyDatalog not available, using fallback:\", e)\n",
    "\n",
    "USER = \"tobias\"\n",
    "\n",
    "if use_pyDatalog:\n",
    "    pyDatalog.clear()\n",
    "    pyDatalog.create_terms('watched_fact, candidateFor, onWatchlist, film_genre, director_fact, '\n",
    "                           'likesGenreFact, likesDirectorFact, dislikesGenreFact, dislikesDirectorFact, '\n",
    "                           'recommendedBase, watchBoost, genreBoost, dirBoost, '\n",
    "                           'genrePenalty, dirPenalty, recommended, U,N,Y,G,D')\n",
    "\n",
    "    # Facts\n",
    "    for n,y in watched_pairs:    +watched_fact(USER,n,y)\n",
    "    for n,y in watchlist_pairs:  +onWatchlist(USER,n,y)\n",
    "    for _,row in recs_df.iterrows():\n",
    "        +candidateFor(USER,row[\"name_norm\"],row[\"year_str\"])\n",
    "    for _,row in recs_df.dropna(subset=[\"genre_list\"]).iterrows():\n",
    "        for g in row[\"genre_list\"]:\n",
    "            +film_genre(row[\"name_norm\"], row[\"year_str\"], g)\n",
    "    for _,row in recs_df.dropna(subset=[\"director_list\"]).iterrows():\n",
    "        for d in row[\"director_list\"]:\n",
    "            +director_fact(row[\"name_norm\"], row[\"year_str\"], d)\n",
    "\n",
    "    for g in liked_genres:       +likesGenreFact(USER,g)\n",
    "    for d in liked_dirs:         +likesDirectorFact(USER,d)\n",
    "    for g in disliked_genres:    +dislikesGenreFact(USER,g)\n",
    "    for d in disliked_dirs:      +dislikesDirectorFact(USER,d)\n",
    "\n",
    "    # Rules\n",
    "    recommendedBase(U,N,Y) <= candidateFor(U,N,Y) & ~watched_fact(U,N,Y)\n",
    "    watchBoost(U,N,Y)      <= recommendedBase(U,N,Y) & onWatchlist(U,N,Y)\n",
    "    genreBoost(U,N,Y)      <= recommendedBase(U,N,Y) & film_genre(N,Y,G) & likesGenreFact(U,G)\n",
    "    dirBoost(U,N,Y)        <= recommendedBase(U,N,Y) & director_fact(N,Y,D) & likesDirectorFact(U,D)\n",
    "    genrePenalty(U,N,Y)    <= recommendedBase(U,N,Y) & film_genre(N,Y,G) & dislikesGenreFact(U,G)\n",
    "    dirPenalty(U,N,Y)      <= recommendedBase(U,N,Y) & director_fact(N,Y,D) & dislikesDirectorFact(U,D)\n",
    "    recommended(U,N,Y)     <= recommendedBase(U,N,Y)\n",
    "\n",
    "    # Collect\n",
    "    def qset(s): \n",
    "        ans = pyDatalog.ask(s); \n",
    "        return set(tuple(x) for x in (ans.answers if ans else []))\n",
    "\n",
    "    all_pairs   = qset(f'recommended(\"{USER}\", N, Y)')\n",
    "    watch_pairs = qset(f'watchBoost(\"{USER}\", N, Y)')\n",
    "    g_like      = qset(f'genreBoost(\"{USER}\", N, Y)')\n",
    "    d_like      = qset(f'dirBoost(\"{USER}\", N, Y)')\n",
    "    g_bad       = qset(f'genrePenalty(\"{USER}\", N, Y)')\n",
    "    d_bad       = qset(f'dirPenalty(\"{USER}\", N, Y)')\n",
    "\n",
    "    if all_pairs:\n",
    "        rec_df = pd.DataFrame(list(all_pairs), columns=[\"name_norm\",\"year_str\"])\n",
    "        out = recs_df.merge(rec_df, on=[\"name_norm\",\"year_str\"], how=\"inner\")\n",
    "    else:\n",
    "        out = recs_df.iloc[0:0].copy()\n",
    "\n",
    "    def flag(df, S, col):\n",
    "        df[col] = list(map(lambda p: p in S, zip(df[\"name_norm\"], df[\"year_str\"])))\n",
    "    flag(out, watch_pairs, \"watchlist_priority\")\n",
    "    flag(out, g_like, \"genre_boost\")\n",
    "    flag(out, d_like, \"director_boost\")\n",
    "    flag(out, g_bad,  \"genre_penalty\")\n",
    "    flag(out, d_bad,  \"director_penalty\")\n",
    "\n",
    "else:\n",
    "    # Fallback in Pandas\n",
    "    cand_pairs = list(zip(recs_df[\"name_norm\"], recs_df[\"year_str\"]))\n",
    "    keep_mask = [pair not in watched_pairs for pair in cand_pairs]\n",
    "    out = recs_df.loc[keep_mask].copy()\n",
    "\n",
    "    out[\"watchlist_priority\"] = list(map(lambda p: p in watchlist_pairs, zip(out[\"name_norm\"], out[\"year_str\"])))\n",
    "\n",
    "    def has_genre_boost(row):\n",
    "        gl = row.get(\"genre_list\", []) or []\n",
    "        return any(g in liked_genres for g in gl)\n",
    "    def has_dir_boost(row):\n",
    "        dl = row.get(\"director_list\", []) or []\n",
    "        return any(d in liked_dirs for d in dl)\n",
    "    def has_genre_penalty(row):\n",
    "        gl = row.get(\"genre_list\", []) or []\n",
    "        return any(g in disliked_genres for g in gl)\n",
    "    def has_dir_penalty(row):\n",
    "        dl = row.get(\"director_list\", []) or []\n",
    "        return any(d in disliked_dirs for d in dl)\n",
    "\n",
    "    out[\"genre_boost\"]    = out.apply(has_genre_boost, axis=1)\n",
    "    out[\"director_boost\"] = out.apply(has_dir_boost, axis=1)\n",
    "    out[\"genre_penalty\"]  = out.apply(has_genre_penalty, axis=1)\n",
    "    out[\"director_penalty\"]= out.apply(has_dir_penalty, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306bb5a",
   "metadata": {},
   "source": [
    "## Scoring, Ranking, Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weights (adjust to taste)\n",
    "w_watch = 2\n",
    "w_glike = 1\n",
    "w_dlike = 2\n",
    "w_gbad  = 1\n",
    "w_dbad  = 2\n",
    "\n",
    "out[\"score\"] = (out[\"watchlist_priority\"].astype(int)*w_watch +\n",
    "                out[\"genre_boost\"].astype(int)*w_glike +\n",
    "                out[\"director_boost\"].astype(int)*w_dlike -\n",
    "                out[\"genre_penalty\"].astype(int)*w_gbad -\n",
    "                out[\"director_penalty\"].astype(int)*w_dbad)\n",
    "\n",
    "sort_cols = [\"score\"]\n",
    "ascending = [False]\n",
    "if \"rank\" in out.columns:\n",
    "    sort_cols.append(\"rank\"); ascending.append(True)\n",
    "\n",
    "out_sorted = out.sort_values(by=sort_cols, ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_sorted.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv.resolve())\n",
    "\n",
    "summary = {\n",
    "    \"candidates_total\": int(len(recs_df)),\n",
    "    \"recommended_total\": int(len(out_sorted)),\n",
    "    \"watchlist_priority_true\": int(out_sorted[\"watchlist_priority\"].sum()),\n",
    "    \"genre_boost_true\": int(out_sorted[\"genre_boost\"].sum()),\n",
    "    \"director_boost_true\": int(out_sorted[\"director_boost\"].sum()),\n",
    "    \"genre_penalty_true\": int(out_sorted[\"genre_penalty\"].sum()),\n",
    "    \"director_penalty_true\": int(out_sorted[\"director_penalty\"].sum()),\n",
    "    \"top5_by_score\": list(out_sorted.head(5).get(\"candidate_title\", out_sorted.head(5).get(\"name\")).astype(str))\n",
    "}\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
