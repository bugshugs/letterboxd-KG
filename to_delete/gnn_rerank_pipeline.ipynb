{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46178abc",
   "metadata": {},
   "source": [
    "# Film-Recommender: LO3 — GNN-Reranking Pipeline\n",
    "*Generated: 2025-09-12T12:28:03 UTC*\n",
    "\n",
    "Dieses Notebook baut direkt auf deiner Datei **`rerank_by_logical_rules.csv`** auf und zeigt, wie du für LO3 (GNNs) einen Graph konstruierst, ein (heterogenes) GNN trainierst und die resultierenden Scores mit deinem bisherigen `final`-Score ensemblest.\n",
    "\n",
    "**What you’ll get:**\n",
    "1. CSV laden (aus deinem ZIP) und erkunden  \n",
    "2. Graph aus `seed → candidate` und `comp_*`-Relationen  \n",
    "3. GNN-Setup mit PyTorch Geometric (R-GCN) *oder* LightGCN (optional)  \n",
    "4. Training für Link Prediction `user → movie` bzw. Reranking der Kandidaten  \n",
    "5. Evaluation (Hit@K, NDCG@K) und **Ensemble mit `final`**  \n",
    "6. Export der neuen Rankings\n",
    "\n",
    "> **Hinweis:** Das Notebook ist so geschrieben, dass es **ohne Internet** zunächst eine *Baseline* (rein aus deinen CSV-Scores) rechnen kann.  \n",
    "> Für GNN-Training brauchst du **PyTorch** und **PyTorch Geometric**. Installationszellen sind enthalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4cf299e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:15:53.862519Z",
     "start_time": "2025-09-14T11:15:53.854272Z"
    }
   },
   "source": [
    "# === Pfade anpassen (lokales Projektsetup) ===\n",
    "CSV_PATH = \"rerank_by_logical_rules.csv\"  # relativer Pfad aus gnn/ zum CSV\n",
    "OUTPUT_DIR = \"outputs\"  # z. B. hierhin speichern\n",
    "OUTPUT_CSV_BASELINE = f\"{OUTPUT_DIR}/baseline_rerank.csv\"\n",
    "OUTPUT_CSV_GNN = f\"{OUTPUT_DIR}/gnn_rerank.csv\"\n",
    "OUTPUT_CSV_ENSEMBLE = f\"{OUTPUT_DIR}/ensemble_rerank.csv\"\n",
    "\n",
    "SEED = 42\n",
    "TOPK = 10\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "34a9e64b",
   "metadata": {},
   "source": [
    "## (Optional) Installationen für lokales Training\n",
    "Führe diese Zelle **lokal** (mit Internet) aus, wenn PyTorch/pyG noch nicht installiert sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a07e631c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:15:58.180752Z",
     "start_time": "2025-09-14T11:15:53.891012Z"
    }
   },
   "source": [
    "!pip install --upgrade pip\n",
    "import sys, platform\n",
    "# Wähle das passende Torch-Whl für deine CUDA/CPU-Umgebung, z. B.:\n",
    "# CPU-Only (einfachster Weg):\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "# PyTorch Geometric Kernpakete:\n",
    "!pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-$(python -c \"import torch;print(torch.__version__.split('+')[0])\").html\n",
    "# Optional: LightGCN-Referenz-Implementationen (nur falls gewünscht)\n",
    "# !pip install recbole  # enthält LightGCN, benötigt evtl. weitere Pakete\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.13/site-packages (25.2)\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\r\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.14.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.8.0.html\r\n",
      "Requirement already satisfied: torch-geometric in /opt/anaconda3/lib/python3.13/site-packages (2.6.1)\r\n",
      "Requirement already satisfied: torch-scatter in /opt/anaconda3/lib/python3.13/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: torch-sparse in /opt/anaconda3/lib/python3.13/site-packages (0.6.18)\r\n",
      "Requirement already satisfied: torch-cluster in /opt/anaconda3/lib/python3.13/site-packages (1.6.3)\r\n",
      "Requirement already satisfied: torch-spline-conv in /opt/anaconda3/lib/python3.13/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (3.11.10)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (2025.3.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (2.1.3)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (5.9.0)\r\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (3.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.13/site-packages (from torch-sparse) (1.15.3)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (2.4.4)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (24.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (6.1.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp->torch-geometric) (1.18.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch-geometric) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->torch-geometric) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->torch-geometric) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->torch-geometric) (2025.7.14)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c3614988",
   "metadata": {},
   "source": [
    "## 1) Daten laden\n",
    "Wir extrahieren `rerank_by_logical_rules.csv` aus dem ZIP und schauen uns die Spalten an.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "063b8e4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:15:59.774114Z",
     "start_time": "2025-09-14T11:15:59.079218Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(df.shape)\n",
    "df.head(3)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 41)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   candidate_id        candidate_title    year  cos    meta   final  \\\n",
       "0          2756              The Abyss  1989.0  0.0  0.4592  0.1837   \n",
       "1          1991            Death Proof  2007.0  0.0  0.5010  0.2004   \n",
       "2         28387  Kicking and Screaming  1995.0  0.0  0.5109  0.2044   \n",
       "\n",
       "                                        seed  comp_genres  comp_keywords  \\\n",
       "0                                     Aliens       0.5000         0.0000   \n",
       "1                          Kill Bill: Vol. 2       0.6667         0.0179   \n",
       "2  The Meyerowitz Stories (New and Selected)       0.6667         0.0000   \n",
       "\n",
       "   comp_cast  ...              name_norm  year_str  \\\n",
       "0     0.0256  ...              the abyss    1989.0   \n",
       "1     0.0526  ...            death proof    2007.0   \n",
       "2     0.0256  ...  kicking and screaming    1995.0   \n",
       "\n",
       "                                     genre_list          director_list  \\\n",
       "0  ['Adventure', 'Thriller', 'Science Fiction']      ['James Cameron']   \n",
       "1                        ['Action', 'Thriller']  ['Quentin Tarantino']   \n",
       "2                ['Comedy', 'Drama', 'Romance']      ['Noah Baumbach']   \n",
       "\n",
       "   watchlist_priority genre_boost director_boost genre_penalty  \\\n",
       "0                True        True           True         False   \n",
       "1               False        True           True         False   \n",
       "2               False        True           True         False   \n",
       "\n",
       "   director_penalty  score  \n",
       "0             False      5  \n",
       "1             False      3  \n",
       "2             False      3  \n",
       "\n",
       "[3 rows x 41 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>candidate_title</th>\n",
       "      <th>year</th>\n",
       "      <th>cos</th>\n",
       "      <th>meta</th>\n",
       "      <th>final</th>\n",
       "      <th>seed</th>\n",
       "      <th>comp_genres</th>\n",
       "      <th>comp_keywords</th>\n",
       "      <th>comp_cast</th>\n",
       "      <th>...</th>\n",
       "      <th>name_norm</th>\n",
       "      <th>year_str</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>director_list</th>\n",
       "      <th>watchlist_priority</th>\n",
       "      <th>genre_boost</th>\n",
       "      <th>director_boost</th>\n",
       "      <th>genre_penalty</th>\n",
       "      <th>director_penalty</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2756</td>\n",
       "      <td>The Abyss</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4592</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>Aliens</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>...</td>\n",
       "      <td>the abyss</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>['Adventure', 'Thriller', 'Science Fiction']</td>\n",
       "      <td>['James Cameron']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>Death Proof</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5010</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>Kill Bill: Vol. 2</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>...</td>\n",
       "      <td>death proof</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>['Action', 'Thriller']</td>\n",
       "      <td>['Quentin Tarantino']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28387</td>\n",
       "      <td>Kicking and Screaming</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5109</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>The Meyerowitz Stories (New and Selected)</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>...</td>\n",
       "      <td>kicking and screaming</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>['Comedy', 'Drama', 'Romance']</td>\n",
       "      <td>['Noah Baumbach']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 41 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "6dee55d6",
   "metadata": {},
   "source": [
    "### Grundbereinigung & Typen\n",
    "Wir stellen sicher, dass Score-Spalten numerisch sind und fehlende Werte sinnvoll ersetzt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9ffe60e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:15:59.878028Z",
     "start_time": "2025-09-14T11:15:59.840901Z"
    }
   },
   "source": [
    "# Kandidatenhafte Standardspalten, die es laut deiner Beschreibung gibt.\n",
    "candidate_like_cols = [\n",
    "    'seed', 'candidate_id', 'candidate_title', 'cos', 'final', 'score',\n",
    "    'comp_genres','comp_keywords','comp_cast','comp_director','comp_runtime',\n",
    "    'comp_language','comp_popularity','comp_vote'\n",
    "]\n",
    "\n",
    "for c in df.columns:\n",
    "    if c.startswith('comp_') or c in ['cos','final','score']:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# fehlende Kompatibilitäten als 0 interpretieren\n",
    "for c in [c for c in df.columns if c.startswith('comp_')]:\n",
    "    df[c] = df[c].fillna(0.0)\n",
    "\n",
    "for c in ['cos','final','score']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# ein paar Hilfssichten\n",
    "print(\"Spalten:\", list(df.columns))\n",
    "print(df.describe(include='all').transpose().head(12))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten: ['candidate_id', 'candidate_title', 'year', 'cos', 'meta', 'final', 'seed', 'comp_genres', 'comp_keywords', 'comp_cast', 'comp_director', 'comp_runtime', 'comp_language', 'comp_popularity', 'comp_vote', 'tmdb_url', 'overview', 'genres', 'runtime', 'vote_average', 'poster_url', 'media_type', 'director', 'actors', 'characters', 'origin_country', 'original_language', 'popularity', 'production_companies', 'production_countries', 'spoken_languages', 'name_norm', 'year_str', 'genre_list', 'director_list', 'watchlist_priority', 'genre_boost', 'director_boost', 'genre_penalty', 'director_penalty', 'score']\n",
      "                 count unique                                        top freq  \\\n",
      "candidate_id     200.0    NaN                                        NaN  NaN   \n",
      "candidate_title    200    200                                  The Abyss    1   \n",
      "year             199.0    NaN                                        NaN  NaN   \n",
      "cos              200.0    NaN                                        NaN  NaN   \n",
      "meta             200.0    NaN                                        NaN  NaN   \n",
      "final            200.0    NaN                                        NaN  NaN   \n",
      "seed               200    130  The Meyerowitz Stories (New and Selected)    7   \n",
      "comp_genres      200.0    NaN                                        NaN  NaN   \n",
      "comp_keywords    200.0    NaN                                        NaN  NaN   \n",
      "comp_cast        200.0    NaN                                        NaN  NaN   \n",
      "comp_director    200.0    NaN                                        NaN  NaN   \n",
      "comp_runtime     200.0    NaN                                        NaN  NaN   \n",
      "\n",
      "                        mean            std     min       25%      50%  \\\n",
      "candidate_id       85991.545  242797.516272   159.0    1988.0  10104.0   \n",
      "candidate_title          NaN            NaN     NaN       NaN      NaN   \n",
      "year             1993.748744      18.735727  1921.0    1987.0   1998.0   \n",
      "cos                 0.015915       0.063595     0.0       0.0      0.0   \n",
      "meta                0.470215       0.046234  0.3224  0.451275   0.4563   \n",
      "final               0.197636       0.038093   0.179  0.180775   0.1833   \n",
      "seed                     NaN            NaN     NaN       NaN      NaN   \n",
      "comp_genres         0.903169       0.176865     0.4      0.95      1.0   \n",
      "comp_keywords       0.073474       0.107405     0.0  0.027425   0.0476   \n",
      "comp_cast           0.019429       0.071228     0.0       0.0      0.0   \n",
      "comp_director        0.27875       0.444553     0.0       0.0      0.0   \n",
      "comp_runtime        0.912667       0.157658  0.0261    0.9104   0.9731   \n",
      "\n",
      "                      75%        max  \n",
      "candidate_id     25265.75  1397640.0  \n",
      "candidate_title       NaN        NaN  \n",
      "year               2005.5     2025.0  \n",
      "cos                   0.0     0.3688  \n",
      "meta              0.47335     0.7713  \n",
      "final            0.194375     0.4301  \n",
      "seed                  NaN        NaN  \n",
      "comp_genres           1.0        1.0  \n",
      "comp_keywords      0.0833        1.0  \n",
      "comp_cast             0.0     0.6316  \n",
      "comp_director         1.0        1.0  \n",
      "comp_runtime       0.9912        1.0  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "838e388c",
   "metadata": {},
   "source": [
    "## 2) Graph aus CSV konstruieren (Relationen & Gewichte)\n",
    "Wir erzeugen Kanten vom Seed-Film zum Kandidaten sowie zusätzliche Relationen aus `comp_*`-Spalten.  \n",
    "Fürs reine Python-Preview bauen wir Edge-Listen und normalisieren Gewichte auf [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3439cf89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.287188Z",
     "start_time": "2025-09-14T11:16:00.034352Z"
    }
   },
   "source": [
    "# Hilfsfunktionen\n",
    "def minmax(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mn, mx = np.nanmin(x), np.nanmax(x)\n",
    "    if mx <= mn:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "# Alle Filme (Seed + Candidate) identifizieren\n",
    "movies = pd.unique(pd.concat([df['seed'], df['candidate_title']], ignore_index=True))\n",
    "movie2id = {m:i for i,m in enumerate(movies)}\n",
    "id2movie = {i:m for m,i in movie2id.items()}\n",
    "\n",
    "# Edge-Typen definieren\n",
    "rel_cols = ['cos','final','comp_genres','comp_keywords','comp_cast','comp_director',\n",
    "            'comp_runtime','comp_language','comp_popularity','comp_vote']\n",
    "rel_cols = [c for c in rel_cols if c in df.columns]\n",
    "\n",
    "edges = {c: [] for c in rel_cols}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    s = row['seed']; c = row['candidate_title']\n",
    "    if pd.isna(s) or pd.isna(c): \n",
    "        continue\n",
    "    u, v = movie2id[s], movie2id[c]\n",
    "    for rc in rel_cols:\n",
    "        w = row[rc]\n",
    "        if pd.notna(w):\n",
    "            edges[rc].append((u, v, float(w)))\n",
    "\n",
    "# Normalisierung je Relation\n",
    "norm_edges = {}\n",
    "for rc, lst in edges.items():\n",
    "    if not lst:\n",
    "        continue\n",
    "    w = np.array([w for (_,_,w) in lst], dtype=float)\n",
    "    wn = minmax(w)\n",
    "    norm_edges[rc] = [(u,v,float(wn[i])) for i,(u,v,_) in enumerate(lst)]\n",
    "\n",
    "# Statistiken\n",
    "for rc, lst in norm_edges.items():\n",
    "    print(rc, \"Edges:\", len(lst), \"Beispiel:\", lst[:3])\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 36\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lst:\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m w \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([w \u001B[38;5;28;01mfor\u001B[39;00m (_,_,w) \u001B[38;5;129;01min\u001B[39;00m lst], dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m)\n\u001B[1;32m     37\u001B[0m wn \u001B[38;5;241m=\u001B[39m minmax(w)\n\u001B[1;32m     38\u001B[0m norm_edges[rc] \u001B[38;5;241m=\u001B[39m [(u,v,\u001B[38;5;28mfloat\u001B[39m(wn[i])) \u001B[38;5;28;01mfor\u001B[39;00m i,(u,v,_) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(lst)]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "5c80aa5e",
   "metadata": {},
   "source": [
    "## 3) Baseline-Reranking (ohne GNN) — zum Vergleich\n",
    "Wir verwenden den vorhandenen `final`-Score als Baseline und evaluieren Hit@K/NDCG@K gegenüber einer Proxy-„Relevanz“.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a66fa802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.382248Z",
     "start_time": "2025-09-12T14:29:42.255533Z"
    }
   },
   "source": [
    "# Proxy-Relevanz: Falls es eine Spalte mit Bodenwahrheit gibt (z. B. 'label', 'liked'), verwende sie.\n",
    "# Hier fallback: Relevanz = 1 für Top-X nach 'score' oder 'final' je Seed (nur für demo). \n",
    "label_col_candidates = [c for c in ['label','liked','relevant','gt','y'] if c in df.columns]\n",
    "if label_col_candidates:\n",
    "    LABEL_COL = label_col_candidates[0]\n",
    "else:\n",
    "    LABEL_COL = None\n",
    "\n",
    "def ndcg_at_k(rel, k=10):\n",
    "    rel = np.array(rel)[:k]\n",
    "    dcg = np.sum((2**rel - 1) / np.log2(np.arange(2, len(rel)+2)))\n",
    "    ideal = np.sort(rel)[::-1]\n",
    "    idcg = np.sum((2**ideal[:k] - 1) / np.log2(np.arange(2, min(k, len(ideal))+2)))\n",
    "    return (dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def hit_at_k(rel, k=10):\n",
    "    return 1.0 if np.any(np.array(rel)[:k] > 0) else 0.0\n",
    "\n",
    "def evaluate_grouped(df, score_col, k=10):\n",
    "    hits, ndcgs, n = [], [], 0\n",
    "    for seed, g in df.groupby('seed', sort=False):\n",
    "        g = g.sort_values(score_col, ascending=False)\n",
    "        if LABEL_COL is None:\n",
    "            # pseudo-label: Top-1 nach 'score' (falls vorhanden), sonst Zufallspositiv\n",
    "            if 'score' in g.columns:\n",
    "                pos_idx = g[score_col].rank(ascending=False, method='first').idxmin()\n",
    "                rel = (g.index == pos_idx).astype(int)\n",
    "            else:\n",
    "                rel = np.zeros(len(g), dtype=int)\n",
    "                if len(g)>0:\n",
    "                    rel[np.random.randint(len(g))] = 1\n",
    "        else:\n",
    "            rel = (g[LABEL_COL] > 0).astype(int).values\n",
    "\n",
    "        hits.append(hit_at_k(rel, k))\n",
    "        ndcgs.append(ndcg_at_k(rel, k))\n",
    "        n += 1\n",
    "    return float(np.mean(hits) if hits else 0.0), float(np.mean(ndcgs) if ndcgs else 0.0), n\n",
    "\n",
    "if 'final' in df.columns:\n",
    "    h, n, cnt = evaluate_grouped(df, 'final', k=TOPK)\n",
    "    print(f\"Baseline (`final`) — Hit@{TOPK}: {h:.3f}, NDCG@{TOPK}: {n:.3f} (Seeds: {cnt})\")\n",
    "else:\n",
    "    print(\"Keine 'final'-Spalte gefunden; Baseline wird übersprungen.\")\n",
    "\n",
    "# Export Baseline-Ranking\n",
    "if 'final' in df.columns:\n",
    "    df.sort_values(['seed','final'], ascending=[True, False]).to_csv(OUTPUT_CSV_BASELINE, index=False)\n",
    "    print(f\"Baseline-Ranking exportiert: {OUTPUT_CSV_BASELINE}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (`final`) — Hit@10: 1.000, NDCG@10: 1.000 (Seeds: 130)\n",
      "Baseline-Ranking exportiert: ../data/kg/outputs/baseline_rerank.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "efdabb08",
   "metadata": {},
   "source": [
    "## 4) GNN-Training (R-GCN über heterogene Film–Film-Relationen)\n",
    "Wir verwenden **PyTorch Geometric** (sofern installiert), um aus den Relationstypen (`cos`, `comp_*`, …) ein heterogenes GNN zu trainieren.  \n",
    "Ziel: Ein **Item-Scoring** pro Seed-Kandidaten ableiten (Link-Prediction-Proxy), das wir anschließend zum Reranking nutzen.\n",
    "\n",
    "> Wenn `torch`/`torch_geometric` nicht verfügbar sind, überspringt diese Sektion automatisch.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "95895932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.438642Z",
     "start_time": "2025-09-12T14:29:46.954566Z"
    }
   },
   "source": [
    "import importlib, math, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "has_torch = importlib.util.find_spec('torch') is not None\n",
    "has_pyg = importlib.util.find_spec('torch_geometric') is not None\n",
    "\n",
    "print(\"Torch installiert:\", has_torch, \"| PyG installiert:\", has_pyg)\n",
    "\n",
    "if not (has_torch and has_pyg):\n",
    "    print(\"GNN-Teil wird übersprungen (Pakete fehlen). Du kannst die Installationszelle oben lokal ausführen und diese Zelle dann erneut laufen lassen.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch installiert: True | PyG installiert: True\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "53a2e826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.522182Z",
     "start_time": "2025-09-12T15:27:08.685205Z"
    }
   },
   "source": [
    "if has_torch and has_pyg:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.data import HeteroData\n",
    "    from torch_geometric.nn import HeteroConv, GATv2Conv  # alternativ: SAGEConv\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # === 1) HeteroData nur für die Kantenstruktur ===\n",
    "    data = HeteroData()\n",
    "    num_movies = len(movie2id)\n",
    "    d = 64\n",
    "\n",
    "    # movie->movie Kanten aus den normalisierten Relationen\n",
    "    for rc, lst in norm_edges.items():\n",
    "        if not lst:\n",
    "            continue\n",
    "        src = torch.tensor([u for (u,_,_) in lst], dtype=torch.long)\n",
    "        dst = torch.tensor([v for (_,v,_) in lst], dtype=torch.long)\n",
    "        data['movie', rc, 'movie'].edge_index = torch.stack([src, dst], dim=0)\n",
    "\n",
    "    # user->movie (positive Beispiele: Top-1 pro Seed)\n",
    "    pos_pairs = []\n",
    "    for seed, g in df.groupby('seed', sort=False):\n",
    "        if len(g) == 0:\n",
    "            continue\n",
    "        if 'final' in g.columns:\n",
    "            g = g.sort_values('final', ascending=False)\n",
    "        cand = g.iloc[0]['candidate_title']\n",
    "        if pd.isna(cand):\n",
    "            continue\n",
    "        mid = movie2id.get(cand)\n",
    "        if mid is not None:\n",
    "            pos_pairs.append((0, mid))\n",
    "\n",
    "    if not pos_pairs:\n",
    "        raise RuntimeError(\"Keine positiven Paare gefunden. Prüfe die 'final'-Spalte und den CSV-Inhalt.\")\n",
    "\n",
    "    u_src = torch.tensor([p[0] for p in pos_pairs], dtype=torch.long)\n",
    "    m_dst = torch.tensor([p[1] for p in pos_pairs], dtype=torch.long)\n",
    "    data['user','likes','movie'].edge_index = torch.stack([u_src, m_dst], dim=0)\n",
    "\n",
    "    # Negative Samples (random ungesehene)\n",
    "    all_movie_ids = torch.arange(num_movies, dtype=torch.long)\n",
    "    pos_set = set(m_dst.tolist())\n",
    "    neg_pairs = []\n",
    "    for _ in range(len(pos_pairs) * 3):\n",
    "        neg_m = int(all_movie_ids[torch.randint(0, num_movies, (1,))])\n",
    "        if neg_m not in pos_set:\n",
    "            neg_pairs.append((0, neg_m))\n",
    "    if not neg_pairs:\n",
    "        neg_pairs.append((0, int(all_movie_ids[0])))\n",
    "    un_src = torch.tensor([p[0] for p in neg_pairs], dtype=torch.long)\n",
    "    mn_dst = torch.tensor([p[1] for p in neg_pairs], dtype=torch.long)\n",
    "\n",
    "    # Nur Kanten, die auf 'movie' zeigen, werden für die Convs benutzt\n",
    "    conv_edge_types = [et for et in data.edge_types if et[2] == 'movie']\n",
    "    edge_index_dict = {et: data[et].edge_index for et in conv_edge_types}\n",
    "\n",
    "    # === 2) Modell mit internen Embeddings (vermeidet None/Shape-Probleme) ===\n",
    "    class HeteroRecommender(nn.Module):\n",
    "        def __init__(self, num_movies, dim=64, layers=2):\n",
    "            super().__init__()\n",
    "            self.movie_emb = nn.Embedding(num_movies, dim)\n",
    "            self.user_emb  = nn.Embedding(1, dim)\n",
    "\n",
    "            self.layers = nn.ModuleList()\n",
    "            for _ in range(layers):\n",
    "                self.layers.append(\n",
    "                    HeteroConv(\n",
    "                        { et: GATv2Conv((-1, -1), dim, add_self_loops=False)\n",
    "                          for et in conv_edge_types },\n",
    "                        aggr='sum'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        def forward(self, edge_index_dict):\n",
    "            # baue x_dict on-the-fly aus Embeddings\n",
    "            x_dict = {\n",
    "                'movie': self.movie_emb.weight,   # [num_movies, d]\n",
    "                'user' : self.user_emb.weight     # [1, d]\n",
    "            }\n",
    "            # nur auf Relationen mit dst='movie' propagieren\n",
    "            out = x_dict\n",
    "            for conv in self.layers:\n",
    "                out_partial = conv(out, edge_index_dict)\n",
    "                # conv liefert nur Outputs für betroffene Zieltypen (hier 'movie'):\n",
    "                out = {**out, **{k: F.relu(v) for k, v in out_partial.items()}}\n",
    "            return out\n",
    "\n",
    "        @staticmethod\n",
    "        def score(user_vec, item_vec):\n",
    "            return (user_vec * item_vec).sum(dim=-1)\n",
    "\n",
    "    model = HeteroRecommender(num_movies=num_movies, dim=d, layers=2)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # === 3) Training ===\n",
    "    for epoch in range(1, 201):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x_out = model(edge_index_dict)\n",
    "        user_pos = x_out['user'][u_src]       # (P, d)\n",
    "        item_pos = x_out['movie'][m_dst]      # (P, d)\n",
    "        user_neg = x_out['user'][un_src]      # (N, d)\n",
    "        item_neg = x_out['movie'][mn_dst]     # (N, d)\n",
    "\n",
    "        pos_logit = HeteroRecommender.score(user_pos, item_pos)\n",
    "        neg_logit = HeteroRecommender.score(user_neg, item_neg)\n",
    "\n",
    "        loss = bce(pos_logit, torch.ones_like(pos_logit)) + \\\n",
    "               bce(neg_logit, torch.zeros_like(neg_logit))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    # === 4) Scoring aller Kandidaten je Seed ===\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_out = model(edge_index_dict)\n",
    "        user_vec = x_out['user'][0:1]  # einziger User\n",
    "\n",
    "    gnn_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        cand = row['candidate_title']\n",
    "        if pd.isna(cand):\n",
    "            gnn_scores.append(np.nan); continue\n",
    "        mid = movie2id.get(cand, None)\n",
    "        if mid is None:\n",
    "            gnn_scores.append(np.nan); continue\n",
    "        item_vec = x_out['movie'][mid:mid+1]\n",
    "        s = float((user_vec * item_vec).sum(dim=-1))\n",
    "        gnn_scores.append(s)\n",
    "\n",
    "    df['s_gnn'] = gnn_scores\n",
    "    df['s_gnn_norm'] = df.groupby('seed')['s_gnn'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9)\n",
    "    )\n",
    "    print(\"GNN-Scoring hinzugefügt: Spalten 's_gnn'/'s_gnn_norm'\")\n",
    "\n",
    "    df.sort_values(['seed','s_gnn_norm'], ascending=[True, False]).to_csv(OUTPUT_CSV_GNN, index=False)\n",
    "    print(f\"GNN-Rerank exportiert: {OUTPUT_CSV_GNN}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 0.0146\n",
      "Epoch 100 | Loss 0.0059\n",
      "Epoch 150 | Loss 0.0035\n",
      "Epoch 200 | Loss 0.0023\n",
      "GNN-Scoring hinzugefügt: Spalten 's_gnn'/'s_gnn_norm'\n",
      "GNN-Rerank exportiert: ../data/kg/outputs/gnn_rerank.csv\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "7ac59b5e",
   "metadata": {},
   "source": [
    "## 5) Ensemble: GNN + `final`\n",
    "Wir kombinieren deinen existierenden `final`-Score mit dem GNN-Score:  \n",
    "\\( \\text{score\\_final} = \\lambda\\, s_{\\text{gnn}} + (1-\\lambda)\\, \\text{final} \\)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe513fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.559238Z",
     "start_time": "2025-09-12T15:27:19.316316Z"
    }
   },
   "source": [
    "LAMBDA = 0.6  # kannst du tunen\n",
    "\n",
    "if 'final' not in df.columns:\n",
    "    print(\"Keine 'final'-Spalte gefunden — Ensemble übersprungen.\")\n",
    "else:\n",
    "    if 's_gnn_norm' not in df.columns:\n",
    "        # Fallback: ohne GNN einfach die Baseline kopieren\n",
    "        df['s_gnn_norm'] = 0.0\n",
    "        print(\"Warnung: Kein GNN-Score gefunden; Ensemble entspricht Baseline.\")\n",
    "    # Normiere final pro Seed auf [0,1]\n",
    "    df['final_norm'] = df.groupby('seed')['final'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "    df['score_ensemble'] = LAMBDA * df['s_gnn_norm'] + (1.0 - LAMBDA) * df['final_norm']\n",
    "\n",
    "    # Evaluation\n",
    "    h_b, n_b, cnt = evaluate_grouped(df, 'final', k=TOPK) if 'final' in df.columns else (0,0,0)\n",
    "    h_e, n_e, _   = evaluate_grouped(df, 'score_ensemble', k=TOPK)\n",
    "    print(f\"Baseline Hit@{TOPK}: {h_b:.3f}, NDCG@{TOPK}: {n_b:.3f}\")\n",
    "    print(f\"Ensemble  Hit@{TOPK}: {h_e:.3f}, NDCG@{TOPK}: {n_e:.3f}\")\n",
    "\n",
    "    # Export\n",
    "    df.sort_values(['seed','score_ensemble'], ascending=[True, False]).to_csv(OUTPUT_CSV_ENSEMBLE, index=False)\n",
    "    print(f\"Ensemble-Ranking exportiert: {OUTPUT_CSV_ENSEMBLE}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Hit@10: 1.000, NDCG@10: 1.000\n",
      "Ensemble  Hit@10: 1.000, NDCG@10: 1.000\n",
      "Ensemble-Ranking exportiert: ../data/kg/outputs/ensemble_rerank.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "e5b2ba6e",
   "metadata": {},
   "source": [
    "## 6) (Optional) LightGCN-Variante\n",
    "Wenn du echte **User–Movie**-Interaktionen (Ratings/Watchlist) einbaust, ist **LightGCN** sehr effizient.  \n",
    "Vorgehen:\n",
    "1. Baue bipartiten Graph `user–movie` aus deinen Interaktionen (positive/negative Kanten via Sampling).  \n",
    "2. Trainiere mit **BPR-Loss**.  \n",
    "3. Nutze zusätzlich deinen **Item–Item**-Graph aus `comp_*` als Regularizer (z. B. SGC/LGConv auf Item-Embeddings zwischen den LightGCN-Runden).\n",
    "\n",
    "> Implementierungshilfen: Recbole/LightGCN oder PyG-Implementationen. Dieses Notebook fokussiert R-GCN für LO3.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T11:16:00.596009Z",
     "start_time": "2025-09-12T15:30:47.425321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cells is in big parts AI generated by the free and paid version of ChatGPT and was afterwards heavily adapted by me. Since it is not possible to accurately say which parts were originaly AI generated by wich promt, I have included all prompts that were used on this file here.\n",
    "These following prompts were used:\n",
    "\n",
    "    \"Das hochgeladene Archiv ist mein Projekt. Über diesen Link https://kg.dbai.tuwien.ac.at/kg-course/details/ sind alle Learning Objectives, die ich erfüllen muss, einzusehen. das PDF erhält meinen On-Pager, in dem ich dmein Projekt kurz erkläre. LO1 und LO2 decke ich schon ab, als nächstes möchte ich LO3 angehen. Bitte erkläre mir, wie ich für mein Film-Empfehlungsprojekt sinnvoll GNN einsetzen kann. Am liebsten würde ich mit der Datei \"rerank_by_logical_rules.csv\" weiterarbeiten, die schon meine mit Embeddings und logischen Regeln bearbeiteten Empfehlungen beinhaltet.\"\n",
    "\n",
    "    \"Ja bitte. Mach mir am besten ein .ipynb file daraus\"\n",
    "\n",
    "    \"das ist meine Ordnerstruktur. \"rerank_by_logical_rules.csv\" befindet sich auf folgendem pfad: \"letterboxd-KG/data/kg/rerank_by_logical_rules.csv\". Das generierte File befindet sich auf folgendem Pfad: \"letterboxd-KG/gnn/gnn_rerank_pipeline.ipynb\". Kannst du mir die zeilen umschreiben?\"\n",
    "\n",
    "    \"--------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[10], line 41 38 return float(np.mean(hits) if hits else 0.0), float(np.mean(ndcgs) if ndcgs else 0.0), n 40 if 'final' in df.columns: ---> 41 h, n, cnt = evaluate_grouped(df, 'final', k=TOPK) 42 print(f\"Baseline (final) — Hit@{TOPK}: {h:.3f}, NDCG@{TOPK}: {n:.3f} (Seeds: {cnt})\") 43 else: Cell In[10], line 27, in evaluate_grouped(df, score_col, k) 25 if 'score' in g.columns: 26 pos_idx = g[score_col].rank(ascending=False, method='first').idxmin() ---> 27 rel = (g.index == pos_idx).astype(int).values 28 else: 29 rel = np.zeros(len(g), dtype=int) AttributeError: 'numpy.ndarray' object has no attribute 'values'\"\n",
    "\n",
    "    \"ich habe .values entfernt. nun bekomme ich diesen fehler: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[13], line 85 80 rel_id_map = {et:i for i,et in enumerate(rel_types)} 82 # RGCNConv erwartet 'edge_type' für homogene Darstellung; wir lassen PyG das intern managen, 83 # indem wir das Hetero-Modell wie gezeigt mit dicts füttern. ---> 85 model = RGCNRecommender(in_dim=d, hid=d, out_dim=d, num_rels=len(rel_types)) 86 opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5) 88 def score_user_item(user_emb, item_emb): 89 # simples Skalarprodukt Cell In[13], line 69, in RGCNRecommender.__init__(self, in_dim, hid, out_dim, num_rels) 67 def __init__(self, in_dim=64, hid=64, out_dim=64, num_rels=0): 68 super().__init__() ---> 69 self.conv1 = RGCNConv(in_dim, hid, num_rels=num_rels) 70 self.conv2 = RGCNConv(hid, out_dim, num_rels=num_rels) TypeError: RGCNConv.__init__() missing 1 required positional argument: 'num_relations'\"\n",
    "\n",
    "    \"kannst du mir die komplette zelle nochmal am stück anzeigen?\"\n",
    "\n",
    "    \"mit diesem code in der zelle bekomme ich folgende fehler: --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) Cell In[15], line 94 92 model.train() 93 opt.zero_grad() ---> 94 x_out = model({'movie': data['movie'].x, 'user': data['user'].x}, edge_index_dict) 96 user_emb = x_out['user'][u_src] 97 pos_item_emb = x_out['movie'][m_dst] File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs) 1771 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1772 else: -> 1773 return self._call_impl(*args, **kwargs) File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784, in Module._call_impl(self, *args, **kwargs) 1779 # If we don't have any hooks, we want to skip the rest of the logic in 1780 # this function, and just call forward. 1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1782 or _global_backward_pre_hooks or _global_backward_hooks 1783 or _global_forward_hooks or _global_forward_pre_hooks): -> 1784 return forward_call(*args, **kwargs) 1786 result = None 1787 called_always_called_hooks = set() Cell In[15], line 76, in HeteroRecommender.forward(self, x_dict, edge_index_dict) 74 def forward(self, x_dict, edge_index_dict): 75 for conv in self.layers: ---> 76 x_dict = conv(x_dict, edge_index_dict) 77 x_dict = {k: F.relu(v) for k,v in x_dict.items()} 78 return x_dict File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs) 1771 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1772 else: -> 1773 return self._call_impl(*args, **kwargs) File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784, in Module._call_impl(self, *args, **kwargs) 1779 # If we don't have any hooks, we want to skip the rest of the logic in 1780 # this function, and just call forward. 1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1782 or _global_backward_pre_hooks or _global_backward_hooks 1783 or _global_forward_hooks or _global_forward_pre_hooks): -> 1784 return forward_call(*args, **kwargs) 1786 result = None 1787 called_always_called_hooks = set() File /opt/anaconda3/lib/python3.13/site-packages/torch_geometric/nn/conv/hetero_conv.py:158, in HeteroConv.forward(self, *args_dict, **kwargs_dict) 155 if not has_edge_level_arg: 156 continue --> 158 out = conv(*args, **kwargs) 160 if dst not in out_dict: 161 out_dict[dst] = [out] File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs) 1771 return self._compiled_call_impl(*args, **kwargs) # type: ignore[misc] 1772 else: -> 1773 return self._call_impl(*args, **kwargs) File /opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1784, in Module._call_impl(self, *args, **kwargs) 1779 # If we don't have any hooks, we want to skip the rest of the logic in 1780 # this function, and just call forward. 1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks 1782 or _global_backward_pre_hooks or _global_backward_hooks 1783 or _global_forward_hooks or _global_forward_pre_hooks): -> 1784 return forward_call(*args, **kwargs) 1786 result = None 1787 called_always_called_hooks = set() File /opt/anaconda3/lib/python3.13/site-packages/torch_geometric/nn/conv/gatv2_conv.py:293, in GATv2Conv.forward(self, x, edge_index, edge_attr, return_attention_weights) 291 else: 292 x_l, x_r = x[0], x[1] --> 293 assert x[0].dim() == 2 295 if x_r is not None and self.res is not None: 296 res = self.res(x_r) AttributeError: 'NoneType' object has no attribute 'dim'\"\n",
    "\n",
    "    \"\"\n",
    "\n",
    "'''"
   ],
   "id": "5915d5b340f3ea2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe code in the previous cells is in big parts AI generated by the free and paid version of ChatGPT and was afterwards heavily adapted by me. Since it is not possible to accurately say which parts were originaly AI generated by wich promt, I have included all prompts that were used on this file here.\\nThese following prompts were used:\\n\\n    \"Das hochgeladene Archiv ist mein Projekt. Über diesen Link https://kg.dbai.tuwien.ac.at/kg-course/details/ sind alle Learning Objectives, die ich erfüllen muss, einzusehen. das PDF erhält meinen On-Pager, in dem ich dmein Projekt kurz erkläre. LO1 und LO2 decke ich schon ab, als nächstes möchte ich LO3 angehen. Bitte erkläre mir, wie ich für mein Film-Empfehlungsprojekt sinnvoll GNN einsetzen kann. Am liebsten würde ich mit der Datei \"rerank_by_logical_rules.csv\" weiterarbeiten, die schon meine mit Embeddings und logischen Regeln bearbeiteten Empfehlungen beinhaltet.\"\\n\\n    \"Ja bitte. Mach mir am besten ein .ipynb file daraus\"\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
