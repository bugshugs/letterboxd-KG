{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d8c16c",
   "metadata": {},
   "source": [
    "# Film-Recommender — LO1–LO3 **komplett** (Labels, Zeit-Split, Ablation, Edge-Weights)\n",
    "*Generated: 2025-09-12T16:00:05 UTC*\n",
    "\n",
    "Dieses Notebook erweitert dein Projekt um:\n",
    "- ✅ **Echte Labels** aus dem Letterboxd-Export (Ratings/Watchlist)\n",
    "- ✅ **Zeitbasierten Split** (Train auf älter, Test auf neuer)\n",
    "- ✅ **Edge-Weighted GNN** (GATv2 mit `edge_attr` aus `comp_*`/`cos`/`final`)\n",
    "- ✅ **Ablation & Baselines**: LO1 (KGE-only), LO1+LO2 (`final`), LO3 (GNN), **Ensemble mit λ-Sweep**\n",
    "- ✅ **Exporte**: Top-K-Listen & Ergebnis-Tabelle\n",
    "\n",
    "> Lege dieses Notebook in `letterboxd-KG/gnn/`. Die Pfade sind relativ zu diesem Ordner.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "319fa3d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:41.783191Z",
     "start_time": "2025-09-12T16:03:40.338608Z"
    }
   },
   "source": [
    "# === Konfiguration & Seeds ===\n",
    "CSV_PATH = \"rerank_by_logical_rules.csv\"\n",
    "LETTERBOXD_DIR = \"../data/letterboxd_export\"\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "# Split-Datum: None -> automatisch 80%-Quantil der positiven Interaktionszeiten\n",
    "SPLIT_DATE = None\n",
    "\n",
    "# Repro\n",
    "SEED = 42\n",
    "TOPK = 10\n",
    "NEG_PER_POS = 3\n",
    "LAMBDA_SWEEP = [0.2, 0.4, 0.6, 0.8]  # Ensemble-Anteile für LO3\n",
    "\n",
    "import os, re, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CSV_PATH:\", CSV_PATH)\n",
    "print(\"LETTERBOXD_DIR:\", LETTERBOXD_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_PATH: ../data/kg/rerank_by_logical_rules.csv\n",
      "LETTERBOXD_DIR: ../data/letterboxd_export\n",
      "OUTPUT_DIR: ../data/kg/outputs\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ca9d1596",
   "metadata": {},
   "source": [
    "## (Optional) Installationen\n",
    "Führe diese Zelle lokal aus, wenn PyTorch/pyG fehlen.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "527f3f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:41.794209Z",
     "start_time": "2025-09-12T16:03:41.792323Z"
    }
   },
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip install torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-$(python -c \"import torch;print(torch.__version__.split('+')[0])\").html\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e32797f6",
   "metadata": {},
   "source": [
    "## 1) Rerank-CSV laden & vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "id": "fef90ca3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:41.850692Z",
     "start_time": "2025-09-12T16:03:41.822224Z"
    }
   },
   "source": [
    "assert Path(CSV_PATH).exists(), f\"CSV nicht gefunden: {CSV_PATH}\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "df.head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 25)\n",
      "['candidate_id', 'candidate_title', 'year', 'cos', 'meta', 'final', 'seed', 'comp_genres', 'comp_keywords', 'comp_cast', 'comp_director', 'comp_runtime', 'comp_language', 'comp_popularity', 'comp_vote', 'name_norm', 'year_str', 'genre_list', 'director_list', 'watchlist_priority', 'genre_boost', 'director_boost', 'genre_penalty', 'director_penalty', 'score']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   candidate_id            candidate_title    year  cos    meta   final  \\\n",
       "0          1420         Breakfast on Pluto  2005.0  0.0  0.4557  0.1823   \n",
       "1        624860   The Matrix Resurrections  2021.0  0.0  0.4696  0.1879   \n",
       "2          1523  The Last King of Scotland  2006.0  0.0  0.4507  0.1803   \n",
       "\n",
       "                 seed  comp_genres  comp_keywords  comp_cast  ...  \\\n",
       "0               Pride       1.0000         0.0769     0.0000  ...   \n",
       "1          The Matrix       0.6667         0.2667     0.0526  ...   \n",
       "2  The Social Network       1.0000         0.0392     0.0000  ...   \n",
       "\n",
       "                   name_norm  year_str  genre_list  director_list  \\\n",
       "0         breakfast on pluto    2005.0          []             []   \n",
       "1   the matrix resurrections    2021.0          []             []   \n",
       "2  the last king of scotland    2006.0          []             []   \n",
       "\n",
       "   watchlist_priority genre_boost  director_boost genre_penalty  \\\n",
       "0                True       False           False         False   \n",
       "1                True       False           False         False   \n",
       "2                True       False           False         False   \n",
       "\n",
       "  director_penalty  score  \n",
       "0            False      2  \n",
       "1            False      2  \n",
       "2            False      2  \n",
       "\n",
       "[3 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_id</th>\n",
       "      <th>candidate_title</th>\n",
       "      <th>year</th>\n",
       "      <th>cos</th>\n",
       "      <th>meta</th>\n",
       "      <th>final</th>\n",
       "      <th>seed</th>\n",
       "      <th>comp_genres</th>\n",
       "      <th>comp_keywords</th>\n",
       "      <th>comp_cast</th>\n",
       "      <th>...</th>\n",
       "      <th>name_norm</th>\n",
       "      <th>year_str</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>director_list</th>\n",
       "      <th>watchlist_priority</th>\n",
       "      <th>genre_boost</th>\n",
       "      <th>director_boost</th>\n",
       "      <th>genre_penalty</th>\n",
       "      <th>director_penalty</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1420</td>\n",
       "      <td>Breakfast on Pluto</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4557</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>Pride</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>breakfast on pluto</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624860</td>\n",
       "      <td>The Matrix Resurrections</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>The Matrix</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>...</td>\n",
       "      <td>the matrix resurrections</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1523</td>\n",
       "      <td>The Last King of Scotland</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4507</td>\n",
       "      <td>0.1803</td>\n",
       "      <td>The Social Network</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>the last king of scotland</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3db28509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:41.928248Z",
     "start_time": "2025-09-12T16:03:41.922804Z"
    }
   },
   "source": [
    "# Numerik säubern\n",
    "num_like = ['cos','final','score','comp_genres','comp_keywords','comp_cast','comp_director',\n",
    "            'comp_runtime','comp_language','comp_popularity','comp_vote']\n",
    "for c in num_like:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "df['seed'] = df['seed'].astype(str)\n",
    "df['candidate_title'] = df['candidate_title'].astype(str)\n",
    "\n",
    "def minmax(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    mn, mx = np.nanmin(x), np.nanmax(x)\n",
    "    if not np.isfinite(mn) or not np.isfinite(mx) or mx<=mn:\n",
    "        return np.zeros_like(x)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "print(\"Zeilen:\", len(df))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeilen: 200\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "f729b413",
   "metadata": {},
   "source": [
    "## 2) Graph aus CSV: Kanten + Gewichte (edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dba0dfae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:42.009613Z",
     "start_time": "2025-09-12T16:03:41.996036Z"
    }
   },
   "source": [
    "movies = pd.unique(pd.concat([df['seed'], df['candidate_title']], ignore_index=True))\n",
    "movie2id = {m:i for i,m in enumerate(movies)}\n",
    "id2movie = {i:m for m,i in movie2id.items()}\n",
    "\n",
    "rel_cols = [c for c in ['cos','final','comp_genres','comp_keywords','comp_cast','comp_director',\n",
    "                        'comp_runtime','comp_language','comp_popularity','comp_vote'] if c in df.columns]\n",
    "\n",
    "# Roh-Edges\n",
    "edges = {c: [] for c in rel_cols}\n",
    "for _, row in df.iterrows():\n",
    "    s = row['seed']; c = row['candidate_title']\n",
    "    if pd.isna(s) or pd.isna(c): \n",
    "        continue\n",
    "    u, v = movie2id[s], movie2id[c]\n",
    "    for rc in rel_cols:\n",
    "        edges[rc].append((u, v, float(row.get(rc, 0.0))))\n",
    "\n",
    "# Normalisierte Gewichte (0..1) je Relation\n",
    "norm_edges = {}\n",
    "for rc, lst in edges.items():\n",
    "    if not lst:\n",
    "        continue\n",
    "    w = np.array([w for (_,_,w) in lst], dtype=float)\n",
    "    wn = minmax(w)\n",
    "    norm_edges[rc] = [(u,v,float(wn[i])) for i,(u,v,_) in enumerate(lst)]\n",
    "\n",
    "for rc, lst in norm_edges.items():\n",
    "    print(rc, \"Edges:\", len(lst))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos Edges: 200\n",
      "final Edges: 200\n",
      "comp_genres Edges: 200\n",
      "comp_keywords Edges: 200\n",
      "comp_cast Edges: 200\n",
      "comp_director Edges: 200\n",
      "comp_runtime Edges: 200\n",
      "comp_language Edges: 200\n",
      "comp_popularity Edges: 200\n",
      "comp_vote Edges: 200\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6967af75",
   "metadata": {},
   "source": [
    "## 3) Letterboxd-Labels (Ratings/Watchlist) + Zeit-Split"
   ]
  },
  {
   "cell_type": "code",
   "id": "8293286b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:42.162027Z",
     "start_time": "2025-09-12T16:03:42.101520Z"
    }
   },
   "source": [
    "def smart_find(base_dir, primary_name_patterns, fallback_exts=('csv', 'CSV')):\n",
    "    base = Path(base_dir)\n",
    "    if not base.exists():\n",
    "        return None\n",
    "    for pat in primary_name_patterns:\n",
    "        for ext in fallback_exts:\n",
    "            cand = base / f\"{pat}.{ext}\"\n",
    "            if cand.exists():\n",
    "                return str(cand)\n",
    "    for p in base.rglob(\"*\"):\n",
    "        name = p.name.lower()\n",
    "        for pat in primary_name_patterns:\n",
    "            if pat.lower() in name and p.suffix.lower() in ('.csv',):\n",
    "                return str(p)\n",
    "    return None\n",
    "\n",
    "ratings_path   = smart_find(LETTERBOXD_DIR, ['ratings', 'ratings-export', 'ratings-2'])\n",
    "watched_path   = smart_find(LETTERBOXD_DIR, ['watched', 'diary'])\n",
    "watchlist_path = smart_find(LETTERBOXD_DIR, ['watchlist'])\n",
    "\n",
    "print(\"ratings_path:\", ratings_path)\n",
    "print(\"watched_path:\", watched_path)\n",
    "print(\"watchlist_path:\", watchlist_path)\n",
    "\n",
    "assert ratings_path or watched_path or watchlist_path, \"Keine Letterboxd-CSV gefunden. Prüfe LETTERBOXD_DIR.\"\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "def normalize_title(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"[^a-z0-9]+\", \" \", t)\n",
    "    t = re.sub(r\"\\b(the|a|an)\\b\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def parse_date_series(s):\n",
    "    if s is None:\n",
    "        return pd.Series(dtype='datetime64[ns]')\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors='coerce', utc=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(s.astype(str), errors='coerce', utc=True)\n",
    "\n",
    "def parse_rating_series(s):\n",
    "    if s is None:\n",
    "        return pd.Series(dtype=float)\n",
    "    def to_num(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            txt = str(x)\n",
    "            stars = txt.count('★')\n",
    "            half  = '½' in txt\n",
    "            return stars + (0.5 if half else 0.0)\n",
    "    return s.apply(to_num).astype(float)\n",
    "\n",
    "frames = []\n",
    "if ratings_path:\n",
    "    r = pd.read_csv(ratings_path)\n",
    "    title = r.get('Name', r.get('Title'))\n",
    "    year  = r.get('Year')\n",
    "    rating= parse_rating_series(r.get('Rating'))\n",
    "    date  = parse_date_series(r.get('Date', r.get('WatchedDate')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': rating, 'date': date, 'watchlist': False}))\n",
    "if watched_path:\n",
    "    w = pd.read_csv(watched_path)\n",
    "    title = w.get('Name', w.get('Title'))\n",
    "    year  = w.get('Year')\n",
    "    rating= parse_rating_series(w.get('Rating'))\n",
    "    date  = parse_date_series(w.get('Date', w.get('WatchedDate')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': rating, 'date': date, 'watchlist': False}))\n",
    "if watchlist_path:\n",
    "    wl = pd.read_csv(watchlist_path)\n",
    "    title = wl.get('Name', wl.get('Title'))\n",
    "    year  = wl.get('Year')\n",
    "    date  = parse_date_series(wl.get('AddedDate', wl.get('Date')))\n",
    "    frames.append(pd.DataFrame({'title': title, 'year': year, 'rating': np.nan, 'date': date, 'watchlist': True}))\n",
    "\n",
    "inter = pd.concat(frames, ignore_index=True).dropna(subset=['title'])\n",
    "inter['title_norm'] = inter['title'].apply(normalize_title)\n",
    "inter['year'] = pd.to_numeric(inter['year'], errors='coerce')\n",
    "inter['date'] = parse_date_series(inter['date'])\n",
    "# positive: rating>=4 oder watchlist\n",
    "inter['is_positive'] = (inter['rating'] >= 4.0) | (inter['watchlist'] == True)\n",
    "\n",
    "# Mapping zu unseren Movies\n",
    "cand_norm = {normalize_title(t): t for t in pd.unique(df['candidate_title'])}\n",
    "seed_norm = {normalize_title(t): t for t in pd.unique(df['seed'])}\n",
    "all_norm  = {**seed_norm, **cand_norm}\n",
    "inter['cand_match'] = inter['title_norm'].map(all_norm)\n",
    "\n",
    "mapped_pos = inter[(inter['is_positive']) & inter['date'].notna() & inter['cand_match'].notna()].copy()\n",
    "assert len(mapped_pos) > 0, \"Keine positiven Interaktionen gemappt. Prüfe Normalisierung/Titel.\"\n",
    "\n",
    "split_date = pd.to_datetime(SPLIT_DATE, utc=True) if SPLIT_DATE else mapped_pos['date'].quantile(0.8)\n",
    "print(\"Split-Datum:\", split_date)\n",
    "\n",
    "train_pos = mapped_pos[mapped_pos['date'] <= split_date]['cand_match'].tolist()\n",
    "test_pos  = mapped_pos[mapped_pos['date'] >  split_date]['cand_match'].tolist()\n",
    "train_pos_ids = [movie2id[t] for t in train_pos if t in movie2id]\n",
    "test_pos_set  = set([t for t in test_pos if t in movie2id])\n",
    "\n",
    "print(\"Train positive IDs:\", len(train_pos_ids), \"| Test positive unique:\", len(test_pos_set))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_path: ../data/letterboxd_export/ratings.csv\n",
      "watched_path: ../data/letterboxd_export/watched.csv\n",
      "watchlist_path: ../data/letterboxd_export/watchlist.csv\n",
      "Split-Datum: 2024-05-22 04:48:00+00:00\n",
      "Train positive IDs: 47 | Test positive unique: 12\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "577c6417",
   "metadata": {},
   "source": [
    "## 4) Eval-Helper (NumPy 2.0 kompatibel)"
   ]
  },
  {
   "cell_type": "code",
   "id": "54a954c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:42.203142Z",
     "start_time": "2025-09-12T16:03:42.197174Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "def _to_float_array(x): return np.asarray(x, dtype=float)\n",
    "\n",
    "def dcg_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    return float(np.sum((np.power(2.0, r) - 1.0) / np.log2(np.arange(2, r.size + 2))))\n",
    "\n",
    "def ndcg_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    ideal = np.sort(r)[::-1]\n",
    "    idcg = dcg_at_k(ideal, k)\n",
    "    return float(dcg / idcg) if idcg > 0 else 0.0\n",
    "\n",
    "def hit_at_k(rel, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    return float(np.any(r > 0))\n",
    "\n",
    "def recall_at_k(rel, total_pos, k=10):\n",
    "    r = _to_float_array(rel)[:k]\n",
    "    found = int(np.sum(r))\n",
    "    return float(found / total_pos) if total_pos > 0 else 0.0\n",
    "\n",
    "def eval_grouped_with_test(df, score_col, k=10, test_pos_set=None):\n",
    "    hits, ndcgs, recalls, cnt = [], [], [], 0\n",
    "    for seed, g in df.groupby('seed', sort=False):\n",
    "        g = g.sort_values(score_col, ascending=False).reset_index(drop=True)\n",
    "        rel = (g['candidate_title'].isin(test_pos_set)).astype(int).to_numpy()\n",
    "        total_pos = int(rel.sum())\n",
    "        hits.append(hit_at_k(rel, k))\n",
    "        ndcgs.append(ndcg_at_k(rel, k))\n",
    "        recalls.append(recall_at_k(rel, total_pos, k))\n",
    "        cnt += 1\n",
    "    return float(np.mean(hits)), float(np.mean(ndcgs)), float(np.mean(recalls)), cnt"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "84509631",
   "metadata": {},
   "source": [
    "## 5) Baselines (LO1 & LO1+LO2) — Eval & Export"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9dbcb5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:42.402840Z",
     "start_time": "2025-09-12T16:03:42.224343Z"
    }
   },
   "source": [
    "# LO1: KGE-only (nutze cos, wenn vorhanden; sonst 'score' falls vorhanden)\n",
    "if 'cos' in df.columns:\n",
    "    df['cos_norm'] = df.groupby('seed')['cos'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "    h, n, r, cnt = eval_grouped_with_test(df, 'cos_norm', k=TOPK, test_pos_set=test_pos_set)\n",
    "    print(f\"LO1 KGE-only (`cos_norm`) — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "    df.sort_values(['seed','cos_norm'], ascending=[True, False]).groupby('seed').head(TOPK).to_csv(f\"{OUTPUT_DIR}/top{TOPK}_lo1_kge.csv\", index=False)\n",
    "elif 'score' in df.columns:\n",
    "    df['score_norm'] = df.groupby('seed')['score'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "    h, n, r, cnt = eval_grouped_with_test(df, 'score_norm', k=TOPK, test_pos_set=test_pos_set)\n",
    "    print(f\"LO1 KGE-only (`score_norm`) — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "    df.sort_values(['seed','score_norm'], ascending=[True, False]).groupby('seed').head(TOPK).to_csv(f\"{OUTPUT_DIR}/top{TOPK}_lo1_kge.csv\", index=False)\n",
    "else:\n",
    "    print(\"Warnung: Keine KGE-Spalte ('cos' oder 'score') gefunden — LO1-Sicht wird übersprungen.\")\n",
    "\n",
    "# LO1+LO2: final\n",
    "if 'final' in df.columns:\n",
    "    df['final_norm'] = df.groupby('seed')['final'].transform(lambda x: (x - x.min()) / (x.max()-x.min() + 1e-9))\n",
    "    h, n, r, cnt = eval_grouped_with_test(df, 'final_norm', k=TOPK, test_pos_set=test_pos_set)\n",
    "    print(f\"LO1+LO2 (`final_norm`) — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "    df.sort_values(['seed','final_norm'], ascending=[True, False]).groupby('seed').head(TOPK).to_csv(f\"{OUTPUT_DIR}/top{TOPK}_lo1_lo2_baseline.csv\", index=False)\n",
    "else:\n",
    "    print(\"Warnung: Keine 'final'-Spalte gefunden — LO1+LO2-Sicht wird übersprungen.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LO1 KGE-only (`cos_norm`) — Hit@10: 0.031 | NDCG@10: 0.031 | Recall@10: 0.031 (Seeds: 130)\n",
      "LO1+LO2 (`final_norm`) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031 (Seeds: 130)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "738d6fe2",
   "metadata": {},
   "source": [
    "## 6) GNN-Training (LO3) mit **edge_attr** (Relationengewichte)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f33c835",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:42.447518Z",
     "start_time": "2025-09-12T16:03:42.443490Z"
    }
   },
   "source": [
    "import importlib, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "has_torch = importlib.util.find_spec('torch') is not None\n",
    "has_pyg   = importlib.util.find_spec('torch_geometric') is not None\n",
    "print(\"Torch installiert:\", has_torch, \"| PyG installiert:\", has_pyg)\n",
    "if not (has_torch and has_pyg):\n",
    "    print(\"GNN-Teil wird übersprungen (Pakete fehlen).\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch installiert: True | PyG installiert: True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e6917c4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:47.679581Z",
     "start_time": "2025-09-12T16:03:42.485665Z"
    }
   },
   "source": [
    "if has_torch and has_pyg:\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch_geometric.data import HeteroData\n",
    "    from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    data = HeteroData()\n",
    "    num_movies = len(movie2id)\n",
    "    d = 64\n",
    "\n",
    "    # movie->movie edges + edge_attr=gewicht (1D)\n",
    "    for rc, lst in norm_edges.items():\n",
    "        if not lst: continue\n",
    "        src = torch.tensor([u for (u,_,_) in lst], dtype=torch.long)\n",
    "        dst = torch.tensor([v for (_,v,_) in lst], dtype=torch.long)\n",
    "        w   = torch.tensor([w for (*_,w) in lst], dtype=torch.float32).view(-1,1)\n",
    "        data['movie', rc, 'movie'].edge_index = torch.stack([src, dst], dim=0)\n",
    "        data['movie', rc, 'movie'].edge_attr  = w\n",
    "\n",
    "    # user->movie positives aus Train\n",
    "    pos_ids = [movie2id[t] for t in train_pos if t in movie2id]\n",
    "    assert len(pos_ids) > 0, \"Keine Trainings-Positives im CSV-Kandidatenraum gefunden.\"\n",
    "    u_src = torch.zeros(len(pos_ids), dtype=torch.long)\n",
    "    m_dst = torch.tensor(pos_ids, dtype=torch.long)\n",
    "    data['user','likes','movie'].edge_index = torch.stack([u_src, m_dst], dim=0)\n",
    "\n",
    "    # Negative Samples\n",
    "    all_movie_ids = torch.arange(num_movies, dtype=torch.long)\n",
    "    pos_set = set(m_dst.tolist())\n",
    "    neg_pool = [int(i) for i in all_movie_ids.tolist() if i not in pos_set]\n",
    "    neg_pairs = [(0, np.random.choice(neg_pool)) for _ in range(len(pos_ids) * NEG_PER_POS)]\n",
    "    un_src = torch.tensor([p[0] for p in neg_pairs], dtype=torch.long)\n",
    "    mn_dst = torch.tensor([p[1] for p in neg_pairs], dtype=torch.long)\n",
    "\n",
    "    # Nur Relationen mit dst='movie'\n",
    "    conv_edge_types = [et for et in data.edge_types if et[2] == 'movie']\n",
    "    edge_index_dict = {et: data[et].edge_index for et in conv_edge_types}\n",
    "    edge_attr_dict  = {et: data[et].edge_attr  for et in conv_edge_types if 'edge_attr' in data[et]}\n",
    "\n",
    "    class HeteroRecommender(nn.Module):\n",
    "        def __init__(self, num_movies, dim=64, layers=2):\n",
    "            super().__init__()\n",
    "            self.movie_emb = nn.Embedding(num_movies, dim)\n",
    "            self.user_emb  = nn.Embedding(1, dim)\n",
    "            self.layers = nn.ModuleList()\n",
    "            for _ in range(layers):\n",
    "                self.layers.append(\n",
    "                    HeteroConv(\n",
    "                        { et: GATv2Conv((-1, -1), dim, edge_dim=1, add_self_loops=False)\n",
    "                          for et in conv_edge_types },\n",
    "                        aggr='sum'\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        def forward(self, edge_index_dict, edge_attr_dict=None):\n",
    "            x = {'movie': self.movie_emb.weight, 'user': self.user_emb.weight}\n",
    "            for conv in self.layers:\n",
    "                if edge_attr_dict:\n",
    "                    out = conv(x, edge_index_dict, edge_attr_dict)\n",
    "                else:\n",
    "                    out = conv(x, edge_index_dict)\n",
    "                out = {k: F.relu(v) for k, v in out.items()}\n",
    "                x.update(out)\n",
    "            return x\n",
    "\n",
    "        @staticmethod\n",
    "        def score(user_vec, item_vec):\n",
    "            return (user_vec * item_vec).sum(dim=-1)\n",
    "\n",
    "    model = HeteroRecommender(num_movies=num_movies, dim=d, layers=2)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1, 201):\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "        x_out = model(edge_index_dict, edge_attr_dict)\n",
    "        user_pos = x_out['user'][u_src]\n",
    "        item_pos = x_out['movie'][m_dst]\n",
    "        user_neg = x_out['user'][un_src]\n",
    "        item_neg = x_out['movie'][mn_dst]\n",
    "        pos_logit = HeteroRecommender.score(user_pos, item_pos)\n",
    "        neg_logit = HeteroRecommender.score(user_neg, item_neg)\n",
    "        loss = bce(pos_logit, torch.ones_like(pos_logit)) + bce(neg_logit, torch.zeros_like(neg_logit))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch:3d} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    # Scoring aller Kandidaten\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_out = model(edge_index_dict, edge_attr_dict)\n",
    "        user_vec = x_out['user'][0:1]\n",
    "\n",
    "    gnn_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        cand = row['candidate_title']\n",
    "        mid = movie2id.get(cand, None)\n",
    "        if mid is None:\n",
    "            gnn_scores.append(np.nan); continue\n",
    "        item_vec = x_out['movie'][mid:mid+1]\n",
    "        s = float((user_vec * item_vec).sum(dim=-1))\n",
    "        gnn_scores.append(s)\n",
    "\n",
    "    df['s_gnn'] = gnn_scores\n",
    "    df['s_gnn_norm'] = df.groupby('seed')['s_gnn'].transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9))\n",
    "    print(\"GNN-Scoring hinzugefügt: 's_gnn'/'s_gnn_norm'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 | Loss 0.0103\n",
      "Epoch 100 | Loss 0.0043\n",
      "Epoch 150 | Loss 0.0025\n",
      "Epoch 200 | Loss 0.0017\n",
      "GNN-Scoring hinzugefügt: 's_gnn'/'s_gnn_norm'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "840d1df3",
   "metadata": {},
   "source": [
    "## 7) Ablation (LO1→LO3) & Ensemble (λ-Sweep) — Eval & Exporte"
   ]
  },
  {
   "cell_type": "code",
   "id": "28c5c786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:48.119962Z",
     "start_time": "2025-09-12T16:03:47.780330Z"
    }
   },
   "source": [
    "results = []\n",
    "\n",
    "def eval_and_log(name, col):\n",
    "    if col not in df.columns: \n",
    "        print(f\"[SKIP] {name}: Spalte '{col}' fehlt.\")\n",
    "        return\n",
    "    h, n, r, cnt = eval_grouped_with_test(df, col, k=TOPK, test_pos_set=test_pos_set)\n",
    "    results.append({'variant': name, 'metric': f'@{TOPK}', 'hit': h, 'ndcg': n, 'recall': r, 'seeds': cnt})\n",
    "    print(f\"{name:>22} — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f} (Seeds: {cnt})\")\n",
    "\n",
    "# LO1\n",
    "if 'cos_norm' in df.columns: eval_and_log(\"LO1 KGE-only (cos)\", 'cos_norm')\n",
    "elif 'score_norm' in df.columns: eval_and_log(\"LO1 KGE-only (score)\", 'score_norm')\n",
    "\n",
    "# LO1+LO2\n",
    "eval_and_log(\"LO1+LO2 (final)\", 'final_norm')\n",
    "\n",
    "# LO3 (GNN)\n",
    "eval_and_log(\"LO3 (GNN)\", 's_gnn_norm')\n",
    "\n",
    "# Ensemble Sweep\n",
    "best = None\n",
    "for lam in LAMBDA_SWEEP:\n",
    "    col = f\"ensemble_{lam:.1f}\"\n",
    "    if 's_gnn_norm' in df.columns and 'final_norm' in df.columns:\n",
    "        df[col] = lam * df['s_gnn_norm'] + (1.0 - lam) * df['final_norm']\n",
    "        h, n, r, cnt = eval_grouped_with_test(df, col, k=TOPK, test_pos_set=test_pos_set)\n",
    "        results.append({'variant': f'Ensemble λ={lam:.1f}', 'metric': f'@{TOPK}', 'hit': h, 'ndcg': n, 'recall': r, 'seeds': cnt})\n",
    "        if best is None or n > best['ndcg']:\n",
    "            best = {'lam': lam, 'hit': h, 'ndcg': n, 'recall': r}\n",
    "        print(f\"Ensemble (λ={lam:.1f}) — Hit@{TOPK}: {h:.3f} | NDCG@{TOPK}: {n:.3f} | Recall@{TOPK}: {r:.3f}\")\n",
    "\n",
    "# Ergebnisse speichern\n",
    "res_df = pd.DataFrame(results)\n",
    "res_path = f\"{OUTPUT_DIR}/ablation_results.csv\"\n",
    "res_df.to_csv(res_path, index=False)\n",
    "print(\"Ablation-Ergebnisse:\", res_path)\n",
    "if best:\n",
    "    print(f\"Bestes Ensemble (nach NDCG): λ={best['lam']:.1f} | Hit@{TOPK}={best['hit']:.3f} | NDCG@{TOPK}={best['ndcg']:.3f} | Recall@{TOPK}={best['recall']:.3f}\")\n",
    "\n",
    "# Top-K-Exporte\n",
    "def export_topk(df, col, fname):\n",
    "    if col not in df.columns: return None\n",
    "    out = df.sort_values(['seed', col], ascending=[True, False]).groupby('seed').head(TOPK).copy()\n",
    "    out['test_relevant'] = out['candidate_title'].isin(test_pos_set)\n",
    "    path = f\"{OUTPUT_DIR}/{fname}\"\n",
    "    out.to_csv(path, index=False); return path\n",
    "\n",
    "exports = []\n",
    "if 'cos_norm' in df.columns: exports.append(export_topk(df, 'cos_norm',     f\"top{TOPK}_lo1_kge.csv\"))\n",
    "if 'final_norm' in df.columns: exports.append(export_topk(df, 'final_norm', f\"top{TOPK}_lo1_lo2_baseline.csv\"))\n",
    "if 's_gnn_norm' in df.columns: exports.append(export_topk(df, 's_gnn_norm', f\"top{TOPK}_lo3_gnn.csv\"))\n",
    "for lam in LAMBDA_SWEEP:\n",
    "    col = f\"ensemble_{lam:.1f}\"\n",
    "    if col in df.columns:\n",
    "        exports.append(export_topk(df, col, f\"top{TOPK}_ensemble_lam{lam:.1f}.csv\"))\n",
    "\n",
    "print(\"Top-K Exporte:\")\n",
    "for p in exports:\n",
    "    if p: print(\" -\", p)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    LO1 KGE-only (cos) — Hit@10: 0.031 | NDCG@10: 0.031 | Recall@10: 0.031 (Seeds: 130)\n",
      "       LO1+LO2 (final) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031 (Seeds: 130)\n",
      "             LO3 (GNN) — Hit@10: 0.031 | NDCG@10: 0.031 | Recall@10: 0.031 (Seeds: 130)\n",
      "Ensemble (λ=0.2) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031\n",
      "Ensemble (λ=0.4) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031\n",
      "Ensemble (λ=0.6) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031\n",
      "Ensemble (λ=0.8) — Hit@10: 0.031 | NDCG@10: 0.023 | Recall@10: 0.031\n",
      "Ablation-Ergebnisse: ../data/kg/outputs/ablation_results.csv\n",
      "Bestes Ensemble (nach NDCG): λ=0.2 | Hit@10=0.031 | NDCG@10=0.023 | Recall@10=0.031\n",
      "Top-K Exporte:\n",
      " - ../data/kg/outputs/top10_lo1_kge.csv\n",
      " - ../data/kg/outputs/top10_lo1_lo2_baseline.csv\n",
      " - ../data/kg/outputs/top10_lo3_gnn.csv\n",
      " - ../data/kg/outputs/top10_ensemble_lam0.2.csv\n",
      " - ../data/kg/outputs/top10_ensemble_lam0.4.csv\n",
      " - ../data/kg/outputs/top10_ensemble_lam0.6.csv\n",
      " - ../data/kg/outputs/top10_ensemble_lam0.8.csv\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "8912d832",
   "metadata": {},
   "source": [
    "## 8) Qualitative Fallstudie (2 Seeds) — Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "id": "a46c734a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-12T16:03:48.160208Z",
     "start_time": "2025-09-12T16:03:48.129850Z"
    }
   },
   "source": [
    "seeds_with_pos = []\n",
    "for seed, g in df.groupby('seed'):\n",
    "    if (g['candidate_title'].isin(test_pos_set)).any():\n",
    "        seeds_with_pos.append(seed)\n",
    "sample_seeds = seeds_with_pos[:2] if len(seeds_with_pos)>=2 else df['seed'].unique()[:2]\n",
    "\n",
    "def show_top(g, col, name):\n",
    "    if col in g.columns:\n",
    "        top = g.sort_values(col, ascending=False).head(5)[['candidate_title', col]].copy()\n",
    "        top['is_test_rel'] = top['candidate_title'].isin(test_pos_set)\n",
    "        print(f\"{name} Top-5:\")\n",
    "        print(top.to_string(index=False))\n",
    "\n",
    "for s in sample_seeds:\n",
    "    print(\"\\n=== Seed:\", s, \"===\")\n",
    "    g = df[df['seed'] == s].copy()\n",
    "    show_top(g, 'cos_norm', 'LO1 KGE')\n",
    "    show_top(g, 'final_norm', 'LO1+LO2 Final')\n",
    "    show_top(g, 's_gnn_norm', 'LO3 GNN')\n",
    "    for lam in LAMBDA_SWEEP:\n",
    "        show_top(g, f'ensemble_{lam:.1f}', f'Ensemble λ={lam:.1f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Seed: Godzilla ===\n",
      "LO1 KGE Top-5:\n",
      "          candidate_title  cos_norm  is_test_rel\n",
      "The War of the Gargantuas       0.0         True\n",
      "LO1+LO2 Final Top-5:\n",
      "          candidate_title  final_norm  is_test_rel\n",
      "The War of the Gargantuas         0.0         True\n",
      "LO3 GNN Top-5:\n",
      "          candidate_title  s_gnn_norm  is_test_rel\n",
      "The War of the Gargantuas         0.0         True\n",
      "Ensemble λ=0.2 Top-5:\n",
      "          candidate_title  ensemble_0.2  is_test_rel\n",
      "The War of the Gargantuas           0.0         True\n",
      "Ensemble λ=0.4 Top-5:\n",
      "          candidate_title  ensemble_0.4  is_test_rel\n",
      "The War of the Gargantuas           0.0         True\n",
      "Ensemble λ=0.6 Top-5:\n",
      "          candidate_title  ensemble_0.6  is_test_rel\n",
      "The War of the Gargantuas           0.0         True\n",
      "Ensemble λ=0.8 Top-5:\n",
      "          candidate_title  ensemble_0.8  is_test_rel\n",
      "The War of the Gargantuas           0.0         True\n",
      "\n",
      "=== Seed: Good Time ===\n",
      "LO1 KGE Top-5:\n",
      " candidate_title  cos_norm  is_test_rel\n",
      "           Bound       0.0         True\n",
      "We Own the Night       0.0        False\n",
      "         Chopper       0.0        False\n",
      "LO1+LO2 Final Top-5:\n",
      " candidate_title  final_norm  is_test_rel\n",
      "We Own the Night    1.000000        False\n",
      "         Chopper    0.133333        False\n",
      "           Bound    0.000000         True\n",
      "LO3 GNN Top-5:\n",
      " candidate_title  s_gnn_norm  is_test_rel\n",
      "           Bound         0.0         True\n",
      "We Own the Night         0.0        False\n",
      "         Chopper         0.0        False\n",
      "Ensemble λ=0.2 Top-5:\n",
      " candidate_title  ensemble_0.2  is_test_rel\n",
      "We Own the Night      0.800000        False\n",
      "         Chopper      0.106667        False\n",
      "           Bound      0.000000         True\n",
      "Ensemble λ=0.4 Top-5:\n",
      " candidate_title  ensemble_0.4  is_test_rel\n",
      "We Own the Night          0.60        False\n",
      "         Chopper          0.08        False\n",
      "           Bound          0.00         True\n",
      "Ensemble λ=0.6 Top-5:\n",
      " candidate_title  ensemble_0.6  is_test_rel\n",
      "We Own the Night      0.400000        False\n",
      "         Chopper      0.053333        False\n",
      "           Bound      0.000000         True\n",
      "Ensemble λ=0.8 Top-5:\n",
      " candidate_title  ensemble_0.8  is_test_rel\n",
      "We Own the Night      0.200000        False\n",
      "         Chopper      0.026667        False\n",
      "           Bound      0.000000         True\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
