{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc7dd7d",
   "metadata": {},
   "source": [
    "# Film-KG — TSV-only Pipeline + Rekursive Traversierung\n",
    "\n",
    "Dieses Notebook:\n",
    "1) Lädt deine `movie_kg_triples.tsv`.\n",
    "2) **Backup** der TSV-Datei anlegen.\n",
    "3) Vorverarbeitung: kanonisierte Character-URIs `ex:char/<slug>` + `ex:featuresCharacter` **ins TSV anhängen** (und in den In-Memory-Graph einfügen).\n",
    "4) Ableitung:\n",
    "   - `ex:SAME_UNIVERSE` (Self-Join über Character) **nur TSV-Append**\n",
    "   - `ex:CREATIVE_PAIR` (Director×Actor ≥2) **nur TSV-Append**\n",
    "5) **Rekursive Traversierung** über `ex:SAME_UNIVERSE` (SPARQL Property Path **und** Python-BFS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bb1095",
   "metadata": {},
   "source": [
    "## 0) Setup & Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a262731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:01.605103Z",
     "start_time": "2025-09-16T21:18:01.197758Z"
    }
   },
   "source": [
    "!python -c \"import rdflib\" 2>/dev/null || pip -q install rdflib==7.0.0\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import shutil, re\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_PATH = Path(\"../data/kg/triples/movie_kg_triples.tsv\")  # bestehende KG-Datei (TSV)\n",
    "OUT_DIR = Path(\"../data/kg/triples\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BATCH_SIZE = 50000  # Tripel je Ausgabedatei\n",
    "\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "EX     = Namespace(\"http://example.org/\")\n",
    "CHAR_NS = Namespace(str(EX) + \"char/\")\n",
    "\n",
    "print(\"Data:\", DATA_PATH.resolve())\n",
    "print(\"Output:\", OUT_DIR.resolve())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: /Users/tschaffel/PycharmProjects/letterboxd-KG/data/kg/triples/movie_kg_triples.tsv\n",
      "Output: /Users/tschaffel/PycharmProjects/letterboxd-KG/data/kg/triples\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "bf21de53",
   "metadata": {},
   "source": [
    "## 1) Daten laden (robuster TSV-Parser)"
   ]
  },
  {
   "cell_type": "code",
   "id": "106d7142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:05.963153Z",
     "start_time": "2025-09-16T21:18:04.868519Z"
    }
   },
   "source": [
    "g = Graph(); g.bind(\"schema\", SCHEMA); g.bind(\"ex\", EX); g.bind(\"rdf\", RDF)\n",
    "prefix_map = {\"schema\": str(SCHEMA), \"rdf\": str(RDF), \"rdfs\": str(RDFS), \"xsd\": str(XSD), \"ex\": str(EX)}\n",
    "\n",
    "def parse_term(term: str):\n",
    "    term = term.strip()\n",
    "    if len(term) >= 2 and term[0] == '\"' and term[-1] == '\"':\n",
    "        return Literal(term[1:-1])\n",
    "    if term.startswith(\"http://\") or term.startswith(\"https://\"):\n",
    "        return URIRef(term)\n",
    "    if \":\" in term:\n",
    "        pfx, local = term.split(\":\", 1)\n",
    "        if pfx in prefix_map:\n",
    "            return URIRef(prefix_map[pfx] + local)\n",
    "    return Literal(term)\n",
    "\n",
    "count = 0\n",
    "with DATA_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue\n",
    "        s, p, o = map(parse_term, parts)\n",
    "        g.add((s, p, o))\n",
    "        count += 1\n",
    "print(\"Geladene Tripel:\", count)\n",
    "print(\"Beispiel-Tripel:\")\n",
    "for i, (s,p,o) in enumerate(g):\n",
    "    print(\"-\", s, p, o)\n",
    "    if i >= 4: break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geladene Tripel: 87251\n",
      "Beispiel-Tripel:\n",
      "- movie9397 http://schema.org/actor person175533\n",
      "- person2512504 http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://schema.org/Person\n",
      "- movie58151 http://schema.org/duration runtime106\n",
      "- movie581997 http://schema.org/name Batman vs Teenage Mutant Ninja Turtles\n",
      "- movie7191 http://schema.org/actor person6858\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "ec92e839",
   "metadata": {},
   "source": [
    "## 2) Backup & TSV-Append-Helfer"
   ]
  },
  {
   "cell_type": "code",
   "id": "229bad2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:13.893520Z",
     "start_time": "2025-09-16T21:18:13.886283Z"
    }
   },
   "source": [
    "BACKUP_DIR = OUT_DIR / \"backups\"; BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BACKUP_PATH = BACKUP_DIR / f\"movie_kg_triples_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.tsv\"\n",
    "\n",
    "# Backup anlegen\n",
    "shutil.copy2(DATA_PATH, BACKUP_PATH)\n",
    "print(\"Backup gespeichert:\", BACKUP_PATH)\n",
    "\n",
    "def term_to_str(term):\n",
    "    if isinstance(term, URIRef):\n",
    "        # Versuche Prefix-Kurzform\n",
    "        for pfx, ns in prefix_map.items():\n",
    "            if str(term).startswith(ns):\n",
    "                return f\"{pfx}:{str(term)[len(ns):]}\"\n",
    "        return str(term)\n",
    "    elif isinstance(term, Literal):\n",
    "        s = str(term).replace('\"', '\\\"')\n",
    "        return f'\"{s}\"'\n",
    "    else:\n",
    "        return str(term)\n",
    "\n",
    "def append_triples_tsv(triples):\n",
    "    with DATA_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for (s,p,o) in triples:\n",
    "            f.write(term_to_str(s) + \"\\t\" + term_to_str(p) + \"\\t\" + term_to_str(o) + \"\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup gespeichert: ../data/kg/triples/backups/movie_kg_triples_backup_20250916_231813.tsv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "e4fd8ed2",
   "metadata": {},
   "source": [
    "## 3) Vorverarbeitung: Character-URIs + `ex:featuresCharacter` (TSV-only)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4db2a6cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:20.745336Z",
     "start_time": "2025-09-16T21:18:20.323547Z"
    }
   },
   "source": [
    "canon_re = re.compile(r\"\\s*\\([^)]*\\)\")\n",
    "def canonize(text: str) -> str:\n",
    "    s = canon_re.sub(\"\", text)\n",
    "    return s.strip().lower()\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    t = re.sub(r\"[^a-z0-9]+\", \"-\", text)\n",
    "    t = re.sub(r\"-+\", \"-\", t).strip(\"-\")\n",
    "    return t or \"x\"\n",
    "\n",
    "char_uri_by_canon = {}\n",
    "added_nodes = 0; added_edges = 0\n",
    "\n",
    "new_feature_triples = []  # für optionales Mitschreiben in TSV\n",
    "\n",
    "for f, _, ch in g.triples((None, SCHEMA.character, None)):\n",
    "    c = canonize(str(ch))\n",
    "    if not c:\n",
    "        continue\n",
    "    uri = char_uri_by_canon.get(c)\n",
    "    if uri is None:\n",
    "        uri = URIRef(CHAR_NS + slugify(c))\n",
    "        char_uri_by_canon[c] = uri\n",
    "        if (uri, RDF.type, EX.Character) not in g:\n",
    "            g.add((uri, RDF.type, EX.Character))\n",
    "            g.add((uri, EX.canonName, Literal(c)))\n",
    "            new_feature_triples.append((uri, RDF.type, EX.Character))\n",
    "            new_feature_triples.append((uri, EX.canonName, Literal(c)))\n",
    "            added_nodes += 1\n",
    "    if (f, EX.featuresCharacter, uri) not in g:\n",
    "        g.add((f, EX.featuresCharacter, uri))\n",
    "        new_feature_triples.append((f, EX.featuresCharacter, uri))\n",
    "        added_edges += 1\n",
    "\n",
    "print(\"Neue Character-Knoten:\", added_nodes)\n",
    "print(\"Neue featuresCharacter-Kanten:\", added_edges)\n",
    "\n",
    "# Optional: diese neuen Vorverarbeitungs-Tripel direkt ins TSV anhängen\n",
    "#if new_feature_triples:\n",
    "#    append_triples_tsv(new_feature_triples)\n",
    "#    print(\"Vorverarbeitungs-Tripel ins TSV angehängt:\", len(new_feature_triples))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neue Character-Knoten: 5392\n",
      "Neue featuresCharacter-Kanten: 6645\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "d490e560",
   "metadata": {},
   "source": [
    "## 4) SAME_UNIVERSE (Self-Join über Character) — **TSV-only**"
   ]
  },
  {
   "cell_type": "code",
   "id": "e077c560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:24.212190Z",
     "start_time": "2025-09-16T21:18:24.151011Z"
    }
   },
   "source": [
    "from itertools import combinations\n",
    "films_by_char = defaultdict(list)\n",
    "for f, _, c in g.triples((None, EX.featuresCharacter, None)):\n",
    "    films_by_char[c].append(f)\n",
    "\n",
    "new_su_triples = []\n",
    "seen = set()\n",
    "for char_uri, films in films_by_char.items():\n",
    "    if len(films) < 2:\n",
    "        continue\n",
    "    films_sorted = sorted(set(films), key=str)\n",
    "    for f1, f2 in combinations(films_sorted, 2):\n",
    "        key = (str(f1), str(f2))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        triple = (f1, EX.sameUniverse, f2)\n",
    "        new_su_triples.append(triple)\n",
    "        g.add(triple)  # auch in-memory für Traversierung\n",
    "\n",
    "append_triples_tsv(new_su_triples)\n",
    "print(\"SAME_UNIVERSE Tripel angehängt:\", len(new_su_triples))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAME_UNIVERSE Tripel angehängt: 1576\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a9c34eb4",
   "metadata": {},
   "source": [
    "## 5) CREATIVE_PAIR (Director×Actor ≥2) — **TSV-only**"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1a5183a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T21:18:29.559735Z",
     "start_time": "2025-09-16T21:18:29.448858Z"
    }
   },
   "source": [
    "pair_counts = defaultdict(int)\n",
    "for f, _, d in g.triples((None, SCHEMA.director, None)):\n",
    "    for _, _, a in g.triples((f, SCHEMA.actor, None)):\n",
    "        pair_counts[(d, a)] += 1\n",
    "\n",
    "new_cp_triples = []\n",
    "for (d, a), n in pair_counts.items():\n",
    "    if n >= 2:\n",
    "        new_cp_triples.append((d, EX.creativePair, a))\n",
    "        new_cp_triples.append((d, EX.creativePairRoles, Literal(\"Director,Actor\")))\n",
    "        new_cp_triples.append((d, EX.creativePairCount, Literal(n)))\n",
    "        g.add((d, EX.creativePair, a))\n",
    "        g.add((d, EX.creativePairRoles, Literal(\"Director,Actor\")))\n",
    "        g.add((d, EX.creativePairCount, Literal(n)))\n",
    "\n",
    "append_triples_tsv(new_cp_triples)\n",
    "print(\"CREATIVE_PAIR Tripel angehängt:\", len(new_cp_triples))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATIVE_PAIR Tripel angehängt: 2763\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "fdf6e52b",
   "metadata": {},
   "source": "## 6) Rekursive Traversierung über `ex:SAME_UNIVERSE`\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-16T23:39:24.452761Z",
     "start_time": "2025-09-16T23:39:24.429741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import URIRef, Literal\n",
    "\n",
    "def _resolve_seed(seed_str: str):\n",
    "    s = seed_str.strip()\n",
    "    if s.startswith(\"http://\") or s.startswith(\"https://\"):\n",
    "        term = URIRef(s)\n",
    "        return term, f\"<{term}>\"\n",
    "    uri = URIRef(str(EX) + s)\n",
    "    if (uri, None, None) in g or (None, None, uri) in g:\n",
    "        return uri, f\"<{uri}>\"\n",
    "    lit = Literal(s)\n",
    "    return lit, f\"\\\"{lit}\\\"\"\n",
    "\n",
    "def sparql_same_universe_bidir(seed_str: str, limit: int = 500, include_seed: bool = False):\n",
    "    seed_term, sparql_seed = _resolve_seed(seed_str)\n",
    "    q = (\n",
    "            \"PREFIX ex: <http://example.org/>\\n\"\n",
    "            \"SELECT DISTINCT ?g WHERE {\\n\"\n",
    "            f\"  VALUES ?seed {{ {sparql_seed} }}\\n\"\n",
    "            \"  ?seed (ex:sameUniverse | ^ex:sameUniverse)+ ?g .\\n\"\n",
    "            \"} LIMIT \" + str(limit)\n",
    "    )\n",
    "    rows = [row[0] for row in g.query(q)]\n",
    "    if not include_seed:\n",
    "        rows = [uri for uri in rows if uri != seed_term]\n",
    "    return [str(u) for u in rows]\n",
    "\n",
    "res = sparql_same_universe_bidir(\"movie452522\", limit=1000, include_seed=False)\n",
    "print(len(res), \"Filme im selben Universe\")\n",
    "res[:10]\n"
   ],
   "id": "56d9db46c09a358",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Filme im selben Universe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['movie1923']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7) Strong pair",
   "id": "aec4ffd31cb4997f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T14:26:21.837956Z",
     "start_time": "2025-09-20T14:26:21.515067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Rule 3: Strong Cast Pair (SQL version; Movie -> Actor direction fixed) ---\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "TRIPLES_TSV       = \"../data/kg/triples/movie_kg_triples.tsv\"  # path to your triples\n",
    "CAST_PREDICATE    = \"schema:actor\"              # <-- set your movie->actor predicate here\n",
    "MIN_COAPPS        = 3                       # threshold for a \"strong\" pair\n",
    "MAKE_SYMMETRIC    = True                    # also emit B->A for strongCastWith\n",
    "OUT_STRONG_PAIRS  = \"../data/kg/triples/derived_strong_cast_pairs.tsv\"\n",
    "OUT_FEATURE_ANN   = \"../data/kg/triples/derived_features_strong_cast.tsv\"\n",
    "# ==================================================\n",
    "\n",
    "# ---------- Robust load of triples -> (subject, predicate, object) -------------\n",
    "def load_triples(path):\n",
    "    # Try TSV with header; if not, fallback strategies\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\",\", dtype=\"string\")\n",
    "\n",
    "    cols = [c.strip().lower() for c in df.columns.tolist()]\n",
    "    # common header variants\n",
    "    mapping_opts = [\n",
    "        {\"subject\":\"subject\",\"predicate\":\"predicate\",\"object\":\"object\"},\n",
    "        {\"s\":\"subject\",\"p\":\"predicate\",\"o\":\"object\"},\n",
    "        {\"head\":\"subject\",\"relation\":\"predicate\",\"tail\":\"object\"},\n",
    "        {\"h\":\"subject\",\"r\":\"predicate\",\"t\":\"object\"},\n",
    "        {\"source\":\"subject\",\"relation\":\"predicate\",\"target\":\"object\"},\n",
    "        {\"from\":\"subject\",\"edge\":\"predicate\",\"to\":\"object\"},\n",
    "    ]\n",
    "    applied = None\n",
    "    for cand in mapping_opts:\n",
    "        if set(cand.keys()).issubset(set(cols)):\n",
    "            applied = {}\n",
    "            for k,v in cand.items():\n",
    "                # find real column name as present\n",
    "                real = df.columns[cols.index(k)]\n",
    "                applied[real] = v\n",
    "            break\n",
    "\n",
    "    if applied:\n",
    "        df = df.rename(columns=applied)\n",
    "    else:\n",
    "        # if no recognizable headers, assume first three columns are s/p/o\n",
    "        if df.shape[1] < 3:\n",
    "            # final fallback: read as no-header\n",
    "            df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                df = pd.read_csv(path, sep=\",\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                raise ValueError(\"Could not detect three columns for triples.\")\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"subject\",\"predicate\",\"object\"]\n",
    "\n",
    "    # Canonicalize & drop NA\n",
    "    df = df[[\"subject\",\"predicate\",\"object\"]].astype(\"string\").dropna()\n",
    "    return df\n",
    "\n",
    "trip = load_triples(TRIPLES_TSV)\n",
    "\n",
    "# ---------- Filter to Movie -> Actor predicate (direction fixed) ---------------\n",
    "cast = (\n",
    "    trip.loc[trip[\"predicate\"] == CAST_PREDICATE, [\"subject\",\"object\"]]\n",
    "    .dropna()\n",
    "    .rename(columns={\"subject\":\"movie\",\"object\":\"actor\"})\n",
    "    .drop_duplicates()\n",
    "    .astype(\"string\")\n",
    ")\n",
    "\n",
    "if cast.empty:\n",
    "    raise ValueError(f\"No triples found with predicate '{CAST_PREDICATE}'. \"\n",
    "                     f\"Check CAST_PREDICATE or your triples file.\")\n",
    "\n",
    "# Optional: strip angle brackets if your IRIs are like <...>\n",
    "# cast[\"movie\"] = cast[\"movie\"].str.replace(r\"^<|>$\", \"\", regex=True)\n",
    "# cast[\"actor\"] = cast[\"actor\"].str.replace(r\"^<|>$\", \"\", regex=True)\n",
    "\n",
    "# ---------- SQL pipeline in SQLite --------------------------------------------\n",
    "con = sqlite3.connect(\":memory:\")\n",
    "cast.to_sql(\"cast\", con, index=False, if_exists=\"replace\")\n",
    "\n",
    "# 1) All unordered actor pairs within each movie (actorA < actorB to canonicalize)\n",
    "con.execute(\"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS idx_cast_movie ON cast(movie);\n",
    "            \"\"\")\n",
    "\n",
    "# 2) Co-appearance counts across all movies\n",
    "#    - Build pairs per movie via self-join\n",
    "#    - Count distinct movies per pair\n",
    "pair_counts_sql = f\"\"\"\n",
    "WITH pairs AS (\n",
    "  SELECT c1.movie AS movie,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "  FROM cast c1\n",
    "  JOIN cast c2\n",
    "    ON c1.movie = c2.movie\n",
    "   AND c1.actor < c2.actor            -- ensures unordered pair once per movie\n",
    ")\n",
    "SELECT actorA, actorB, COUNT(DISTINCT movie) AS coapps\n",
    "FROM pairs\n",
    "GROUP BY actorA, actorB\n",
    "HAVING coapps >= {int(MIN_COAPPS)};\n",
    "\"\"\"\n",
    "\n",
    "df_strong = pd.read_sql_query(pair_counts_sql, con)\n",
    "\n",
    "# 3) Emit strongCastWith triples (optionally symmetric)\n",
    "if df_strong.empty:\n",
    "    trip_strong_pairs = pd.DataFrame(columns=[\"subject\",\"predicate\",\"object\"])\n",
    "else:\n",
    "    base = pd.DataFrame({\n",
    "        \"subject\": df_strong[\"actorA\"],\n",
    "        \"predicate\": \"strongCastWith\",\n",
    "        \"object\": df_strong[\"actorB\"]\n",
    "    })\n",
    "    if MAKE_SYMMETRIC:\n",
    "        sym = pd.DataFrame({\n",
    "            \"subject\": df_strong[\"actorB\"],\n",
    "            \"predicate\": \"strongCastWith\",\n",
    "            \"object\": df_strong[\"actorA\"]\n",
    "        })\n",
    "        trip_strong_pairs = pd.concat([base, sym], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        trip_strong_pairs = base\n",
    "\n",
    "trip_strong_pairs.to_csv(OUT_STRONG_PAIRS, sep=\"\\t\", index=False)\n",
    "\n",
    "# 4) Annotate movies that feature at least one strong pair\n",
    "#    Reuse the CTE of strong pairs and join with within-movie pairs\n",
    "feature_sql = f\"\"\"\n",
    "WITH pairs AS (\n",
    "  SELECT c1.movie AS movie,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "  FROM cast c1\n",
    "  JOIN cast c2\n",
    "    ON c1.movie = c2.movie\n",
    "   AND c1.actor < c2.actor\n",
    "),\n",
    "strong AS (\n",
    "  SELECT actorA, actorB\n",
    "  FROM (\n",
    "    SELECT CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "           CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB,\n",
    "           COUNT(DISTINCT c1.movie) AS coapps\n",
    "    FROM cast c1\n",
    "    JOIN cast c2\n",
    "      ON c1.movie = c2.movie\n",
    "     AND c1.actor < c2.actor\n",
    "    GROUP BY actorA, actorB\n",
    "  )\n",
    "  WHERE coapps >= {int(MIN_COAPPS)}\n",
    ")\n",
    "SELECT p.movie, p.actorA, p.actorB\n",
    "FROM pairs p\n",
    "JOIN strong s\n",
    "  ON p.actorA = s.actorA AND p.actorB = s.actorB;\n",
    "\"\"\"\n",
    "\n",
    "df_feats = pd.read_sql_query(feature_sql, con)\n",
    "\n",
    "if df_feats.empty:\n",
    "    trip_features = pd.DataFrame(columns=[\"subject\",\"predicate\",\"object\"])\n",
    "else:\n",
    "    trip_features = pd.DataFrame({\n",
    "        \"subject\": df_feats[\"movie\"],\n",
    "        \"predicate\": \"featuresStrongCastPair\",\n",
    "        \"object\": (df_feats[\"actorA\"] + \"::\" + df_feats[\"actorB\"])\n",
    "    }).drop_duplicates()\n",
    "\n",
    "trip_features.to_csv(OUT_FEATURE_ANN, sep=\"\\t\", index=False)\n",
    "\n",
    "con.close()\n",
    "\n",
    "# ---------- Report -------------------------------------------------------------\n",
    "print(\"✅ Strong cast (SQL) rule applied.\")\n",
    "print(f\"Cast edges (Movie -> Actor) loaded: {len(cast):,}\")\n",
    "print(f\"Strong pairs (canonical, coapps ≥ {MIN_COAPPS}): {len(df_strong):,}\")\n",
    "print(f\"Wrote: {OUT_STRONG_PAIRS}  ({len(trip_strong_pairs):,} triples)\")\n",
    "print(f\"Wrote: {OUT_FEATURE_ANN}   ({len(trip_features):,} triples)\")\n"
   ],
   "id": "b68825095f1e4b10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Strong cast (SQL) rule applied.\n",
      "Cast edges (Movie -> Actor) loaded: 12,586\n",
      "Strong pairs (canonical, coapps ≥ 3): 674\n",
      "Wrote: ../data/kg/triples/derived_strong_cast_pairs.tsv  (1,348 triples)\n",
      "Wrote: ../data/kg/triples/derived_features_strong_cast.tsv   (2,445 triples)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T14:39:24.362928Z",
     "start_time": "2025-09-20T14:39:24.039719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Rule 3 (SQL) with prefixed preds + castPairIDs ---------------------------\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "TRIPLES_TSV         = \"../data/kg/triples/movie_kg_triples.tsv\"   # path to your triples\n",
    "CAST_PREDICATE      = \"schema:actor\"               # movie -> actor predicate in your KG\n",
    "MIN_COAPPS          = 3                        # threshold for strong pair\n",
    "MAKE_SYMMETRIC      = True                     # also write B->A for strongCastWith\n",
    "PRED_STRONG         = \"ex:strongCastWith\"\n",
    "PRED_FEATURES       = \"ex:featuresStrongCastPair\"\n",
    "PRED_CASTPAIR_ACTOR = \"schema:actor\"\n",
    "\n",
    "OUT_STRONG_PAIRS    = \"../data/kg/triples/derived_strong_cast_pairs.tsv\"\n",
    "OUT_FEATURE_ANN     = \"../data/kg/triples/derived_features_strong_cast.tsv\"\n",
    "OUT_CASTPAIR_NODES  = \"../data/kg/triples/derived_castpair_nodes.tsv\"\n",
    "# ==================================================\n",
    "\n",
    "# ---------- Robust load of triples -> (subject, predicate, object) -------------\n",
    "def load_triples(path):\n",
    "    # Try TSV, fallback to CSV\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\",\", dtype=\"string\")\n",
    "\n",
    "    cols = [c.strip().lower() for c in df.columns.tolist()]\n",
    "    mapping_opts = [\n",
    "        {\"subject\":\"subject\",\"predicate\":\"predicate\",\"object\":\"object\"},\n",
    "        {\"s\":\"subject\",\"p\":\"predicate\",\"o\":\"object\"},\n",
    "        {\"head\":\"subject\",\"relation\":\"predicate\",\"tail\":\"object\"},\n",
    "        {\"h\":\"subject\",\"r\":\"predicate\",\"t\":\"object\"},\n",
    "        {\"source\":\"subject\",\"relation\":\"predicate\",\"target\":\"object\"},\n",
    "        {\"from\":\"subject\",\"edge\":\"predicate\",\"to\":\"object\"},\n",
    "    ]\n",
    "    applied = None\n",
    "    for cand in mapping_opts:\n",
    "        if set(cand.keys()).issubset(set(cols)):\n",
    "            applied = {}\n",
    "            for k,v in cand.items():\n",
    "                real = df.columns[cols.index(k)]\n",
    "                applied[real] = v\n",
    "            break\n",
    "    if applied:\n",
    "        df = df.rename(columns=applied)\n",
    "    else:\n",
    "        # assume first three columns are s/p/o\n",
    "        if df.shape[1] < 3:\n",
    "            # try reading as no header\n",
    "            df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                df = pd.read_csv(path, sep=\",\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                raise ValueError(\"Could not detect three columns for triples.\")\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"subject\",\"predicate\",\"object\"]\n",
    "\n",
    "    return df[[\"subject\",\"predicate\",\"object\"]].astype(\"string\").dropna()\n",
    "\n",
    "trip = load_triples(TRIPLES_TSV)\n",
    "\n",
    "# ---------- Filter to Movie -> Actor predicate (direction fixed) ---------------\n",
    "cast = (\n",
    "    trip.loc[trip[\"predicate\"] == CAST_PREDICATE, [\"subject\",\"object\"]]\n",
    "    .dropna()\n",
    "    .rename(columns={\"subject\":\"movie\",\"object\":\"actor\"})\n",
    "    .drop_duplicates()\n",
    "    .astype(\"string\")\n",
    ")\n",
    "\n",
    "if cast.empty:\n",
    "    raise ValueError(f\"No triples found with predicate '{CAST_PREDICATE}' (Movie -> Actor).\")\n",
    "\n",
    "# ---------- SQL pipeline in SQLite --------------------------------------------\n",
    "con = sqlite3.connect(\":memory:\")\n",
    "cast.to_sql(\"cast\", con, index=False, if_exists=\"replace\")\n",
    "con.execute(\"CREATE INDEX IF NOT EXISTS idx_cast_movie ON cast(movie);\")\n",
    "\n",
    "# Pairs per movie (unordered via actorA < actorB) and strong pair detection\n",
    "pair_counts_sql = f\"\"\"\n",
    "WITH pairs AS (\n",
    "  SELECT c1.movie AS movie,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "  FROM cast c1\n",
    "  JOIN cast c2\n",
    "    ON c1.movie = c2.movie\n",
    "   AND c1.actor < c2.actor\n",
    ")\n",
    "SELECT actorA, actorB, COUNT(DISTINCT movie) AS coapps\n",
    "FROM pairs\n",
    "GROUP BY actorA, actorB\n",
    "HAVING coapps >= {int(MIN_COAPPS)};\n",
    "\"\"\"\n",
    "df_strong = pd.read_sql_query(pair_counts_sql, con)\n",
    "\n",
    "# If no strong pairs, write empty outputs and finish\n",
    "if df_strong.empty:\n",
    "    pd.DataFrame(columns=[\"subject\",\"predicate\",\"object\"]).to_csv(OUT_STRONG_PAIRS, sep=\"\\t\", index=False)\n",
    "    pd.DataFrame(columns=[\"subject\",\"predicate\",\"object\"]).to_csv(OUT_FEATURE_ANN,  sep=\"\\t\", index=False)\n",
    "    pd.DataFrame(columns=[\"subject\",\"predicate\",\"object\"]).to_csv(OUT_CASTPAIR_NODES, sep=\"\\t\", index=False)\n",
    "    con.close()\n",
    "    print(\"✅ Strong cast (SQL) rule applied. No strong pairs found.\")\n",
    "    print(f\"Wrote: {OUT_STRONG_PAIRS} (0), {OUT_FEATURE_ANN} (0), {OUT_CASTPAIR_NODES} (0)\")\n",
    "else:\n",
    "    # ---------- Assign deterministic castPairIDs (castPair1, castPair2, ...) ----\n",
    "    # Sort for stable numbering\n",
    "    df_strong = df_strong.sort_values([\"actorA\",\"actorB\"]).reset_index(drop=True)\n",
    "    df_strong[\"castPairID\"] = [\"castPair{}\".format(i+1) for i in range(len(df_strong))]\n",
    "\n",
    "    # ---------- (1) ex:strongCastWith triples between the two actors -----------\n",
    "    base = pd.DataFrame({\n",
    "        \"subject\": df_strong[\"actorA\"],\n",
    "        \"predicate\": PRED_STRONG,\n",
    "        \"object\": df_strong[\"actorB\"]\n",
    "    })\n",
    "    if MAKE_SYMMETRIC:\n",
    "        sym = pd.DataFrame({\n",
    "            \"subject\": df_strong[\"actorB\"],\n",
    "            \"predicate\": PRED_STRONG,\n",
    "            \"object\": df_strong[\"actorA\"]\n",
    "        })\n",
    "        trip_strong_pairs = pd.concat([base, sym], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        trip_strong_pairs = base\n",
    "    trip_strong_pairs.to_csv(OUT_STRONG_PAIRS, sep=\"\\t\", index=False)\n",
    "\n",
    "    # ---------- (2) ex:featuresStrongCastPair: Movie -> castPairID -------------\n",
    "    # Recompute movie-level pairs in SQL and join to strong pairs to know which movie has which castPairID\n",
    "    movie_pairs_sql = \"\"\"\n",
    "                      WITH pairs AS (\n",
    "                          SELECT c1.movie AS movie,\n",
    "                                 CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "                                 CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "                          FROM cast c1\n",
    "                                   JOIN cast c2\n",
    "                                        ON c1.movie = c2.movie\n",
    "                                            AND c1.actor < c2.actor\n",
    "                      )\n",
    "                      SELECT movie, actorA, actorB\n",
    "                      FROM pairs; \\\n",
    "                      \"\"\"\n",
    "    df_movie_pairs = pd.read_sql_query(movie_pairs_sql, con)\n",
    "\n",
    "    # Join movie pairs with strong list to get castPairID per movie\n",
    "    df_join = df_movie_pairs.merge(\n",
    "        df_strong[[\"actorA\",\"actorB\",\"castPairID\"]],\n",
    "        on=[\"actorA\",\"actorB\"],\n",
    "        how=\"inner\"\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    trip_features = pd.DataFrame({\n",
    "        \"subject\": df_join[\"movie\"],\n",
    "        \"predicate\": PRED_FEATURES,\n",
    "        \"object\": df_join[\"castPairID\"]\n",
    "    }).drop_duplicates()\n",
    "    trip_features.to_csv(OUT_FEATURE_ANN, sep=\"\\t\", index=False)\n",
    "\n",
    "    # ---------- (3) CastPair node → schema:actor → Person triples --------------\n",
    "    # For each castPairID, add two triples linking to the two actors\n",
    "    cp_actor_a = pd.DataFrame({\n",
    "        \"subject\": df_strong[\"castPairID\"],\n",
    "        \"predicate\": PRED_CASTPAIR_ACTOR,\n",
    "        \"object\": df_strong[\"actorA\"]\n",
    "    })\n",
    "    cp_actor_b = pd.DataFrame({\n",
    "        \"subject\": df_strong[\"castPairID\"],\n",
    "        \"predicate\": PRED_CASTPAIR_ACTOR,\n",
    "        \"object\": df_strong[\"actorB\"]\n",
    "    })\n",
    "    trip_castpair_nodes = pd.concat([cp_actor_a, cp_actor_b], ignore_index=True).drop_duplicates()\n",
    "    trip_castpair_nodes.to_csv(OUT_CASTPAIR_NODES, sep=\"\\t\", index=False)\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    # ---------- Report ---------------------------------------------------------\n",
    "    print(\"✅ Strong cast (SQL) rule applied.\")\n",
    "    print(f\"Strong pairs (canonical, coapps ≥ {MIN_COAPPS}): {len(df_strong):,}\")\n",
    "    print(f\"Wrote:\")\n",
    "    print(f\"  - {OUT_STRONG_PAIRS}     ({len(trip_strong_pairs):,} triples, {PRED_STRONG})\")\n",
    "    print(f\"  - {OUT_FEATURE_ANN}      ({len(trip_features):,} triples, {PRED_FEATURES})\")\n",
    "    print(f\"  - {OUT_CASTPAIR_NODES}   ({len(trip_castpair_nodes):,} triples, {PRED_CASTPAIR_ACTOR})\")\n"
   ],
   "id": "58beb7fde3aee2da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Strong cast (SQL) rule applied.\n",
      "Strong pairs (canonical, coapps ≥ 3): 674\n",
      "Wrote:\n",
      "  - ../data/kg/triples/derived_strong_cast_pairs.tsv     (1,348 triples, ex:strongCastWith)\n",
      "  - ../data/kg/triples/derived_features_strong_cast.tsv      (2,445 triples, ex:featuresStrongCastPair)\n",
      "  - ../data/kg/triples/derived_castpair_nodes.tsv   (1,348 triples, schema:actor)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-20T15:23:08.906834Z",
     "start_time": "2025-09-20T15:23:08.551332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Rule 3 (SQL) -> append new triples directly to movie_kg_triples.tsv ------\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os, csv, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "TRIPLES_TSV         = \"../data/kg/triples/movie_kg_triples.tsv\"   # dein KG\n",
    "CAST_PREDICATE      = \"schema:actor\"   # Movie -> Actor\n",
    "MIN_COAPPS          = 3                # threshold for strong pair\n",
    "MAKE_SYMMETRIC      = True             # B->A für ex:strongCastWith\n",
    "PRED_STRONG         = \"ex:strongCastWith\"\n",
    "PRED_FEATURES       = \"ex:featuresStrongCastPair\"\n",
    "PRED_CASTPAIR_ACTOR = \"schema:actor\"\n",
    "\n",
    "MAKE_BACKUP         = True             # vor dem Anhängen Backup erzeugen\n",
    "BACKUP_DIR          = \"../data/kg/triples/backups\"  # wohin sichern\n",
    "# ==================================================\n",
    "\n",
    "# ---------- helpers: load triples + safe append --------------------------------\n",
    "def load_triples(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=\"\\t\", dtype=\"string\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, sep=\",\", dtype=\"string\")\n",
    "    cols = [c.strip().lower() for c in df.columns.tolist()]\n",
    "    mapping_opts = [\n",
    "        {\"subject\":\"subject\",\"predicate\":\"predicate\",\"object\":\"object\"},\n",
    "        {\"s\":\"subject\",\"p\":\"predicate\",\"o\":\"object\"},\n",
    "        {\"head\":\"subject\",\"relation\":\"predicate\",\"tail\":\"object\"},\n",
    "        {\"h\":\"subject\",\"r\":\"predicate\",\"t\":\"object\"},\n",
    "        {\"source\":\"subject\",\"relation\":\"predicate\",\"target\":\"object\"},\n",
    "        {\"from\":\"subject\",\"edge\":\"predicate\",\"to\":\"object\"},\n",
    "    ]\n",
    "    applied = None\n",
    "    for cand in mapping_opts:\n",
    "        if set(cand.keys()).issubset(set(cols)):\n",
    "            applied = {}\n",
    "            for k,v in cand.items():\n",
    "                real = df.columns[cols.index(k)]\n",
    "                applied[real] = v\n",
    "            break\n",
    "    if applied:\n",
    "        df = df.rename(columns=applied)\n",
    "    else:\n",
    "        if df.shape[1] < 3:\n",
    "            df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                df = pd.read_csv(path, sep=\",\", header=None, dtype=\"string\")\n",
    "            if df.shape[1] < 3:\n",
    "                raise ValueError(\"Could not detect three columns for triples.\")\n",
    "        df = df.iloc[:, :3].copy()\n",
    "        df.columns = [\"subject\",\"predicate\",\"object\"]\n",
    "    return df[[\"subject\",\"predicate\",\"object\"]].astype(\"string\").dropna()\n",
    "\n",
    "def sanitize_cell(x: str) -> str:\n",
    "    if pd.isna(x): return \"\"\n",
    "    s = str(x)\n",
    "    return s.replace(\"\\t\",\" \").replace(\"\\r\",\" \").replace(\"\\n\",\" \").strip()\n",
    "\n",
    "def append_triples_safely(df_triples: pd.DataFrame, path: str):\n",
    "    # erwartet Spalten: subject, predicate, object\n",
    "    df = df_triples[[\"subject\",\"predicate\",\"object\"]].copy()\n",
    "    for c in [\"subject\",\"predicate\",\"object\"]:\n",
    "        df[c] = df[c].map(sanitize_cell)\n",
    "\n",
    "    # optional: Backup\n",
    "    if MAKE_BACKUP and os.path.exists(path):\n",
    "        os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_path = os.path.join(BACKUP_DIR, f\"movie_kg_triples_backup_{ts}.tsv\")\n",
    "        shutil.copy2(path, backup_path)\n",
    "        print(\"📂 Backup gespeichert unter:\", backup_path)\n",
    "\n",
    "    # falls Datei existiert, sicherstellen, dass sie mit \\n endet\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            try:\n",
    "                f.seek(-1, os.SEEK_END)\n",
    "                if f.read(1) != b\"\\n\":\n",
    "                    with open(path, \"ab\") as g:\n",
    "                        g.write(b\"\\n\")\n",
    "            except OSError:\n",
    "                pass  # leere Datei\n",
    "\n",
    "    # tatsächlich anhängen (ohne Header!)\n",
    "    df.to_csv(\n",
    "        path,\n",
    "        mode=\"a\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "        sep=\"\\t\",\n",
    "        lineterminator=\"\\n\",\n",
    "        encoding=\"utf-8\",\n",
    "        quoting=csv.QUOTE_MINIMAL,\n",
    "        escapechar=\"\\\\\",\n",
    "    )\n",
    "\n",
    "# ---------- load triples + filter cast ----------------------------------------\n",
    "trip = load_triples(TRIPLES_TSV)\n",
    "\n",
    "cast = (\n",
    "    trip.loc[trip[\"predicate\"] == CAST_PREDICATE, [\"subject\",\"object\"]]\n",
    "    .dropna()\n",
    "    .rename(columns={\"subject\":\"movie\",\"object\":\"actor\"})\n",
    "    .drop_duplicates()\n",
    "    .astype(\"string\")\n",
    ")\n",
    "\n",
    "if cast.empty:\n",
    "    raise ValueError(f\"No triples found with predicate '{CAST_PREDICATE}' (Movie -> Actor).\")\n",
    "\n",
    "# ---------- SQL pipeline in SQLite --------------------------------------------\n",
    "con = sqlite3.connect(\":memory:\")\n",
    "cast.to_sql(\"cast\", con, index=False, if_exists=\"replace\")\n",
    "con.execute(\"CREATE INDEX IF NOT EXISTS idx_cast_movie ON cast(movie);\")\n",
    "\n",
    "pair_counts_sql = f\"\"\"\n",
    "WITH pairs AS (\n",
    "  SELECT c1.movie AS movie,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "         CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "  FROM cast c1\n",
    "  JOIN cast c2\n",
    "    ON c1.movie = c2.movie\n",
    "   AND c1.actor < c2.actor\n",
    ")\n",
    "SELECT actorA, actorB, COUNT(DISTINCT movie) AS coapps\n",
    "FROM pairs\n",
    "GROUP BY actorA, actorB\n",
    "HAVING coapps >= {int(MIN_COAPPS)};\n",
    "\"\"\"\n",
    "df_strong = pd.read_sql_query(pair_counts_sql, con)\n",
    "\n",
    "if df_strong.empty:\n",
    "    con.close()\n",
    "    print(\"✅ Strong cast (SQL) rule angewendet. Keine starken Paare gefunden. Nichts angehängt.\")\n",
    "else:\n",
    "    # stabile IDs\n",
    "    df_strong = df_strong.sort_values([\"actorA\",\"actorB\"]).reset_index(drop=True)\n",
    "    df_strong[\"castPairID\"] = [f\"castPair{i+1}\" for i in range(len(df_strong))]\n",
    "\n",
    "    # (1) ex:strongCastWith\n",
    "    base = pd.DataFrame({\n",
    "        \"subject\":  df_strong[\"actorA\"],\n",
    "        \"predicate\": PRED_STRONG,\n",
    "        \"object\":   df_strong[\"actorB\"]\n",
    "    })\n",
    "    if MAKE_SYMMETRIC:\n",
    "        sym = pd.DataFrame({\n",
    "            \"subject\":  df_strong[\"actorB\"],\n",
    "            \"predicate\": PRED_STRONG,\n",
    "            \"object\":   df_strong[\"actorA\"]\n",
    "        })\n",
    "        trip_strong_pairs = pd.concat([base, sym], ignore_index=True).drop_duplicates()\n",
    "    else:\n",
    "        trip_strong_pairs = base\n",
    "\n",
    "    # (2) ex:featuresStrongCastPair: Movie -> castPairID\n",
    "    movie_pairs_sql = \"\"\"\n",
    "                      WITH pairs AS (\n",
    "                          SELECT c1.movie AS movie,\n",
    "                                 CASE WHEN c1.actor < c2.actor THEN c1.actor ELSE c2.actor END AS actorA,\n",
    "                                 CASE WHEN c1.actor < c2.actor THEN c2.actor ELSE c1.actor END AS actorB\n",
    "                          FROM cast c1\n",
    "                                   JOIN cast c2\n",
    "                                        ON c1.movie = c2.movie\n",
    "                                            AND c1.actor < c2.actor\n",
    "                      )\n",
    "                      SELECT movie, actorA, actorB\n",
    "                      FROM pairs; \\\n",
    "                      \"\"\"\n",
    "    df_movie_pairs = pd.read_sql_query(movie_pairs_sql, con)\n",
    "    con.close()\n",
    "\n",
    "    df_join = df_movie_pairs.merge(\n",
    "        df_strong[[\"actorA\",\"actorB\",\"castPairID\"]],\n",
    "        on=[\"actorA\",\"actorB\"],\n",
    "        how=\"inner\"\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    trip_features = pd.DataFrame({\n",
    "        \"subject\":  df_join[\"movie\"],\n",
    "        \"predicate\": PRED_FEATURES,\n",
    "        \"object\":   df_join[\"castPairID\"]\n",
    "    }).drop_duplicates()\n",
    "\n",
    "    # (3) CastPair node -> schema:actor -> Person (2 Triples je Pair)\n",
    "    cp_actor_a = pd.DataFrame({\n",
    "        \"subject\":  df_strong[\"castPairID\"],\n",
    "        \"predicate\": PRED_CASTPAIR_ACTOR,\n",
    "        \"object\":   df_strong[\"actorA\"]\n",
    "    })\n",
    "    cp_actor_b = pd.DataFrame({\n",
    "        \"subject\":  df_strong[\"castPairID\"],\n",
    "        \"predicate\": PRED_CASTPAIR_ACTOR,\n",
    "        \"object\":   df_strong[\"actorB\"]\n",
    "    })\n",
    "    trip_castpair_nodes = pd.concat([cp_actor_a, cp_actor_b], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # --- combine all new triples and append to main KG ------------------------\n",
    "    all_new = pd.concat(\n",
    "        [trip_strong_pairs, trip_features, trip_castpair_nodes],\n",
    "        ignore_index=True\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    if all_new.empty:\n",
    "        print(\"ℹ️ Keine neuen Triples zum Anhängen gefunden.\")\n",
    "    else:\n",
    "        append_triples_safely(all_new, TRIPLES_TSV)\n",
    "        print(\"✅ Neue Triples angehängt an:\", TRIPLES_TSV)\n",
    "        print(f\"   + ex:strongCastWith:        {(trip_strong_pairs.shape[0]):,}\")\n",
    "        print(f\"   + ex:featuresStrongCastPair:{(trip_features.shape[0]):,}\")\n",
    "        print(f\"   + schema:actor (CastPair):  {(trip_castpair_nodes.shape[0]):,}\")\n"
   ],
   "id": "473bc3b8760348d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Backup gespeichert unter: ../data/kg/triples/backups/movie_kg_triples_backup_20250920_172308.tsv\n",
      "✅ Neue Triples angehängt an: ../data/kg/triples/movie_kg_triples.tsv\n",
      "   + ex:strongCastWith:        1,348\n",
      "   + ex:featuresStrongCastPair:2,445\n",
      "   + schema:actor (CastPair):  1,348\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
