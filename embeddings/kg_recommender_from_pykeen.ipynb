{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb77ce8d",
   "metadata": {},
   "source": [
    "# KG Recommender with PyKEEN (TransE)\n",
    "\n",
    "This notebook:\n",
    "1) Loads your triples (`movie_kg_triples.tsv`)\n",
    "2) Trains **TransE** with PyKEEN\n",
    "3) Extracts **entity** and **relation** embeddings\n",
    "4) Builds a **user vector** from your ratings/likes\n",
    "5) Composes and **scores a new movie** in embedding space\n",
    "\n",
    "> Update paths as needed for your project layout."
   ]
  },
  {
   "cell_type": "code",
   "id": "a09bfbd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:00:21.565790Z",
     "start_time": "2025-09-02T21:00:21.559094Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "print(\"PyTorch:\", torch.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3d670be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:00:25.688222Z",
     "start_time": "2025-09-02T21:00:25.685172Z"
    }
   },
   "source": [
    "# Path to your triples. Adjust if needed.\n",
    "# If you run this notebook right after downloading, the example here expects your project layout.\n",
    "# For quick testing in this environment, we also show a fallback to the uploaded file.\n",
    "default_path = Path('../data/kg/movie_kg_triples.tsv')\n",
    "uploaded_path = Path('/mnt/data/movie_kg_triples.tsv')\n",
    "\n",
    "triples_path = default_path if default_path.exists() else uploaded_path\n",
    "print(\"Using triples from:\", triples_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using triples from: ../data/kg/movie_kg_triples.tsv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "ed7f07bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:00:52.246771Z",
     "start_time": "2025-09-02T21:00:52.074027Z"
    }
   },
   "source": [
    "# Load triples into a PyKEEN TriplesFactory\n",
    "tf = TriplesFactory.from_path(str(triples_path))\n",
    "print(f\"Loaded {len(tf.triples)} triples. Entities: {tf.num_entities}, Relations: {tf.num_relations}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reconstructing all label-based triples. This is expensive and rarely needed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41184 triples. Entities: 19891, Relations: 21\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1a419fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:02:37.868131Z",
     "start_time": "2025-09-02T21:00:57.691687Z"
    }
   },
   "source": [
    "# Split and train TransE\n",
    "train_tf, test_tf = tf.split([0.8, 0.2])\n",
    "result = pipeline(\n",
    "    model='TransE',\n",
    "    training=train_tf,\n",
    "    testing=test_tf,\n",
    "    training_kwargs=dict(num_epochs=100),\n",
    "    random_seed=42,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=1525797145\n",
      "/opt/anaconda3/envs/letterboxd-KG/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Training epochs on cpu:   0%|          | 0/100 [00:00<?, ?epoch/s]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  12%|█▏        | 15.0/129 [00:00<00:00, 149batch/s]\u001B[A\n",
      "Training batches on cpu:  33%|███▎      | 42.0/129 [00:00<00:00, 219batch/s]\u001B[A\n",
      "Training batches on cpu:  55%|█████▌    | 71.0/129 [00:00<00:00, 250batch/s]\u001B[A\n",
      "Training batches on cpu:  74%|███████▍  | 96.0/129 [00:00<00:00, 238batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 223batch/s] \u001B[A\n",
      "Training epochs on cpu:   1%|          | 1/100 [00:00<01:11,  1.39epoch/s, loss=1, prev_loss=nan]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:   9%|▉         | 12.0/129 [00:00<00:00, 119batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▊        | 24.0/129 [00:00<00:01, 104batch/s]\u001B[A\n",
      "Training batches on cpu:  29%|██▉       | 38.0/129 [00:00<00:00, 118batch/s]\u001B[A\n",
      "Training batches on cpu:  43%|████▎     | 56.0/129 [00:00<00:00, 141batch/s]\u001B[A\n",
      "Training batches on cpu:  55%|█████▌    | 71.0/129 [00:00<00:00, 140batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 149batch/s]\u001B[A\n",
      "Training batches on cpu:  81%|████████  | 104/129 [00:00<00:00, 130batch/s] \u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 125batch/s]\u001B[A\n",
      "Training epochs on cpu:   2%|▏         | 2/100 [00:01<01:29,  1.10epoch/s, loss=0.819, prev_loss=1]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  18%|█▊        | 23.0/129 [00:00<00:00, 226batch/s]\u001B[A\n",
      "Training batches on cpu:  37%|███▋      | 48.0/129 [00:00<00:00, 235batch/s]\u001B[A\n",
      "Training batches on cpu:  56%|█████▌    | 72.0/129 [00:00<00:00, 232batch/s]\u001B[A\n",
      "Training batches on cpu:  74%|███████▍  | 96.0/129 [00:00<00:00, 230batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 230batch/s] \u001B[A\n",
      "Training epochs on cpu:   3%|▎         | 3/100 [00:02<01:16,  1.26epoch/s, loss=0.675, prev_loss=0.819]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  16%|█▌        | 20.0/129 [00:00<00:00, 196batch/s]\u001B[A\n",
      "Training batches on cpu:  33%|███▎      | 43.0/129 [00:00<00:00, 215batch/s]\u001B[A\n",
      "Training batches on cpu:  52%|█████▏    | 67.0/129 [00:00<00:00, 225batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 235batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████ | 117/129 [00:00<00:00, 236batch/s] \u001B[A\n",
      "Training epochs on cpu:   4%|▍         | 4/100 [00:03<01:10,  1.37epoch/s, loss=0.566, prev_loss=0.675]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  15%|█▍        | 19.0/129 [00:00<00:00, 189batch/s]\u001B[A\n",
      "Training batches on cpu:  32%|███▏      | 41.0/129 [00:00<00:00, 202batch/s]\u001B[A\n",
      "Training batches on cpu:  50%|████▉     | 64.0/129 [00:00<00:00, 211batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 220batch/s]\u001B[A\n",
      "Training batches on cpu:  87%|████████▋ | 112/129 [00:00<00:00, 223batch/s] \u001B[A\n",
      "Training epochs on cpu:   5%|▌         | 5/100 [00:03<01:07,  1.42epoch/s, loss=0.47, prev_loss=0.566] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  18%|█▊        | 23.0/129 [00:00<00:00, 221batch/s]\u001B[A\n",
      "Training batches on cpu:  37%|███▋      | 48.0/129 [00:00<00:00, 234batch/s]\u001B[A\n",
      "Training batches on cpu:  56%|█████▌    | 72.0/129 [00:00<00:00, 225batch/s]\u001B[A\n",
      "Training batches on cpu:  76%|███████▌  | 98.0/129 [00:00<00:00, 237batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 241batch/s] \u001B[A\n",
      "Training epochs on cpu:   6%|▌         | 6/100 [00:04<01:03,  1.47epoch/s, loss=0.391, prev_loss=0.47]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  16%|█▋        | 21.0/129 [00:00<00:00, 200batch/s]\u001B[A\n",
      "Training batches on cpu:  36%|███▋      | 47.0/129 [00:00<00:00, 230batch/s]\u001B[A\n",
      "Training batches on cpu:  57%|█████▋    | 73.0/129 [00:00<00:00, 240batch/s]\u001B[A\n",
      "Training batches on cpu:  76%|███████▌  | 98.0/129 [00:00<00:00, 240batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 246batch/s] \u001B[A\n",
      "Training epochs on cpu:   7%|▋         | 7/100 [00:04<01:01,  1.50epoch/s, loss=0.321, prev_loss=0.391]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  20%|██        | 26.0/129 [00:00<00:00, 257batch/s]\u001B[A\n",
      "Training batches on cpu:  40%|████      | 52.0/129 [00:00<00:00, 259batch/s]\u001B[A\n",
      "Training batches on cpu:  60%|██████    | 78.0/129 [00:00<00:00, 253batch/s]\u001B[A\n",
      "Training batches on cpu:  81%|████████  | 104/129 [00:00<00:00, 249batch/s] \u001B[A\n",
      "Training batches on cpu: 100%|██████████| 129/129 [00:00<00:00, 249batch/s]\u001B[A\n",
      "Training epochs on cpu:   8%|▊         | 8/100 [00:05<00:58,  1.56epoch/s, loss=0.266, prev_loss=0.321]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  20%|██        | 26.0/129 [00:00<00:00, 254batch/s]\u001B[A\n",
      "Training batches on cpu:  41%|████      | 53.0/129 [00:00<00:00, 261batch/s]\u001B[A\n",
      "Training batches on cpu:  62%|██████▏   | 80.0/129 [00:00<00:00, 258batch/s]\u001B[A\n",
      "Training batches on cpu:  83%|████████▎ | 107/129 [00:00<00:00, 260batch/s] \u001B[A\n",
      "Training epochs on cpu:   9%|▉         | 9/100 [00:06<00:56,  1.62epoch/s, loss=0.223, prev_loss=0.266]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 285batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 274batch/s] \u001B[A\n",
      "Training epochs on cpu:  10%|█         | 10/100 [00:06<00:53,  1.69epoch/s, loss=0.186, prev_loss=0.223]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 28.0/129 [00:00<00:00, 277batch/s]\u001B[A\n",
      "Training batches on cpu:  44%|████▍     | 57.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 87.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  90%|████████▉ | 116/129 [00:00<00:00, 282batch/s] \u001B[A\n",
      "Training epochs on cpu:  11%|█         | 11/100 [00:07<00:53,  1.66epoch/s, loss=0.166, prev_loss=0.186]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  14%|█▍        | 18.0/129 [00:00<00:00, 179batch/s]\u001B[A\n",
      "Training batches on cpu:  29%|██▉       | 38.0/129 [00:00<00:00, 184batch/s]\u001B[A\n",
      "Training batches on cpu:  44%|████▍     | 57.0/129 [00:00<00:00, 165batch/s]\u001B[A\n",
      "Training batches on cpu:  57%|█████▋    | 74.0/129 [00:00<00:00, 156batch/s]\u001B[A\n",
      "Training batches on cpu:  76%|███████▌  | 98.0/129 [00:00<00:00, 183batch/s]\u001B[A\n",
      "Training batches on cpu:  98%|█████████▊| 127/129 [00:00<00:00, 215batch/s] \u001B[A\n",
      "Training epochs on cpu:  12%|█▏        | 12/100 [00:08<00:56,  1.55epoch/s, loss=0.141, prev_loss=0.166]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  13%|█▎        | 13/100 [00:08<00:52,  1.65epoch/s, loss=0.122, prev_loss=0.141]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 289batch/s] \u001B[A\n",
      "Training epochs on cpu:  14%|█▍        | 14/100 [00:09<00:49,  1.73epoch/s, loss=0.11, prev_loss=0.122] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 295batch/s] \u001B[A\n",
      "Training epochs on cpu:  15%|█▌        | 15/100 [00:09<00:47,  1.80epoch/s, loss=0.0961, prev_loss=0.11]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 298batch/s] \u001B[A\n",
      "Training epochs on cpu:  16%|█▌        | 16/100 [00:10<00:45,  1.85epoch/s, loss=0.0884, prev_loss=0.0961]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 291batch/s] \u001B[A\n",
      "Training epochs on cpu:  17%|█▋        | 17/100 [00:10<00:44,  1.88epoch/s, loss=0.0813, prev_loss=0.0884]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 295batch/s] \u001B[A\n",
      "Training epochs on cpu:  18%|█▊        | 18/100 [00:11<00:43,  1.90epoch/s, loss=0.0779, prev_loss=0.0813]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  19%|█▉        | 19/100 [00:11<00:42,  1.92epoch/s, loss=0.0677, prev_loss=0.0779]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 302batch/s] \u001B[A\n",
      "Training epochs on cpu:  20%|██        | 20/100 [00:12<00:41,  1.95epoch/s, loss=0.0657, prev_loss=0.0677]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 296batch/s] \u001B[A\n",
      "Training epochs on cpu:  21%|██        | 21/100 [00:12<00:40,  1.95epoch/s, loss=0.0587, prev_loss=0.0657]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  14%|█▍        | 18.0/129 [00:00<00:00, 179batch/s]\u001B[A\n",
      "Training batches on cpu:  33%|███▎      | 42.0/129 [00:00<00:00, 211batch/s]\u001B[A\n",
      "Training batches on cpu:  53%|█████▎    | 68.0/129 [00:00<00:00, 232batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 221batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 240batch/s] \u001B[A\n",
      "Training epochs on cpu:  22%|██▏       | 22/100 [00:13<00:42,  1.83epoch/s, loss=0.0568, prev_loss=0.0587]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 300batch/s] \u001B[A\n",
      "Training epochs on cpu:  23%|██▎       | 23/100 [00:13<00:40,  1.88epoch/s, loss=0.052, prev_loss=0.0568] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 287batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  24%|██▍       | 24/100 [00:14<00:39,  1.91epoch/s, loss=0.0517, prev_loss=0.052]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 302batch/s] \u001B[A\n",
      "Training epochs on cpu:  25%|██▌       | 25/100 [00:14<00:38,  1.94epoch/s, loss=0.0487, prev_loss=0.0517]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 306batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 291batch/s] \u001B[A\n",
      "Training epochs on cpu:  26%|██▌       | 26/100 [00:15<00:37,  1.95epoch/s, loss=0.0484, prev_loss=0.0487]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▊        | 24.0/129 [00:00<00:00, 236batch/s]\u001B[A\n",
      "Training batches on cpu:  40%|████      | 52.0/129 [00:00<00:00, 257batch/s]\u001B[A\n",
      "Training batches on cpu:  64%|██████▎   | 82.0/129 [00:00<00:00, 276batch/s]\u001B[A\n",
      "Training batches on cpu:  88%|████████▊ | 113/129 [00:00<00:00, 288batch/s] \u001B[A\n",
      "Training epochs on cpu:  27%|██▋       | 27/100 [00:15<00:37,  1.93epoch/s, loss=0.0443, prev_loss=0.0484]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 284batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 286batch/s] \u001B[A\n",
      "Training epochs on cpu:  28%|██▊       | 28/100 [00:16<00:37,  1.92epoch/s, loss=0.0425, prev_loss=0.0443]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  20%|██        | 26.0/129 [00:00<00:00, 258batch/s]\u001B[A\n",
      "Training batches on cpu:  43%|████▎     | 56.0/129 [00:00<00:00, 279batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 87.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████ | 117/129 [00:00<00:00, 295batch/s] \u001B[A\n",
      "Training epochs on cpu:  29%|██▉       | 29/100 [00:16<00:36,  1.93epoch/s, loss=0.0433, prev_loss=0.0425]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  30%|███       | 30/100 [00:17<00:35,  1.95epoch/s, loss=0.0407, prev_loss=0.0433]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 301batch/s] \u001B[A\n",
      "Training epochs on cpu:  31%|███       | 31/100 [00:17<00:35,  1.96epoch/s, loss=0.0393, prev_loss=0.0407]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 243batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 243batch/s] \u001B[A\n",
      "Training epochs on cpu:  32%|███▏      | 32/100 [00:18<00:36,  1.88epoch/s, loss=0.0354, prev_loss=0.0393]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▉        | 25.0/129 [00:00<00:00, 245batch/s]\u001B[A\n",
      "Training batches on cpu:  42%|████▏     | 54.0/129 [00:00<00:00, 269batch/s]\u001B[A\n",
      "Training batches on cpu:  65%|██████▌   | 84.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  88%|████████▊ | 113/129 [00:00<00:00, 279batch/s] \u001B[A\n",
      "Training epochs on cpu:  33%|███▎      | 33/100 [00:18<00:35,  1.87epoch/s, loss=0.0373, prev_loss=0.0354]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 300batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 300batch/s] \u001B[A\n",
      "Training epochs on cpu:  34%|███▍      | 34/100 [00:19<00:34,  1.90epoch/s, loss=0.0366, prev_loss=0.0373]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  35%|███▌      | 35/100 [00:19<00:33,  1.92epoch/s, loss=0.0351, prev_loss=0.0366]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 307batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 306batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 306batch/s] \u001B[A\n",
      "Training epochs on cpu:  36%|███▌      | 36/100 [00:20<00:32,  1.96epoch/s, loss=0.0354, prev_loss=0.0351]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 304batch/s] \u001B[A\n",
      "Training epochs on cpu:  37%|███▋      | 37/100 [00:20<00:31,  1.98epoch/s, loss=0.0341, prev_loss=0.0354]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 306batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  38%|███▊      | 38/100 [00:21<00:31,  1.98epoch/s, loss=0.0332, prev_loss=0.0341]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 300batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 289batch/s] \u001B[A\n",
      "Training epochs on cpu:  39%|███▉      | 39/100 [00:21<00:30,  1.97epoch/s, loss=0.0307, prev_loss=0.0332]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 300batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 301batch/s] \u001B[A\n",
      "Training epochs on cpu:  40%|████      | 40/100 [00:22<00:30,  1.98epoch/s, loss=0.0337, prev_loss=0.0307]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  41%|████      | 41/100 [00:22<00:29,  1.98epoch/s, loss=0.0342, prev_loss=0.0337]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 302batch/s] \u001B[A\n",
      "Training epochs on cpu:  42%|████▏     | 42/100 [00:23<00:29,  1.99epoch/s, loss=0.0335, prev_loss=0.0342]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▊        | 24.0/129 [00:00<00:00, 236batch/s]\u001B[A\n",
      "Training batches on cpu:  38%|███▊      | 49.0/129 [00:00<00:00, 240batch/s]\u001B[A\n",
      "Training batches on cpu:  57%|█████▋    | 74.0/129 [00:00<00:00, 240batch/s]\u001B[A\n",
      "Training batches on cpu:  76%|███████▌  | 98.0/129 [00:00<00:00, 236batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 233batch/s] \u001B[A\n",
      "Training epochs on cpu:  43%|████▎     | 43/100 [00:24<00:30,  1.86epoch/s, loss=0.0332, prev_loss=0.0335]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  44%|████▍     | 44/100 [00:24<00:29,  1.90epoch/s, loss=0.0306, prev_loss=0.0332]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 232batch/s] \u001B[A\n",
      "Training epochs on cpu:  45%|████▌     | 45/100 [00:25<00:30,  1.82epoch/s, loss=0.028, prev_loss=0.0306] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▊        | 24.0/129 [00:00<00:00, 237batch/s]\u001B[A\n",
      "Training batches on cpu:  37%|███▋      | 48.0/129 [00:00<00:00, 223batch/s]\u001B[A\n",
      "Training batches on cpu:  55%|█████▌    | 71.0/129 [00:00<00:00, 197batch/s]\u001B[A\n",
      "Training batches on cpu:  77%|███████▋  | 99.0/129 [00:00<00:00, 224batch/s]\u001B[A\n",
      "Training batches on cpu:  98%|█████████▊| 126/129 [00:00<00:00, 238batch/s] \u001B[A\n",
      "Training epochs on cpu:  46%|████▌     | 46/100 [00:25<00:31,  1.73epoch/s, loss=0.0312, prev_loss=0.028]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 250batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 265batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 275batch/s] \u001B[A\n",
      "Training epochs on cpu:  47%|████▋     | 47/100 [00:26<00:29,  1.77epoch/s, loss=0.0297, prev_loss=0.0312]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  48%|████▊     | 48/100 [00:26<00:28,  1.83epoch/s, loss=0.0313, prev_loss=0.0297]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 284batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████ | 117/129 [00:00<00:00, 288batch/s] \u001B[A\n",
      "Training epochs on cpu:  49%|████▉     | 49/100 [00:27<00:27,  1.86epoch/s, loss=0.0323, prev_loss=0.0313]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 286batch/s] \u001B[A\n",
      "Training epochs on cpu:  50%|█████     | 50/100 [00:27<00:26,  1.88epoch/s, loss=0.0296, prev_loss=0.0323]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 300batch/s] \u001B[A\n",
      "Training epochs on cpu:  51%|█████     | 51/100 [00:28<00:25,  1.91epoch/s, loss=0.0301, prev_loss=0.0296]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 308batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 300batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  52%|█████▏    | 52/100 [00:28<00:24,  1.94epoch/s, loss=0.0315, prev_loss=0.0301]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 220batch/s]\u001B[A\n",
      "Training batches on cpu:  65%|██████▌   | 84.0/129 [00:00<00:00, 224batch/s]\u001B[A\n",
      "Training batches on cpu:  84%|████████▎ | 108/129 [00:00<00:00, 180batch/s] \u001B[A\n",
      "Training batches on cpu:  99%|█████████▉| 128/129 [00:00<00:00, 181batch/s]\u001B[A\n",
      "Training epochs on cpu:  53%|█████▎    | 53/100 [00:29<00:27,  1.71epoch/s, loss=0.0313, prev_loss=0.0315]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  21%|██        | 27.0/129 [00:00<00:00, 269batch/s]\u001B[A\n",
      "Training batches on cpu:  43%|████▎     | 56.0/129 [00:00<00:00, 280batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 86.0/129 [00:00<00:00, 287batch/s]\u001B[A\n",
      "Training batches on cpu:  90%|████████▉ | 116/129 [00:00<00:00, 292batch/s] \u001B[A\n",
      "Training epochs on cpu:  54%|█████▍    | 54/100 [00:30<00:26,  1.76epoch/s, loss=0.0317, prev_loss=0.0313]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 28.0/129 [00:00<00:00, 272batch/s]\u001B[A\n",
      "Training batches on cpu:  44%|████▍     | 57.0/129 [00:00<00:00, 281batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 86.0/129 [00:00<00:00, 282batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████ | 117/129 [00:00<00:00, 290batch/s] \u001B[A\n",
      "Training epochs on cpu:  55%|█████▌    | 55/100 [00:30<00:24,  1.80epoch/s, loss=0.0297, prev_loss=0.0317]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 287batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 87.0/129 [00:00<00:00, 285batch/s]\u001B[A\n",
      "Training batches on cpu:  90%|████████▉ | 116/129 [00:00<00:00, 282batch/s] \u001B[A\n",
      "Training epochs on cpu:  56%|█████▌    | 56/100 [00:31<00:24,  1.82epoch/s, loss=0.0294, prev_loss=0.0297]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 296batch/s] \u001B[A\n",
      "Training epochs on cpu:  57%|█████▋    | 57/100 [00:31<00:23,  1.87epoch/s, loss=0.0288, prev_loss=0.0294]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  58%|█████▊    | 58/100 [00:32<00:22,  1.90epoch/s, loss=0.0302, prev_loss=0.0288]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 288batch/s] \u001B[A\n",
      "Training epochs on cpu:  59%|█████▉    | 59/100 [00:32<00:21,  1.91epoch/s, loss=0.0294, prev_loss=0.0302]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  21%|██        | 27.0/129 [00:00<00:00, 267batch/s]\u001B[A\n",
      "Training batches on cpu:  44%|████▍     | 57.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 289batch/s] \u001B[A\n",
      "Training epochs on cpu:  60%|██████    | 60/100 [00:33<00:20,  1.91epoch/s, loss=0.029, prev_loss=0.0294] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 295batch/s] \u001B[A\n",
      "Training epochs on cpu:  61%|██████    | 61/100 [00:33<00:20,  1.93epoch/s, loss=0.0305, prev_loss=0.029]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  46%|████▌     | 59.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 296batch/s] \u001B[A\n",
      "Training epochs on cpu:  62%|██████▏   | 62/100 [00:34<00:19,  1.94epoch/s, loss=0.0299, prev_loss=0.0305]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 287batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 240batch/s] \u001B[A\n",
      "Training epochs on cpu:  63%|██████▎   | 63/100 [00:34<00:20,  1.81epoch/s, loss=0.0278, prev_loss=0.0299]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  17%|█▋        | 22.0/129 [00:00<00:00, 214batch/s]\u001B[A\n",
      "Training batches on cpu:  36%|███▌      | 46.0/129 [00:00<00:00, 222batch/s]\u001B[A\n",
      "Training batches on cpu:  57%|█████▋    | 73.0/129 [00:00<00:00, 242batch/s]\u001B[A\n",
      "Training batches on cpu:  78%|███████▊  | 100/129 [00:00<00:00, 252batch/s] \u001B[A\n",
      "Training batches on cpu:  98%|█████████▊| 127/129 [00:00<00:00, 258batch/s]\u001B[A\n",
      "Training epochs on cpu:  64%|██████▍   | 64/100 [00:35<00:20,  1.78epoch/s, loss=0.0275, prev_loss=0.0278]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  65%|██████▌   | 65/100 [00:36<00:19,  1.84epoch/s, loss=0.0295, prev_loss=0.0275]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 290batch/s] \u001B[A\n",
      "Training epochs on cpu:  66%|██████▌   | 66/100 [00:36<00:18,  1.86epoch/s, loss=0.0268, prev_loss=0.0295]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  67%|██████▋   | 67/100 [00:37<00:17,  1.89epoch/s, loss=0.0281, prev_loss=0.0268]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 286batch/s] \u001B[A\n",
      "Training epochs on cpu:  68%|██████▊   | 68/100 [00:37<00:16,  1.90epoch/s, loss=0.0278, prev_loss=0.0281]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 87.0/129 [00:00<00:00, 287batch/s]\u001B[A\n",
      "Training batches on cpu:  90%|████████▉ | 116/129 [00:00<00:00, 285batch/s] \u001B[A\n",
      "Training epochs on cpu:  69%|██████▉   | 69/100 [00:38<00:16,  1.89epoch/s, loss=0.0283, prev_loss=0.0278]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████▏  | 92.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 282batch/s] \u001B[A\n",
      "Training epochs on cpu:  70%|███████   | 70/100 [00:38<00:15,  1.89epoch/s, loss=0.0292, prev_loss=0.0283]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▌| 123/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  71%|███████   | 71/100 [00:39<00:15,  1.92epoch/s, loss=0.0288, prev_loss=0.0292]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 298batch/s] \u001B[A\n",
      "Training epochs on cpu:  72%|███████▏  | 72/100 [00:39<00:14,  1.93epoch/s, loss=0.0281, prev_loss=0.0288]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 294batch/s] \u001B[A\n",
      "Training epochs on cpu:  73%|███████▎  | 73/100 [00:40<00:13,  1.95epoch/s, loss=0.0276, prev_loss=0.0281]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▉        | 25.0/129 [00:00<00:00, 246batch/s]\u001B[A\n",
      "Training batches on cpu:  39%|███▉      | 50.0/129 [00:00<00:00, 238batch/s]\u001B[A\n",
      "Training batches on cpu:  57%|█████▋    | 74.0/129 [00:00<00:00, 238batch/s]\u001B[A\n",
      "Training batches on cpu:  76%|███████▌  | 98.0/129 [00:00<00:00, 217batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 217batch/s] \u001B[A\n",
      "Training epochs on cpu:  74%|███████▍  | 74/100 [00:40<00:14,  1.79epoch/s, loss=0.0268, prev_loss=0.0276]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 285batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 280batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 274batch/s] \u001B[A\n",
      "Training epochs on cpu:  75%|███████▌  | 75/100 [00:41<00:13,  1.81epoch/s, loss=0.0271, prev_loss=0.0268]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 281batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 281batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 279batch/s] \u001B[A\n",
      "Training epochs on cpu:  76%|███████▌  | 76/100 [00:41<00:13,  1.83epoch/s, loss=0.026, prev_loss=0.0271] \n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  46%|████▌     | 59.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 289batch/s] \u001B[A\n",
      "Training epochs on cpu:  77%|███████▋  | 77/100 [00:42<00:12,  1.86epoch/s, loss=0.0279, prev_loss=0.026]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  21%|██        | 27.0/129 [00:00<00:00, 270batch/s]\u001B[A\n",
      "Training batches on cpu:  42%|████▏     | 54.0/129 [00:00<00:00, 257batch/s]\u001B[A\n",
      "Training batches on cpu:  64%|██████▎   | 82.0/129 [00:00<00:00, 265batch/s]\u001B[A\n",
      "Training batches on cpu:  84%|████████▍ | 109/129 [00:00<00:00, 256batch/s] \u001B[A\n",
      "Training epochs on cpu:  78%|███████▊  | 78/100 [00:42<00:12,  1.82epoch/s, loss=0.0278, prev_loss=0.0279]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  21%|██        | 27.0/129 [00:00<00:00, 270batch/s]\u001B[A\n",
      "Training batches on cpu:  44%|████▍     | 57.0/129 [00:00<00:00, 280batch/s]\u001B[A\n",
      "Training batches on cpu:  67%|██████▋   | 86.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  90%|████████▉ | 116/129 [00:00<00:00, 289batch/s] \u001B[A\n",
      "Training epochs on cpu:  79%|███████▉  | 79/100 [00:43<00:11,  1.85epoch/s, loss=0.0265, prev_loss=0.0278]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 299batch/s] \u001B[A\n",
      "Training epochs on cpu:  80%|████████  | 80/100 [00:44<00:10,  1.89epoch/s, loss=0.0254, prev_loss=0.0265]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 300batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 283batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 278batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 284batch/s] \u001B[A\n",
      "Training epochs on cpu:  81%|████████  | 81/100 [00:44<00:10,  1.89epoch/s, loss=0.0273, prev_loss=0.0254]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  45%|████▍     | 58.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  68%|██████▊   | 88.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  91%|█████████▏| 118/129 [00:00<00:00, 290batch/s] \u001B[A\n",
      "Training epochs on cpu:  82%|████████▏ | 82/100 [00:45<00:09,  1.90epoch/s, loss=0.0285, prev_loss=0.0273]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 285batch/s]\u001B[A\n",
      "Training batches on cpu:  46%|████▌     | 59.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 293batch/s] \u001B[A\n",
      "Training epochs on cpu:  83%|████████▎ | 83/100 [00:45<00:08,  1.92epoch/s, loss=0.0246, prev_loss=0.0285]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 262batch/s] \u001B[A\n",
      "Training epochs on cpu:  84%|████████▍ | 84/100 [00:46<00:08,  1.85epoch/s, loss=0.0266, prev_loss=0.0246]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  18%|█▊        | 23.0/129 [00:00<00:00, 210batch/s]\u001B[A\n",
      "Training batches on cpu:  35%|███▍      | 45.0/129 [00:00<00:00, 195batch/s]\u001B[A\n",
      "Training batches on cpu:  53%|█████▎    | 69.0/129 [00:00<00:00, 211batch/s]\u001B[A\n",
      "Training batches on cpu:  77%|███████▋  | 99.0/129 [00:00<00:00, 242batch/s]\u001B[A\n",
      "Training epochs on cpu:  85%|████████▌ | 85/100 [00:46<00:08,  1.79epoch/s, loss=0.0276, prev_loss=0.0266]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 297batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 302batch/s] \u001B[A\n",
      "Training epochs on cpu:  86%|████████▌ | 86/100 [00:47<00:07,  1.85epoch/s, loss=0.0263, prev_loss=0.0276]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 298batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  87%|████████▋ | 87/100 [00:47<00:06,  1.88epoch/s, loss=0.0275, prev_loss=0.0263]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 29.0/129 [00:00<00:00, 286batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  88%|████████▊ | 88/100 [00:48<00:06,  1.91epoch/s, loss=0.0266, prev_loss=0.0275]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 273batch/s] \u001B[A\n",
      "Training epochs on cpu:  89%|████████▉ | 89/100 [00:48<00:05,  1.88epoch/s, loss=0.0274, prev_loss=0.0266]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 291batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 290batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  94%|█████████▍| 121/129 [00:00<00:00, 295batch/s] \u001B[A\n",
      "Training epochs on cpu:  90%|█████████ | 90/100 [00:49<00:05,  1.90epoch/s, loss=0.0274, prev_loss=0.0274]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 299batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 61.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  71%|███████   | 91.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 285batch/s] \u001B[A\n",
      "Training epochs on cpu:  91%|█████████ | 91/100 [00:49<00:04,  1.91epoch/s, loss=0.0256, prev_loss=0.0274]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 302batch/s] \u001B[A\n",
      "Training epochs on cpu:  92%|█████████▏| 92/100 [00:50<00:04,  1.94epoch/s, loss=0.0243, prev_loss=0.0256]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 301batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 302batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 297batch/s] \u001B[A\n",
      "Training epochs on cpu:  93%|█████████▎| 93/100 [00:50<00:03,  1.96epoch/s, loss=0.0257, prev_loss=0.0243]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 294batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 288batch/s] \u001B[A\n",
      "Training epochs on cpu:  94%|█████████▍| 94/100 [00:51<00:03,  1.95epoch/s, loss=0.0269, prev_loss=0.0257]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  22%|██▏       | 28.0/129 [00:00<00:00, 272batch/s]\u001B[A\n",
      "Training batches on cpu:  43%|████▎     | 56.0/129 [00:00<00:00, 201batch/s]\u001B[A\n",
      "Training batches on cpu:  60%|██████    | 78.0/129 [00:00<00:00, 197batch/s]\u001B[A\n",
      "Training batches on cpu:  77%|███████▋  | 99.0/129 [00:00<00:00, 171batch/s]\u001B[A\n",
      "Training batches on cpu:  95%|█████████▍| 122/129 [00:00<00:00, 186batch/s] \u001B[A\n",
      "Training epochs on cpu:  95%|█████████▌| 95/100 [00:52<00:02,  1.70epoch/s, loss=0.0264, prev_loss=0.0269]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  19%|█▉        | 25.0/129 [00:00<00:00, 242batch/s]\u001B[A\n",
      "Training batches on cpu:  41%|████      | 53.0/129 [00:00<00:00, 264batch/s]\u001B[A\n",
      "Training batches on cpu:  64%|██████▍   | 83.0/129 [00:00<00:00, 279batch/s]\u001B[A\n",
      "Training batches on cpu:  87%|████████▋ | 112/129 [00:00<00:00, 280batch/s] \u001B[A\n",
      "Training epochs on cpu:  96%|█████████▌| 96/100 [00:52<00:02,  1.74epoch/s, loss=0.0266, prev_loss=0.0264]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  25%|██▍       | 32.0/129 [00:00<00:00, 311batch/s]\u001B[A\n",
      "Training batches on cpu:  50%|████▉     | 64.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  74%|███████▎  | 95.0/129 [00:00<00:00, 304batch/s]\u001B[A\n",
      "Training batches on cpu:  98%|█████████▊| 126/129 [00:00<00:00, 287batch/s] \u001B[A\n",
      "Training epochs on cpu:  97%|█████████▋| 97/100 [00:53<00:01,  1.80epoch/s, loss=0.0253, prev_loss=0.0266]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 293batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 289batch/s]\u001B[A\n",
      "Training batches on cpu:  69%|██████▉   | 89.0/129 [00:00<00:00, 288batch/s]\u001B[A\n",
      "Training batches on cpu:  92%|█████████▏| 119/129 [00:00<00:00, 290batch/s] \u001B[A\n",
      "Training epochs on cpu:  98%|█████████▊| 98/100 [00:53<00:01,  1.84epoch/s, loss=0.0265, prev_loss=0.0253]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  24%|██▍       | 31.0/129 [00:00<00:00, 306batch/s]\u001B[A\n",
      "Training batches on cpu:  48%|████▊     | 62.0/129 [00:00<00:00, 303batch/s]\u001B[A\n",
      "Training batches on cpu:  72%|███████▏  | 93.0/129 [00:00<00:00, 305batch/s]\u001B[A\n",
      "Training batches on cpu:  96%|█████████▌| 124/129 [00:00<00:00, 306batch/s] \u001B[A\n",
      "Training epochs on cpu:  99%|█████████▉| 99/100 [00:54<00:00,  1.89epoch/s, loss=0.0243, prev_loss=0.0265]\n",
      "Training batches on cpu:   0%|          | 0.00/129 [00:00<?, ?batch/s]\u001B[A\n",
      "Training batches on cpu:  23%|██▎       | 30.0/129 [00:00<00:00, 292batch/s]\u001B[A\n",
      "Training batches on cpu:  47%|████▋     | 60.0/129 [00:00<00:00, 296batch/s]\u001B[A\n",
      "Training batches on cpu:  70%|██████▉   | 90.0/129 [00:00<00:00, 295batch/s]\u001B[A\n",
      "Training batches on cpu:  93%|█████████▎| 120/129 [00:00<00:00, 296batch/s] \u001B[A\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [00:54<00:00,  1.83epoch/s, loss=0.0253, prev_loss=0.0243]\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/8.24k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 8.24k/8.24k [00:44<00:00, 185triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 44.56s seconds\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "962408b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:06:01.393769Z",
     "start_time": "2025-09-02T21:06:01.017179Z"
    }
   },
   "source": [
    "# Extract embeddings\n",
    "entity_rep = result.model.entity_representations[0]\n",
    "relation_rep = result.model.relation_representations[0]\n",
    "\n",
    "entity_embeds = entity_rep(torch.arange(tf.num_entities)).detach().cpu().numpy()\n",
    "relation_embeds = relation_rep(torch.arange(tf.num_relations)).detach().cpu().numpy()\n",
    "\n",
    "entity_df = pd.DataFrame(entity_embeds, index=tf.entity_labeling.label_to_id.keys())\n",
    "relation_df = pd.DataFrame(relation_embeds, index=tf.relation_labeling.label_to_id.keys())\n",
    "\n",
    "entity_df.to_csv(\"../data/kg/embeddings/entity_embeddings.csv\")\n",
    "relation_df.to_csv(\"../data/kg/embeddings/relation_embeddings.csv\")\n",
    "\n",
    "print(\"Entity embedding shape:\", entity_df.shape)\n",
    "print(\"Relation embedding shape:\", relation_df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity embedding shape: (19891, 50)\n",
      "Relation embedding shape: (21, 50)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "b6aa4e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:06:47.013829Z",
     "start_time": "2025-09-02T21:06:46.954622Z"
    }
   },
   "source": [
    "# Parse ratings, likes, names from triples file for profile building\n",
    "triples_df = pd.read_csv(triples_path, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
    "\n",
    "def parse_rating(v):\n",
    "    import re\n",
    "    m = re.match(r'personalVote_([0-9.]+)', str(v))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "ratings = triples_df[triples_df['relation'] == 'schema:review']\n",
    "likes = triples_df[triples_df['relation'] == 'ex:liked']\n",
    "names = triples_df[triples_df['relation'] == 'schema:name']\n",
    "\n",
    "movie2rating = dict(zip(ratings['head'], ratings['tail'].map(parse_rating)))\n",
    "movie2liked = dict(zip(likes['head'], likes['tail'].map(lambda x: str(x).lower() == 'liked_yes')))\n",
    "id2name = dict(zip(names['head'], names['tail']))\n",
    "\n",
    "valid_ratings = [v for v in movie2rating.values() if v is not None]\n",
    "rmin, rmax = (min(valid_ratings), max(valid_ratings)) if valid_ratings else (0.0, 1.0)\n",
    "denom = (rmax - rmin) if rmax > rmin else 1.0\n",
    "\n",
    "def weight(movie, like_bonus=0.15):\n",
    "    r = movie2rating.get(movie)\n",
    "    if r is None: return 0.0\n",
    "    w = (r - rmin) / denom\n",
    "    if movie2liked.get(movie, False):\n",
    "        w += like_bonus\n",
    "    return max(0.0, w)\n",
    "\n",
    "weighted_vecs = []\n",
    "for movie in entity_df.index:\n",
    "    if movie in movie2rating and movie in entity_df.index:\n",
    "        w = weight(movie)\n",
    "        if w > 0:\n",
    "            weighted_vecs.append(w * entity_df.loc[movie].values)\n",
    "\n",
    "if not weighted_vecs:\n",
    "    raise RuntimeError(\"No weighted movies found. Check that your triples contain schema:review values.\")\n",
    "user_vec = np.mean(weighted_vecs, axis=0)\n",
    "user_vec = user_vec / (np.linalg.norm(user_vec) + 1e-12)\n",
    "print(\"User vector created. Norm:\", np.linalg.norm(user_vec))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User vector created. Norm: 1.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "50999e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:06:49.499771Z",
     "start_time": "2025-09-02T21:06:49.491482Z"
    }
   },
   "source": [
    "# Compose a new (unseen) movie embedding via TransE translations\n",
    "def translate_neighbor_to_movie(neighbor_id, relation_label):\n",
    "    if neighbor_id not in entity_df.index or relation_label not in relation_df.index:\n",
    "        return None\n",
    "    # TransE: movie + r ≈ neighbor  => movie ≈ neighbor - r\n",
    "    return entity_df.loc[neighbor_id].values - relation_df.loc[relation_label].values\n",
    "\n",
    "# Example inputs (replace with your own test)\n",
    "actors = [\"Joaqin Phoenix\", \"Charles Dance\"]\n",
    "directors = [\"James Cameron\"]\n",
    "genres = [\"Action\"]      # ensure your KG has an entity labeled 'Action'; else leave empty or adjust\n",
    "language = [\"en\"]        # ensure language token exists as an entity if you want to include it\n",
    "\n",
    "# Resolve names -> ids\n",
    "name2id = {v: k for k, v in id2name.items()}\n",
    "actor_ids = [name2id.get(n) for n in actors if name2id.get(n) in entity_df.index]\n",
    "director_ids = [name2id.get(n) for n in directors if name2id.get(n) in entity_df.index]\n",
    "genre_ids = [n for n in genres if n in entity_df.index]\n",
    "lang_ids = [n for n in language if n in entity_df.index]\n",
    "\n",
    "parts = []\n",
    "for aid in actor_ids:\n",
    "    v = translate_neighbor_to_movie(aid, \"schema:actor\")\n",
    "    if v is not None: parts.append(v)\n",
    "for did in director_ids:\n",
    "    v = translate_neighbor_to_movie(did, \"schema:director\")\n",
    "    if v is not None: parts.append(v)\n",
    "for gid in genre_ids:\n",
    "    v = translate_neighbor_to_movie(gid, \"schema:genre\")\n",
    "    if v is not None: parts.append(v)\n",
    "for lid in lang_ids:\n",
    "    v = translate_neighbor_to_movie(lid, \"ex:originalLanguage\")\n",
    "    if v is not None: parts.append(v)\n",
    "\n",
    "if not parts:\n",
    "    print(\"⚠️ No components found for the new movie. Check that the chosen names/labels exist in your KG.\")\n",
    "else:\n",
    "    new_movie_vec = np.mean(parts, axis=0)\n",
    "    new_movie_vec = new_movie_vec / (np.linalg.norm(new_movie_vec) + 1e-12)\n",
    "    cosine = float(np.dot(user_vec, new_movie_vec))\n",
    "    print(f\"New movie similarity score: {cosine:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New movie similarity score: 0.8937\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T21:06:52.519060Z",
     "start_time": "2025-09-02T21:06:52.485763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 🔍 Automatically extract favorites from KG using rating-weighted frequency\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Load full triples if not already loaded\n",
    "if 'triples_df' not in locals():\n",
    "    triples_df = pd.read_csv(triples_path, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
    "\n",
    "# --- QUICK FIX: enrich id2name with labels from the KG (covers genres) ---\n",
    "label_preds = {\"ex:name\", \"schema:name\", \"rdfs:label\"}\n",
    "label_map = (\n",
    "    triples_df[triples_df['relation'].isin(label_preds)]\n",
    "    .dropna(subset=['head','tail'])\n",
    "    .drop_duplicates(subset=['head'])\n",
    "    .set_index('head')['tail']\n",
    "    .to_dict()\n",
    ")\n",
    "# Merge into existing id2name if present, otherwise create it\n",
    "try:\n",
    "    id2name\n",
    "    id2name = {**label_map, **id2name}  # your custom names (right) win\n",
    "except NameError:\n",
    "    id2name = label_map\n",
    "# --- end quick fix ---\n",
    "\n",
    "# Get rated/liked movies with weight > 0\n",
    "relevant_movies = {m for m in movie2rating if weight(m) > 0.0}\n",
    "\n",
    "# Helper: count tail values linked to these movies by a relation\n",
    "def top_related_entities(relation, top_k=10):\n",
    "    related = triples_df[\n",
    "        (triples_df['relation'] == relation) &\n",
    "        (triples_df['head'].isin(relevant_movies))\n",
    "        ]['tail']\n",
    "    return Counter(related).most_common(top_k)\n",
    "\n",
    "# Top actors, directors, genres, and original languages\n",
    "top_actors = top_related_entities(\"schema:actor\", top_k=10)\n",
    "top_directors = top_related_entities(\"schema:director\", top_k=10)\n",
    "top_genres = top_related_entities(\"schema:genre\", top_k=5)\n",
    "top_langs = top_related_entities(\"ex:originalLanguage\", top_k=3)\n",
    "\n",
    "# Map IDs to readable names\n",
    "def display_top(counter_list):\n",
    "    return [(id2name.get(eid, eid), count) for eid, count in counter_list]\n",
    "\n",
    "print(\"🎭 Top actors:\")\n",
    "print(display_top(top_actors))\n",
    "print(\"\\n🎬 Top directors:\")\n",
    "print(display_top(top_directors))\n",
    "print(\"\\n🏷️ Top genres:\")\n",
    "print(display_top(top_genres))\n",
    "print(\"\\n🌍 Top languages:\")\n",
    "print(display_top(top_langs))"
   ],
   "id": "c0ce55b8d1061ab0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Top actors:\n",
      "[('Willem Dafoe', 10), ('Bill Murray', 9), ('Keanu Reeves', 7), ('Margot Robbie', 7), ('Jason Schwartzman', 7), ('J.K. Simmons', 7), ('Laurence Fishburne', 6), ('Sigourney Weaver', 6), ('Woody Harrelson', 6), ('Jeffrey Wright', 6)]\n",
      "\n",
      "🎬 Top directors:\n",
      "[('Wes Anderson', 11), ('David Lynch', 5), ('Quentin Tarantino', 5), ('Zack Snyder', 5), ('George Miller', 4), ('Hayao Miyazaki', 4), ('Gore Verbinski', 4), ('Bo Burnham', 4), ('Paul W. S. Anderson', 3), ('Dan Trachtenberg', 3)]\n",
      "\n",
      "🏷️ Top genres:\n",
      "[('Drama', 115), ('Comedy', 112), ('Adventure', 91), ('Action', 86), ('Science Fiction', 74)]\n",
      "\n",
      "🌍 Top languages:\n",
      "[('English', 270), ('Japanese', 8), ('German', 6)]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "The code in the previous cells is in big parts AI generated by the free and paid version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "\n",
    "    \"what i originally wanted to do was create new movie recommendations based on these data. can I do this, for example deriving info on liked actors/directors and applying this info to a new list on movies that is currently not in the data yet?\"\n",
    "\n",
    "\n",
    "    \"Lets start from scratch - having only my movie_kg_triples.tsv file, can you generate a script that uses PyKEEN for exactly the things your script did before? only now I want a Jupyter notebook file (.ipynb) that I can include in my project.\"\n",
    "\n",
    "    \"# 🔍 Automatically extract favorites from KG using rating-weighted frequency\n",
    "\n",
    "        from collections import Counter\n",
    "\n",
    "        # Load full triples if not already loaded\n",
    "        if 'triples_df' not in locals():\n",
    "            triples_df = pd.read_csv(triples_path, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
    "\n",
    "        # Get rated/liked movies with weight > 0\n",
    "        relevant_movies = {m for m in movie2rating if weight(m) > 0.0}\n",
    "\n",
    "        # Helper: count tail values linked to these movies by a relation\n",
    "        def top_related_entities(relation, top_k=10):\n",
    "            related = triples_df[\n",
    "                (triples_df['relation'] == relation) &\n",
    "                (triples_df['head'].isin(relevant_movies))\n",
    "                ]['tail']\n",
    "            return Counter(related).most_common(top_k)\n",
    "\n",
    "        # Top actors, directors, genres, and original languages\n",
    "        top_actors = top_related_entities(\"schema:actor\", top_k=10)\n",
    "        top_directors = top_related_entities(\"schema:director\", top_k=10)\n",
    "        top_genres = top_related_entities(\"schema:genre\", top_k=5)\n",
    "        top_langs = top_related_entities(\"ex:originalLanguage\", top_k=3)\n",
    "\n",
    "        # Map IDs to readable names\n",
    "        def display_top(counter_list):\n",
    "            return [(id2name.get(eid, eid), count) for eid, count in counter_list]\n",
    "\n",
    "        print(\"🎭 Top actors:\")\n",
    "        print(display_top(top_actors))\n",
    "        print(\"\\n🎬 Top directors:\")\n",
    "        print(display_top(top_directors))\n",
    "        print(\"\\n🏷️ Top genres:\")\n",
    "        print(display_top(top_genres))\n",
    "        print(\"\\n🌍 Top languages:\")\n",
    "        print(display_top(top_langs))\n",
    "\n",
    "        this code works on this file: movie_kg_triples.tsv\n",
    "\n",
    "        🎭 Top actors:\n",
    "        [('Willem Dafoe', 10), ('Bill Murray', 9), ('Keanu Reeves', 7), ('Margot Robbie', 7), ('Jason Schwartzman', 7), ('J.K. Simmons', 7), ('Sigourney Weaver', 6), ('Woody Harrelson', 6), ('Jeffrey Wright', 6), ('Stanley Tucci', 6)]\n",
    "\n",
    "        🎬 Top directors:\n",
    "        [('Wes Anderson', 11), ('David Lynch', 5), ('Quentin Tarantino', 5), ('Zack Snyder', 5), ('George Miller', 4), ('Hayao Miyazaki', 4), ('Gore Verbinski', 4), ('Bo Burnham', 4), ('Paul W. S. Anderson', 3), ('Dan Trachtenberg', 3)]\n",
    "\n",
    "        🏷️ Top genres:\n",
    "        [('genre18', 115), ('genre35', 112), ('genre12', 91), ('genre28', 86), ('genre878', 74)]\n",
    "\n",
    "        🌍 Top languages:\n",
    "        [('English', 270), ('Japanese', 8), ('German', 6)]\n",
    "\n",
    "        the output like this looks good, but i want the genres also connected to their names, which can be found in the triples.\"\n",
    "\n",
    "    \"# 🔍 Automatically extract favorites from KG using rating-weighted frequency\n",
    "\n",
    "        from collections import Counter\n",
    "\n",
    "        # Load full triples if not already loaded\n",
    "        if 'triples_df' not in locals():\n",
    "            triples_df = pd.read_csv(triples_path, sep='\\t', header=None, names=['head', 'relation', 'tail'])\n",
    "\n",
    "        # Get rated/liked movies with weight > 0\n",
    "        relevant_movies = {m for m in movie2rating if weight(m) > 0.0}\n",
    "\n",
    "        # Helper: count tail values linked to these movies by a relation\n",
    "        def top_related_entities(relation, top_k=10):\n",
    "            related = triples_df[\n",
    "                (triples_df['relation'] == relation) &\n",
    "                (triples_df['head'].isin(relevant_movies))\n",
    "                ]['tail']\n",
    "            return Counter(related).most_common(top_k)\n",
    "\n",
    "        # Top actors, directors, genres, and original languages\n",
    "        top_actors = top_related_entities(\"schema:actor\", top_k=10)\n",
    "        top_directors = top_related_entities(\"schema:director\", top_k=10)\n",
    "        top_genres = top_related_entities(\"schema:genre\", top_k=5)\n",
    "        top_langs = top_related_entities(\"ex:originalLanguage\", top_k=3)\n",
    "\n",
    "        # Map IDs to readable names\n",
    "        def display_top(counter_list):\n",
    "            return [(id2name.get(eid, eid), count) for eid, count in counter_list]\n",
    "\n",
    "        print(\"🎭 Top actors:\")\n",
    "        print(display_top(top_actors))\n",
    "        print(\"\\n🎬 Top directors:\")\n",
    "        print(display_top(top_directors))\n",
    "        print(\"\\n🏷️ Top genres:\")\n",
    "        print(display_top(top_genres))\n",
    "        print(\"\\n🌍 Top languages:\")\n",
    "        print(display_top(top_langs))\n",
    "\n",
    "        can you just include your quick fix in this code please\"\n",
    "\n",
    "\n",
    "'''"
   ],
   "id": "7bab530e42d19e24"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
