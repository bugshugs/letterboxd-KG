{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Global setup\n",
    "Setup of everything needed to globally"
   ],
   "id": "d0483a97ba16b160"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')"
   ],
   "id": "1c2be7c81e867fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Update Letterboxd Data\n",
    "\n",
    "Update the imported Letterboxd data to always use the most up-to-date files"
   ],
   "id": "cd11e21ee987a572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Boolean flag to enable/disable downloading\n",
    "# This should ONLY be set to True if you have an .env file containing valid letterboxd login credentials.\n",
    "# Otherwise, this could possibly render the given data unusable.\n",
    "\n",
    "UPDATE_DATA = True  # Set True to enable download"
   ],
   "id": "f102d8c7643c49c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Code updating data should the flag be set to True",
   "id": "54ebea304dc48dc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "if not UPDATE_DATA:\n",
    "    print(\"Download flag is OFF. Exiting without downloading.\")\n",
    "    exit(0)\n",
    "else:\n",
    "    USERNAME = os.getenv('LETTERBOXD_USERNAME')\n",
    "    PASSWORD = os.getenv('LETTERBOXD_PASSWORD')\n",
    "    DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR')\n",
    "\n",
    "    # Set up Firefox options\n",
    "    options = Options()\n",
    "    options.set_preference(\"browser.download.folderList\", 2)\n",
    "    options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\n",
    "    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/zip\")\n",
    "    options.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Log in\n",
    "        print(\"Logging in...\")\n",
    "        driver.get(\"https://letterboxd.com/sign-in/\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.find_element(By.ID, \"field-username\").send_keys(USERNAME)\n",
    "        driver.find_element(By.ID, \"field-password\").send_keys(PASSWORD)\n",
    "        driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "\n",
    "        time.sleep(5)  # Wait for login to complete\n",
    "\n",
    "        # Step 2: Go to data page\n",
    "        print(\"Navigating to export page...\")\n",
    "        driver.get(\"https://letterboxd.com/settings/data/\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Step 3: Click export button\n",
    "        print(\"Clicking export button...\")\n",
    "        export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/user/exportdata')]\")\n",
    "\n",
    "        export_link.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Step 4: Click confirm export button\n",
    "        print(\"Clicking confirm export button...\")\n",
    "        confirm_export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/data/export/')]\")\n",
    "\n",
    "        confirm_export_link.click()\n",
    "\n",
    "        print(\"Waiting for download to complete...\")\n",
    "        time.sleep(15)  # Adjust this depending on your connection speed\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "    # Step 4: Unzip and Load\n",
    "    print(\"Looking for ZIP file in download directory...\")\n",
    "    zip_path = None\n",
    "    for file in os.listdir(DOWNLOAD_DIR):\n",
    "        if file.endswith(\".zip\") and \"letterboxd\" in file.lower():\n",
    "            zip_path = os.path.join(DOWNLOAD_DIR, file)\n",
    "            break\n",
    "\n",
    "    if not zip_path:\n",
    "        raise FileNotFoundError(\"Export ZIP file not found!\")\n",
    "\n",
    "    extract_path = os.path.join(DOWNLOAD_DIR, \"letterboxd_export\")\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Unzipping to {extract_path}...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "\n",
    "    # Delete the zip file after extraction\n",
    "    os.remove(zip_path)\n",
    "    print(f\"Deleted ZIP file: {zip_path}\")\n"
   ],
   "id": "ba04fcf36cca61e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "    \"i want to write a python script that automatically exports my letterboxd data and loads it into my jupyter notebook. can you help me with that?\"\n",
    "\n",
    "\n",
    "    \"i can manually download my data from this page\n",
    "        https://letterboxd.com/settings/data/\n",
    "\n",
    "        can the script navigate there and download it?\"\n",
    "\n",
    "\n",
    "    \"I want to use Firefox and store the credentioals in an .env\"\n",
    "\n",
    "\n",
    "    \"add the following functionality:\n",
    "\n",
    "        there is a simple boolean flag that is per default on false. only if the flag is set to true, the download of the letterboxd data triggers. otherwise it does nothing\"\n",
    "\n",
    "\n",
    "    \"its the only button on the website. can I just try to locate any button element?\"\n",
    "\n",
    "\n",
    "    \"<a href=\"/data/export/\" class=\"button -action button-action export-data-button\">Export Data</a>\n",
    "\n",
    "        now i want to locate this button here\"\n",
    "\n",
    "\n",
    "    \"i want to write a python script that unzips a file for me and then deletes said file\"\n",
    "\n",
    "\n",
    "    \"import os\n",
    "        import time\n",
    "        import zipfile\n",
    "        from dotenv import load_dotenv\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.firefox.options import Options\n",
    "        from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "        if not UPDATE_DATA:\n",
    "            print(\"Download flag is OFF. Exiting without downloading.\")\n",
    "            exit(0)\n",
    "        else:\n",
    "            # Load environment variables\n",
    "            load_dotenv()\n",
    "            USERNAME = os.getenv('LETTERBOXD_USERNAME')\n",
    "            PASSWORD = os.getenv('LETTERBOXD_PASSWORD')\n",
    "            DOWNLOAD_DIR = os.getenv('DOWNLOAD_DIR')\n",
    "\n",
    "            # Set up Firefox options\n",
    "            options = Options()\n",
    "            options.set_preference(\"browser.download.folderList\", 2)\n",
    "            options.set_preference(\"browser.download.dir\", DOWNLOAD_DIR)\n",
    "            options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/zip\")\n",
    "            options.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "\n",
    "            try:\n",
    "                # Step 1: Log in\n",
    "                print(\"Logging in...\")\n",
    "                driver.get(\"https://letterboxd.com/sign-in/\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                driver.find_element(By.ID, \"field-username\").send_keys(USERNAME)\n",
    "                driver.find_element(By.ID, \"field-password\").send_keys(PASSWORD)\n",
    "                driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "\n",
    "                time.sleep(5)  # Wait for login to complete\n",
    "\n",
    "                # Step 2: Go to data page\n",
    "                print(\"Navigating to export page...\")\n",
    "                driver.get(\"https://letterboxd.com/settings/data/\")\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Step 3: Click export button\n",
    "                print(\"Clicking export button...\")\n",
    "                export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/user/exportdata')]\")\n",
    "\n",
    "                export_link.click()\n",
    "                time.sleep(3)\n",
    "\n",
    "                # Step 4: Click confirm export button\n",
    "                print(\"Clicking confirm export button...\")\n",
    "                confirm_export_link = driver.find_element(By.XPATH, \"//a[contains(@href, '/data/export/')]\")\n",
    "\n",
    "                confirm_export_link.click()\n",
    "\n",
    "                print(\"Waiting for download to complete...\")\n",
    "                time.sleep(15)  # Adjust this depending on your connection speed\n",
    "\n",
    "            finally:\n",
    "                driver.quit()\n",
    "                print(\"Browser closed.\")\n",
    "\n",
    "            # Step 4: Unzip and Load\n",
    "            print(\"Looking for ZIP file in download directory...\")\n",
    "            zip_path = None\n",
    "            for file in os.listdir(DOWNLOAD_DIR):\n",
    "                if file.endswith(\".zip\") and \"letterboxd\" in file.lower():\n",
    "                    zip_path = os.path.join(DOWNLOAD_DIR, file)\n",
    "                    break\n",
    "\n",
    "            if not zip_path:\n",
    "                raise FileNotFoundError(\"Export ZIP file not found!\")\n",
    "\n",
    "            extract_path = os.path.join(DOWNLOAD_DIR, \"letterboxd_export\")\n",
    "            os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "            print(f\"Unzipping to {extract_path}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_path)\n",
    "\n",
    "            # Delete the zip file after extraction\n",
    "            os.remove(zip_path)\n",
    "            print(f\"Deleted ZIP file: {zip_path}\")\n",
    "\n",
    "\n",
    "\n",
    "        Zu beginn des else zweiges möchte ich den kompletten order \"Users/tschaffel/Documents/PycharmProjects/JupyterProject/data/\" löschen.\"\n",
    "\n",
    "\n",
    "    \"\"\n",
    "\n",
    "'''"
   ],
   "id": "d665c8a15072193a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "Imports and files setup"
   ],
   "id": "11257c811890c6e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"data/letterboxd_export/ratings.csv\")\n",
    "watched = pd.read_csv(\"data/letterboxd_export/watched.csv\")\n",
    "likes = pd.read_csv(\"data/letterboxd_export/likes/films.csv\")\n",
    "diary = pd.read_csv(\"data/letterboxd_export/diary.csv\")\n",
    "\n",
    "orphaned_diary = pd.read_csv(\"data/letterboxd_export/orphaned/diary.csv\")"
   ],
   "id": "ea0758aac0c1eadd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## See Data\n",
    "\n",
    "First check on how the tables look"
   ],
   "id": "728cab10b3155747"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ratings.tail(10)",
   "id": "b783b33168960af3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "watched.tail(10)",
   "id": "801888289335e675",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "likes.tail(10)",
   "id": "c2063d1cf83f6513",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diary.tail(10)",
   "id": "22330eeffb2f6b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "orphaned_diary.tail(10)",
   "id": "b785160cb5f5c5b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Connect tables\n",
    "Connect letterboxd tables to get one table with following data for every movie:\n",
    "- Title and Year\n",
    "- Letterboxd URI\n",
    "- User Rating\n",
    "- Was the movie liked\n",
    "- When was the movie last watched\n",
    "- How many times has the movie been watched"
   ],
   "id": "fe426351a58bc056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged = pd.merge(\n",
    "    watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "    ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "    on=['Name', 'Year', 'Letterboxd URI'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add \"Liked\" information\n",
    "liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "    lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    ")\n",
    "\n",
    "\n",
    "# Add \"Last Watched\" information\n",
    "\n",
    "# Step 1: Group diary entries by movie and take the latest date as a string\n",
    "last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "# Step 2: Merge with the existing merged DataFrame\n",
    "merged = pd.merge(\n",
    "    merged,\n",
    "    last_watched,\n",
    "    on=[\"Name\", \"Year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add \"Times watched\" information\n",
    "\n",
    "# Step 1: Sort diary by \"Watched Date\" so the earliest watches come first\n",
    "diary_sorted = diary.sort_values(\"Watched Date\")\n",
    "\n",
    "# Step 2: Define a function to count watches and check if first was a rewatch\n",
    "def get_times_watched(group):\n",
    "    count = len(group)\n",
    "    first_rewatch = group.iloc[0][\"Rewatch\"] == \"Yes\"\n",
    "    return f\"{count}+\" if first_rewatch else str(count)\n",
    "\n",
    "# Step 3: Group and apply, excluding group keys from the inner DataFrame\n",
    "times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\n",
    "    get_times_watched\n",
    ").reset_index(name=\"Times watched\")\n",
    "\n",
    "\n",
    "# Step 4: Merge into the main DataFrame\n",
    "merged = (pd.merge(\n",
    "    merged,\n",
    "    times_watched,\n",
    "    on=[\"Name\", \"Year\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={'Name': 'Title'}).sort_values(by=\"Last watched date\", ascending=False))\n",
    "\n",
    "merged.head(15)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8caefb07b3b194b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "    \"wie kann ich selbst technisch die beiden tabellen verknüpfen?\"\n",
    "\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched,\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "        merged.head()\n",
    "\n",
    "        adaptiere mir diesen code bitte so, dass das CSV file unter \"likes/films\" auch eingelesen wird. in der resultierenden tabelle gibt es eine neue spalte namens \"Liked\", in der jeder film, der in \"likes/films\" enhalten ist, einen eintrag \"Yes\" bekommt, jeder andere einen eintrag \"No\" \"\n",
    "\n",
    "\n",
    "    \"Anstelle von \"ratings\", benutze \"diary\" und mach eine neue Spalte \"Anzahl\", die beinhaltet wie oft der selbe Film in \"diary\" vorkommt. als rating soll immer das datumsmäßig letzte verwendet werden.\"\n",
    "\n",
    "\n",
    "    \"das funktioniert nicht, date rating und anzahl sind alles NaN jetzt\"\n",
    "\n",
    "\n",
    "    \"date ist jetzt NaT, rating und number immer noch NaN\"\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "        this is my combined table so far. Now i want to add further information.\n",
    "        I want to add this:\n",
    "        First, from the table \"diary\" i want to add the LAST watched date of the movie. use the column \"watched date\", and if the movie has multiple entries i want the last one. call this new column \"Last watched date\". For movies that don't exist in \"diary\" we will have NaN.\"\n",
    "\n",
    "\n",
    "    \"I dont want to convert the date to datetime\"\n",
    "\n",
    "\n",
    "    \"merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add \"Last Watched\" information\n",
    "\n",
    "        # Step 1: Group diary entries by movie and take the latest date as a string\n",
    "        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "        # Step 2: Merge with the existing merged DataFrame\n",
    "        merged = pd.merge(\n",
    "            merged,\n",
    "            last_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        this is my code rn\n",
    "\n",
    "        now I want another thing:\n",
    "        I want a new column called \"Times watched\". From diary use the following information:\n",
    "        - Count the number of entries and display the number in the column \"Times watched\".\n",
    "        - If the oldest entry has \"Yes\" in the \"Rewatch\" column, then add a \"+\" after the number. Another way to do this is if every entry has \"Yes\" in the \"Rewatch\" column, do whatever is easier.\"\n",
    "\n",
    "\n",
    "    \"it is not chronologically correct, so i would prefer the safer option\"\n",
    "\n",
    "\n",
    "'''"
   ],
   "id": "f83a062950ad4864",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Connect Letterboxd Data with TMDB data",
   "id": "31806bb8a835f302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Bearer \" + TMDB_API_TOKEN,\n",
    "    \"Content-Type\": \"application/json;charset=utf-8\"\n",
    "}\n",
    "\n",
    "def normalize_title(title):\n",
    "    if not title:\n",
    "        return \"\"\n",
    "    # Unicode normalize, convert to ASCII-compatible\n",
    "    title = unicodedata.normalize(\"NFKC\", title)\n",
    "\n",
    "    # Replace common visually similar characters\n",
    "    substitutions = {\n",
    "        \"–\": \"-\",  # en dash\n",
    "        \"—\": \"-\",  # em dash\n",
    "        \"−\": \"-\",  # minus\n",
    "        \"×\": \"x\",  # multiplication sign\n",
    "        \"’\": \"'\",  # curly apostrophe\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"…\": \"...\",\n",
    "        \"&\": \"and\",  # optional\n",
    "    }\n",
    "\n",
    "    for orig, repl in substitutions.items():\n",
    "        title = title.replace(orig, repl)\n",
    "\n",
    "    # Collapse multiple spaces and lowercase\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip().lower()\n",
    "    return title\n",
    "\n",
    "def search_exact_match(results, search_title):\n",
    "    norm_search = normalize_title(search_title)\n",
    "    for r in results:\n",
    "        tmdb_title = r.get(\"title\") or r.get(\"name\") or \"\"\n",
    "        if normalize_title(tmdb_title) == norm_search:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def search_movie_or_tv(title, year=None):\n",
    "    # First: try movie search\n",
    "    movie_url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "    params = {\"query\": title}\n",
    "    if year:\n",
    "        params[\"year\"] = year\n",
    "\n",
    "    response = requests.get(movie_url, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        match = search_exact_match(results, title)\n",
    "        if match:\n",
    "            match[\"media_type\"] = \"movie\"\n",
    "            return match\n",
    "\n",
    "    # Second: try TV search\n",
    "    tv_url = \"https://api.themoviedb.org/3/search/tv\"\n",
    "    params = {\"query\": title}\n",
    "    if year:\n",
    "        params[\"first_air_date_year\"] = year\n",
    "\n",
    "    response = requests.get(tv_url, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json().get(\"results\", [])\n",
    "        match = search_exact_match(results, title)\n",
    "        if match:\n",
    "            match[\"media_type\"] = \"tv\"\n",
    "            return match\n",
    "    return None\n",
    "\n",
    "def get_details(tmdb_id, media_type):\n",
    "    url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "def get_credits(tmdb_id, media_type):\n",
    "    url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}/credits\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return response.json() if response.status_code == 200 else None\n",
    "\n",
    "def enrich_dataframe(df):\n",
    "    tmdb_media_base_url = \"https://www.themoviedb.org/\"\n",
    "    tmdb_poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "    tmdb_person_base_url = \"http://www.themoviedb.org/person/\"\n",
    "    tmdb_genre_base_url = \"https://www.themoviedb.org/genre/\"\n",
    "    tmdb_company_base_url = \"https://www.themoviedb.org/company/\"\n",
    "    size = \"original\"\n",
    "\n",
    "    # Create empty columns for TMDB data\n",
    "    df[\"tmdb_url\"] = None\n",
    "    df[\"overview\"] = None\n",
    "    df[\"genres\"] = None\n",
    "    df[\"runtime\"] = None\n",
    "    df[\"vote_average\"] = None\n",
    "    df[\"poster_url\"] = None\n",
    "    df[\"media_type\"] = None\n",
    "    df[\"director\"] = None\n",
    "    df[\"actors\"] = None\n",
    "    df[\"characters\"] = None\n",
    "    df[\"origin_country\"] = None # from here\n",
    "    df[\"original_language\"] = None\n",
    "    df[\"popularity\"] = None\n",
    "    df[\"production_companies\"] = None\n",
    "    df[\"production_countries\"] = None\n",
    "    df[\"spoken_languages\"] = None\n",
    "\n",
    "    total = len(df)\n",
    "    print(f\"Starting enrichment for {total} titles using TMDB data...\\n\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        title = row[\"Title\"]\n",
    "        year = row.get(\"Year\", None)\n",
    "\n",
    "        print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "        result = search_movie_or_tv(title, year)\n",
    "        if result:\n",
    "            print(\" ✅ Match found\")\n",
    "            media_type = result[\"media_type\"]\n",
    "            tmdb_id = result[\"id\"]\n",
    "\n",
    "            details = get_details(tmdb_id, media_type)\n",
    "            credits = get_credits(tmdb_id, media_type)\n",
    "\n",
    "            if details:\n",
    "                df.at[idx, \"media_type\"] = media_type\n",
    "                df.at[idx, \"tmdb_url\"] = tmdb_media_base_url + f\"{media_type}/\" + str(tmdb_id)\n",
    "                df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                df.at[idx, \"genres\"] = [str(g[\"name\"]) + \":\" + tmdb_genre_base_url + str(g[\"id\"]) for g in details.get(\"genres\", [])]\n",
    "                df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "                df.at[idx, \"origin_country\"] = details.get(\"origin_country\")\n",
    "                df.at[idx, \"original_language\"] = details.get(\"original_language\")\n",
    "                df.at[idx, \"popularity\"] = details.get(\"popularity\")\n",
    "                df.at[idx, \"production_companies\"] = [str(p[\"name\"]) + \":\" + tmdb_company_base_url + str(p[\"id\"]) + \":\" + str(p[\"origin_country\"]) for p in details.get(\"production_companies\", [])]\n",
    "                df.at[idx, \"production_countries\"] = [str(p[\"name\"]) + \":\" + str(p[\"iso_3166_1\"]) for p in details.get(\"production_countries\", [])]\n",
    "                df.at[idx, \"spoken_languages\"] = [str(s[\"english_name\"]) + \":\" + str(s[\"iso_639_1\"]) for s in details.get(\"spoken_languages\", [])]\n",
    "\n",
    "                if media_type == \"movie\":\n",
    "                    df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                else:\n",
    "                    df.at[idx, \"runtime\"] = None\n",
    "\n",
    "                poster_path = details.get(\"poster_path\")\n",
    "                if poster_path:\n",
    "                    df.at[idx, \"poster_url\"] = tmdb_poster_base_url + size + poster_path\n",
    "\n",
    "            if credits:\n",
    "                # Directors (may be multiple)\n",
    "                crew = credits.get(\"crew\", [])\n",
    "                directors = [str(p[\"name\"]) + \":\" + tmdb_person_base_url + str(p[\"id\"]) for p in crew if p.get(\"job\") == \"Director\"]\n",
    "                df.at[idx, \"director\"] = directors if directors else None\n",
    "\n",
    "                # Top 5 actors and their characters\n",
    "                cast = credits.get(\"cast\", [])[:5]\n",
    "                actor_names = [str(a[\"name\"]) + \":\" + tmdb_person_base_url + str(a[\"id\"]) for a in cast]\n",
    "                character_names = [a[\"character\"] for a in cast]\n",
    "                df.at[idx, \"actors\"] = actor_names if actor_names else None\n",
    "                df.at[idx, \"characters\"] = character_names if character_names else None\n",
    "        else:\n",
    "            print(\" ❌ No exact match found\")\n",
    "\n",
    "        sleep(0.25)\n",
    "\n",
    "    print(\"\\n✔️  Enrichment completed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enriched_merged = enrich_dataframe(merged)\n",
    "enriched_merged.to_csv(\"data/enriched_merged.csv\", index=False)\n"
   ],
   "id": "f75da710592dea1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "The code in the previous cell is in big parts AI generated by the free version of ChatGPT and was afterwards adapted by me.\n",
    "These following prompts were used:\n",
    "\n",
    "\n",
    "    \"ich baue einen Knowledge graph basierend auf Letterboxd User-Daten in einem Jupyter Notebook. Diese möchte ich jetzt mit TMDB Daten anreichern. Dazu habe ich mir gerade API Zugang verschafft, ich habe jetzt einen API Read Access Token.\n",
    "        Wie beginne ich, meine bestehende Tabelle mit weitern Film-Daten anzureichern?\"\n",
    "\n",
    "\n",
    "    \"def enrich_dataframe(df):\n",
    "            new_data = []\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                result = search_movie(title, year)\n",
    "                if result:\n",
    "                    details = get_movie_details(result[\"id\"])\n",
    "                    if details:\n",
    "                        new_data.append({\n",
    "                            \"title\": title,\n",
    "                            \"year\": year,\n",
    "                            \"tmdb_id\": result[\"id\"],\n",
    "                            \"overview\": details.get(\"overview\"),\n",
    "                            \"genres\": [g[\"name\"] for g in details.get(\"genres\", [])],\n",
    "                            \"runtime\": details.get(\"runtime\"),\n",
    "                            \"vote_average\": details.get(\"vote_average\"),\n",
    "                            \"poster_path\": details.get(\"poster_path\")\n",
    "                        })\n",
    "                else:\n",
    "                    new_data.append({\n",
    "                        \"title\": title,\n",
    "                        \"year\": year,\n",
    "                        \"tmdb_id\": None,\n",
    "                        \"overview\": None,\n",
    "                        \"genres\": None,\n",
    "                        \"runtime\": None,\n",
    "                        \"vote_average\": None,\n",
    "                        \"poster_path\": None\n",
    "                    })\n",
    "\n",
    "                sleep(0.25)  # Vermeide Rate-Limits\n",
    "\n",
    "            return pd.DataFrame(new_data)\n",
    "\n",
    "        add print statements to this method so the user gets feedback while the method is running\"\n",
    "\n",
    "\n",
    "    \"bitte englische kommentare stattdessen\"\n",
    "\n",
    "\n",
    "    \"Gerade erstellt die Funktion \"enrich_dataframe\" eine völlig neue Tabelle mit TMDB daten und beinhaltet aber keine meiner Daten die ich bereits in einer Tabelle habe.\n",
    "\n",
    "        Das ist mein Code bisher:\n",
    "        merged = pd.merge(\n",
    "            watched[['Name', 'Year', 'Letterboxd URI']],\n",
    "            ratings[['Name', 'Year', 'Letterboxd URI', 'Rating']],\n",
    "            on=['Name', 'Year', 'Letterboxd URI'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Add \"Liked\" information\n",
    "        liked_uris = set(likes[\"Letterboxd URI\"])\n",
    "\n",
    "        merged[\"Liked\"] = merged[\"Letterboxd URI\"].apply(\n",
    "            lambda uri: \"Yes\" if uri in liked_uris else \"No\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # Add \"Last Watched\" information\n",
    "\n",
    "        # Step 1: Group diary entries by movie and take the latest date as a string\n",
    "        last_watched = diary.groupby([\"Name\", \"Year\"])[\"Watched Date\"].max().reset_index()\n",
    "        last_watched.rename(columns={\"Watched Date\": \"Last watched date\"}, inplace=True)\n",
    "\n",
    "        # Step 2: Merge with the existing merged DataFrame\n",
    "        merged = pd.merge(\n",
    "            merged,\n",
    "            last_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # Add \"Times watched\" information\n",
    "\n",
    "        # Step 1: Sort diary by \"Watched Date\" so earliest watches come first\n",
    "        diary_sorted = diary.sort_values(\"Watched Date\")\n",
    "\n",
    "        # Step 2: Define a function to count watches and check if first was a rewatch\n",
    "        def get_times_watched(group):\n",
    "            count = len(group)\n",
    "            first_rewatch = group.iloc[0][\"Rewatch\"] == \"Yes\"\n",
    "            return f\"{count}+\" if first_rewatch else str(count)\n",
    "\n",
    "        # Step 3: Group and apply, excluding group keys from the inner DataFrame\n",
    "        times_watched = diary_sorted.groupby([\"Name\", \"Year\"], group_keys=False).apply(\n",
    "            get_times_watched\n",
    "        ).reset_index(name=\"Times watched\")\n",
    "\n",
    "\n",
    "        # Step 4: Merge into the main DataFrame\n",
    "        merged = (pd.merge(\n",
    "            merged,\n",
    "            times_watched,\n",
    "            on=[\"Name\", \"Year\"],\n",
    "            how=\"left\"\n",
    "        ).rename(columns={'Name': 'Title'}).sort_values(by=\"Last watched date\", ascending=False))\n",
    "\n",
    "        merged.head(15)\n",
    "\n",
    "        Meine Tabelle beinhaltet die Spalten:\n",
    "        Title, Year, Letterboxd URI, Rating, Liked, Last watched date, Times watched\n",
    "\n",
    "        Nun möchte ich die TMDB daten an diese tabelle anheften.\"\n",
    "\n",
    "\n",
    "    \"automatically include the full poster path in the table. use size \"original\"\"\n",
    "\n",
    "\n",
    "    \"def search_movie(title, year=None):\n",
    "            url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"year\"] = year\n",
    "            response = requests.get(url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\")\n",
    "                return results[0] if results else None\n",
    "            else:\n",
    "                print(f\"Error at {title}: {response.status_code}\")\n",
    "                return None\n",
    "\n",
    "        def get_movie_details(tmdb_id):\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def enrich_dataframe(df):\n",
    "            tmdb_base_url = \"https://www.themoviedb.org/movie/\"\n",
    "            poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "            size = \"original\"  # Use original size for poster images\n",
    "\n",
    "            # Create empty columns for TMDB data\n",
    "            df[\"tmdb_url\"] = None\n",
    "            df[\"overview\"] = None\n",
    "            df[\"genres\"] = None\n",
    "            df[\"runtime\"] = None\n",
    "            df[\"vote_average\"] = None\n",
    "            df[\"poster_url\"] = None\n",
    "\n",
    "            total = len(df)\n",
    "            print(f\"Starting enrichment for {total} films using TMDB data...\\n\")\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "                result = search_movie(title, year)\n",
    "                if result:\n",
    "                    print(\" ✅ Match found\")\n",
    "                    details = get_movie_details(result[\"id\"])\n",
    "                    if details:\n",
    "                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + str(result[\"id\"])\n",
    "                        df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\n",
    "                        df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "\n",
    "                        poster_path = details.get(\"poster_path\")\n",
    "                        if poster_path:\n",
    "                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\n",
    "                        else:\n",
    "                            df.at[idx, \"poster_url\"] = None\n",
    "                    else:\n",
    "                        print(\" ⚠️  Details not found\")\n",
    "                else:\n",
    "                    print(\" ❌ No match\")\n",
    "\n",
    "                sleep(0.25)  # Avoid rate limit\n",
    "\n",
    "            print(\"\\n✔️  Enrichment completed.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "        i want the following changes to this code:\n",
    "\n",
    "        instead of only searching for movies, it searches for movies and tv shows.\n",
    "        the algorithm works like this:\n",
    "        first it searches for it in the movie database always with title and year. from the list of search results it will try to match the title EXACTLY. only if an exact title match fails, it then tries to search for it in the tv show database. again, it tries to match the title exactly.\"\n",
    "\n",
    "\n",
    "    \"if it is a TV show, i dont want a runtime in my results\"\n",
    "\n",
    "\n",
    "    \"add a new column that includes info on whether the list entry is a TV show or a Movie\"\n",
    "\n",
    "\n",
    "    \"this works well! I have an additional problem:\n",
    "\n",
    "        in my testing i found problems with for example these two movies:\n",
    "\n",
    "        In my letterboxd data, the movie is called\n",
    "        The Hunger Games: Mockingjay – Part 1\n",
    "\n",
    "        TMDB has it listed as\n",
    "        The Hunger Games: Mockingjay - Part 1\n",
    "\n",
    "        the algorithm is unable to match it, i suspect because of the two different hypthens.\n",
    "\n",
    "        another case is this, letterboxd has the movie as\n",
    "        Godzilla × Kong: The New Empire\n",
    "\n",
    "        whereas TMDB lists it as\n",
    "        Godzilla x Kong: The New Empire\n",
    "\n",
    "        the algorithm is unable to match it, again likely because the \"x\" is different.\n",
    "\n",
    "        how can I solve this problem?\"\n",
    "\n",
    "\n",
    "    \"import requests\n",
    "        from time import sleep\n",
    "        import os\n",
    "        from dotenv import load_dotenv\n",
    "        import unicodedata\n",
    "        import re\n",
    "\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        TMDB_API_TOKEN = os.getenv('TMDB_API_TOKEN')\n",
    "\n",
    "        HEADERS = {\n",
    "            \"Authorization\": \"Bearer \" + TMDB_API_TOKEN,\n",
    "            \"Content-Type\": \"application/json;charset=utf-8\"\n",
    "        }\n",
    "\n",
    "        def normalize_title(title):\n",
    "            if not title:\n",
    "                return \"\"\n",
    "            # Unicode normalize, convert to ASCII-compatible\n",
    "            title = unicodedata.normalize(\"NFKC\", title)\n",
    "\n",
    "            # Replace common visually similar characters\n",
    "            substitutions = {\n",
    "                \"–\": \"-\",  # en dash\n",
    "                \"—\": \"-\",  # em dash\n",
    "                \"−\": \"-\",  # minus\n",
    "                \"×\": \"x\",  # multiplication sign\n",
    "                \"’\": \"'\",  # curly apostrophe\n",
    "                \"“\": '\"',\n",
    "                \"”\": '\"',\n",
    "                \"…\": \"...\",\n",
    "                \"&\": \"and\",  # optional\n",
    "            }\n",
    "\n",
    "            for orig, repl in substitutions.items():\n",
    "                title = title.replace(orig, repl)\n",
    "\n",
    "            # Collapse multiple spaces and lowercase\n",
    "            title = re.sub(r\"\\s+\", \" \", title).strip().lower()\n",
    "            return title\n",
    "\n",
    "        def search_exact_match(results, search_title):\n",
    "            norm_search = normalize_title(search_title)\n",
    "            for r in results:\n",
    "                tmdb_title = r.get(\"title\") or r.get(\"name\") or \"\"\n",
    "                if normalize_title(tmdb_title) == norm_search:\n",
    "                    return r\n",
    "            return None\n",
    "\n",
    "        def search_movie_or_tv(title, year=None):\n",
    "            # First: try movie search\n",
    "            movie_url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"year\"] = year\n",
    "\n",
    "            response = requests.get(movie_url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\", [])\n",
    "                match = search_exact_match(results, title)\n",
    "                if match:\n",
    "                    match[\"media_type\"] = \"movie\"\n",
    "                    return match\n",
    "\n",
    "            # Second: try TV search\n",
    "            tv_url = \"https://api.themoviedb.org/3/search/tv\"\n",
    "            params = {\"query\": title}\n",
    "            if year:\n",
    "                params[\"first_air_date_year\"] = year\n",
    "\n",
    "            response = requests.get(tv_url, headers=HEADERS, params=params)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"results\", [])\n",
    "                match = search_exact_match(results, title)\n",
    "                if match:\n",
    "                    match[\"media_type\"] = \"tv\"\n",
    "                    return match\n",
    "\n",
    "            # print(f\" Error or no match for '{title}' \")\n",
    "            return None\n",
    "\n",
    "        def get_movie_details(tmdb_id):\n",
    "            url = f\"https://api.themoviedb.org/3/movie/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        def get_details(tmdb_id, media_type):\n",
    "            url = f\"https://api.themoviedb.org/3/{media_type}/{tmdb_id}\"\n",
    "            response = requests.get(url, headers=HEADERS)\n",
    "            return response.json() if response.status_code == 200 else None\n",
    "\n",
    "        def enrich_dataframe(df):\n",
    "            tmdb_base_url = \"https://www.themoviedb.org/\"\n",
    "            poster_base_url = \"https://image.tmdb.org/t/p/\"\n",
    "            size = \"original\"\n",
    "\n",
    "            # Create empty columns for TMDB data\n",
    "            df[\"tmdb_url\"] = None\n",
    "            df[\"overview\"] = None\n",
    "            df[\"genres\"] = None\n",
    "            df[\"runtime\"] = None\n",
    "            df[\"vote_average\"] = None\n",
    "            df[\"poster_url\"] = None\n",
    "            df[\"media_type\"] = None  # New column: \"movie\" or \"tv\"\n",
    "\n",
    "            total = len(df)\n",
    "            print(f\"Starting enrichment for {total} titles using TMDB data...\\n\")\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                title = row[\"Title\"]\n",
    "                year = row.get(\"Year\", None)\n",
    "\n",
    "                print(f\"[{idx+1}/{total}] Searching: '{title}' ({year})\", end=\"\")\n",
    "\n",
    "                result = search_movie_or_tv(title, year)\n",
    "                if result:\n",
    "                    print(\" ✅ Match found\")\n",
    "                    media_type = result[\"media_type\"]\n",
    "                    details = get_details(result[\"id\"], media_type)\n",
    "\n",
    "                    if details:\n",
    "                        df.at[idx, \"media_type\"] = media_type\n",
    "                        df.at[idx, \"tmdb_url\"] = tmdb_base_url + f\"{media_type}/\" + str(result[\"id\"])\n",
    "                        df.at[idx, \"overview\"] = details.get(\"overview\")\n",
    "                        df.at[idx, \"genres\"] = [g[\"name\"] for g in details.get(\"genres\", [])]\n",
    "                        df.at[idx, \"vote_average\"] = details.get(\"vote_average\")\n",
    "\n",
    "                        # Only add runtime for movies\n",
    "                        if media_type == \"movie\":\n",
    "                            df.at[idx, \"runtime\"] = details.get(\"runtime\")\n",
    "                        else:\n",
    "                            df.at[idx, \"runtime\"] = None\n",
    "\n",
    "                        poster_path = details.get(\"poster_path\")\n",
    "                        if poster_path:\n",
    "                            df.at[idx, \"poster_url\"] = poster_base_url + size + poster_path\n",
    "                else:\n",
    "                    print(\" ❌ No exact match found\")\n",
    "\n",
    "                sleep(0.25)\n",
    "\n",
    "            print(\"\\n✔️  Enrichment completed.\")\n",
    "            return df\n",
    "\n",
    "\n",
    "\n",
    "        enriched_merged = enrich_dataframe(merged)\n",
    "        enriched_merged.to_csv(\"data/enriched_merged.csv\", index=False)\n",
    "\n",
    "        this is my code. Adapt it so that it includes a new column that includes the directors name. another column should include an array of actor names, and another column should include an array of character names.\"\n",
    "\n",
    "\n",
    "    \"i want to store them like the genres:\n",
    "        ['Adventure', 'Drama', 'Science Fiction']\n",
    "        also, it should be possible to have multiple directors.\n",
    "        please give me an updated \"enrich dataframe\" function\"\n",
    "\n",
    "'''"
   ],
   "id": "4501e4dd2abd2cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Knowledge Graph",
   "id": "5f0d3d84584b841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T22:56:34.359352Z",
     "start_time": "2025-08-05T22:56:34.247421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "# Load the CSV\n",
    "file_path = \"data/enriched_merged.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Helper: convert TMDB URLs to local IDs like movie123, person456\n",
    "def get_local_id(tmdb_url):\n",
    "    if not isinstance(tmdb_url, str):\n",
    "        return None  # Invalid or missing\n",
    "    match = re.search(r'themoviedb\\.org/(movie|tv|person|genre|company)/(\\d+)', tmdb_url)\n",
    "    if match:\n",
    "        entity_type, entity_id = match.groups()\n",
    "        return f\"{entity_type}{entity_id}\"\n",
    "    return None\n",
    "\n",
    "triples = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    tmdb_url = row.get('tmdb_url')\n",
    "    if not isinstance(tmdb_url, str) or not tmdb_url.startswith(\"http\"):\n",
    "        continue  # Skip rows with invalid or missing TMDB URL\n",
    "\n",
    "    # Extract ID and media type\n",
    "    try:\n",
    "        tmdb_id = tmdb_url.rstrip('/').split(\"/\")[-1]\n",
    "        media_type = str(row.get(\"media_type\", \"movie\")).strip().lower()\n",
    "        if media_type not in [\"movie\", \"tv\"]:\n",
    "            media_type = \"movie\"  # default fallback\n",
    "        subj = f\"{media_type}{tmdb_id}\"\n",
    "    except Exception as e:\n",
    "        continue  # Skip this row if any error occurs\n",
    "\n",
    "    # Basic movie/tv info\n",
    "    triples.append((subj, \"rdf:type\", f\"schema:{media_type.capitalize()}\"))\n",
    "    triples.append((subj, \"schema:name\", row[\"Title\"]))\n",
    "    triples.append((subj, \"schema:datePublished\", str(row[\"Year\"])))\n",
    "    triples.append((subj, \"schema:aggregateRating\", str(row[\"vote_average\"])))\n",
    "    triples.append((subj, \"schema:review\", str(row[\"Rating\"])))\n",
    "    triples.append((subj, \"ex:liked\", str(row[\"Liked\"])))\n",
    "    triples.append((subj, \"ex:lastWatched\", str(row[\"Last watched date\"])))\n",
    "    triples.append((subj, \"ex:timesWatched\", str(row[\"Times watched\"])))\n",
    "    triples.append((subj, \"ex:originalLanguage\", str(row[\"original_language\"])))\n",
    "    triples.append((subj, \"ex:popularity\", str(row[\"popularity\"])))\n",
    "\n",
    "\n",
    "    if not pd.isna(row[\"runtime\"]):\n",
    "        triples.append((subj, \"schema:duration\", str(int(row[\"runtime\"]))))\n",
    "\n",
    "    # Directors\n",
    "    try:\n",
    "        directors_raw = literal_eval(row[\"director\"])\n",
    "        for entry in directors_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            director_id = get_local_id(url)  # e.g. person1673654\n",
    "            if not director_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:director\", director_id))\n",
    "            triples.append((director_id, \"rdf:type\", \"schema:Person\"))\n",
    "            triples.append((director_id, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Actors\n",
    "    try:\n",
    "        actors_raw = literal_eval(row[\"actors\"])\n",
    "        for entry in actors_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            actor_id = get_local_id(url)  # e.g. person12345\n",
    "            if not actor_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:actor\", actor_id))\n",
    "            triples.append((actor_id, \"rdf:type\", \"schema:Person\"))\n",
    "            triples.append((actor_id, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Characters\n",
    "    try:\n",
    "        characters = literal_eval(row[\"characters\"])\n",
    "        for character in characters:\n",
    "            triples.append((subj, \"schema:character\", character))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Genres\n",
    "    try:\n",
    "        genres_raw = literal_eval(row[\"genres\"])\n",
    "        for entry in genres_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, url = entry.split(\":\", 1)\n",
    "            genre_id = get_local_id(url)  # e.g. genre18\n",
    "            if not genre_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:genre\", genre_id))\n",
    "            triples.append((genre_id, \"rdf:type\", \"schema:Text\"))\n",
    "            triples.append((genre_id, \"ex:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Countries Of Origin\n",
    "    try:\n",
    "        countriesOfOrigin = literal_eval(row[\"origin_country\"])\n",
    "        for entry in countriesOfOrigin:\n",
    "            triples.append((subj, \"schema:countryOfOrigin\", entry))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Production Companies\n",
    "    try:\n",
    "        companies_raw = literal_eval(row[\"production_companies\"])\n",
    "        for entry in companies_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue  # skip malformed\n",
    "            name, rest = entry.split(\":\", 1)\n",
    "            url, country = rest.rsplit(\":\", 1)\n",
    "            company_id = get_local_id(url)\n",
    "            if not company_id:\n",
    "                continue\n",
    "            triples.append((subj, \"schema:productionCompany\", company_id))\n",
    "            triples.append((company_id, \"rdf:type\", \"schema:Company\"))\n",
    "            triples.append((company_id, \"schema:name\", name.strip()))\n",
    "            triples.append((company_id, \"ex:country\", country))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Production Countries\n",
    "    try:\n",
    "        countries_raw = literal_eval(row[\"production_countries\"])\n",
    "        for entry in countries_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue\n",
    "            name, code = entry.split(\":\", 1)\n",
    "            triples.append((subj, \"ex:productionCountry\", code))\n",
    "            triples.append((code, \"rdf:type\", \"schema:Country\"))\n",
    "            triples.append((code, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Spoken Languages\n",
    "    try:\n",
    "        languages_raw = literal_eval(row[\"spoken_languages\"])\n",
    "        for entry in languages_raw:\n",
    "            if \":\" not in entry:\n",
    "                continue\n",
    "            name, code = entry.split(\":\", 1)\n",
    "            triples.append((subj, \"schema:inLanguage\", code))\n",
    "            triples.append((code, \"rdf:type\", \"schema:Language\"))\n",
    "            triples.append((code, \"schema:name\", name.strip()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame and export for PyKEEN\n",
    "triples_df = pd.DataFrame(triples, columns=[\"subject\", \"predicate\", \"object\"])\n",
    "triples_df.to_csv(\"data/movie_kg_triples.tsv\", sep=\"\\t\", index=False, header=False)\n"
   ],
   "id": "9fa708a446d79d3e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert to .ttl (Delete this again, as it is not necessary)",
   "id": "d7b7c368676c50bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T22:55:07.612311Z",
     "start_time": "2025-08-05T22:55:07.574871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert_tsv_to_ttl.py\n",
    "\n",
    "INPUT_FILE = \"data/movie_kg_triples.tsv\"\n",
    "OUTPUT_FILE = \"data/triples.ttl\"\n",
    "\n",
    "# Prefixes\n",
    "prefixes = [\n",
    "    \"@prefix schema: <http://schema.org/> .\",\n",
    "    \"@prefix ex: <http://example.org/> .\",\n",
    "    \"@prefix tmdb: <https://www.themoviedb.org/> .\",\n",
    "    \"@prefix : <http://example.org/> .\",\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "def escape(term):\n",
    "    # Basic escaping for URIs (if needed)\n",
    "    return term.replace(\" \", \"_\")\n",
    "\n",
    "def convert_tsv_to_ttl(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    triples = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            continue  # Skip malformed lines\n",
    "\n",
    "        subj, pred, obj = map(escape, parts)\n",
    "\n",
    "        # Expand schema: to full URI\n",
    "        if pred.startswith(\"schema:\"):\n",
    "            predicate = pred  # already prefixed\n",
    "        else:\n",
    "            predicate = f\":{pred}\"\n",
    "\n",
    "        triple = f\":{subj} {predicate} :{obj} .\"\n",
    "        triples.append(triple)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(prefixes + triples))\n",
    "\n",
    "    print(f\"✅ Converted {len(triples)} triples to {output_file}\")\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    convert_tsv_to_ttl(INPUT_FILE, OUTPUT_FILE)\n"
   ],
   "id": "3f8e0ee3e769bb77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 44946 triples to data/triples.ttl\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
